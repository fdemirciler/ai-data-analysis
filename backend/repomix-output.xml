This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
deploy-analysis.ps1
deploy-preprocess.ps1
docs/api.md
docs/ctx_contract.md
docs/ops_checklist.md
docs/payload_schema.json
docs/payload_v1_vs_v2.md
docs/runtime_flags.md
env.chat.env
env.chat.yaml
env.sign-upload-url.env
env.sign-upload-url.yaml
functions/orchestrator/gemini_client.py
functions/orchestrator/main.py
functions/orchestrator/requirements.txt
functions/orchestrator/sandbox_runner.py
functions/orchestrator/worker.py
functions/sign_upload_url/main.py
functions/sign_upload_url/requirements.txt
run-preprocess/header_utils.py
run-preprocess/main.py
run-preprocess/pipeline_adapter_polars.py
run-preprocess/pipeline_adapter.py
run-preprocess/Procfile
run-preprocess/project.toml
run-preprocess/README.md
run-preprocess/requirements-dev.txt
run-preprocess/requirements.txt
run-preprocess/runtime.txt
run-preprocess/tests/fixtures/balance_sheet.csv
run-preprocess/tests/fixtures/multi_row_header.csv
run-preprocess/tests/fixtures/title_rows.csv
run-preprocess/tests/test_header_detection_simple.py
run-preprocess/tests/test_integration_polars_balance_sheet.py
run-preprocess/tests/test_polars_helpers.py
shared/ast_validator.py
test.ps1
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="deploy-analysis.ps1">
# =========
# Settings (Analysis Stage Only)
# =========
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

# Configure gcloud
gcloud config set project $PROJECT_ID | Out-Null

# Script-relative paths
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$SRC_FN_SIGN = Join-Path $SCRIPT_DIR "functions\sign_upload_url"
$SRC_FN_ORCH = Join-Path $SCRIPT_DIR "functions\orchestrator"

# =====================
# Enable required APIs
# =====================
gcloud services enable `
  cloudfunctions.googleapis.com `
  run.googleapis.com `
  artifactregistry.googleapis.com `
  storage.googleapis.com `
  firestore.googleapis.com `
  secretmanager.googleapis.com `
  iamcredentials.googleapis.com

# ===========================
# Project number and SA vars
# ===========================
$PROJECT_NUMBER = gcloud projects describe $PROJECT_ID --format="value(projectNumber)"
$SERVICE_ACCOUNT = "$PROJECT_NUMBER-compute@developer.gserviceaccount.com"

# ==============================
# Compose ALLOWED_ORIGINS string
# ==============================
$ALLOWED_ORIGINS = "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com"

# Build YAML env files for Gen2
$SIGN_ENV_FILE = Join-Path $SCRIPT_DIR "env.sign-upload-url.yaml"
$CHAT_ENV_FILE = Join-Path $SCRIPT_DIR "env.chat.yaml"

# Sign-upload-url env (YAML)
@"
FILES_BUCKET: "$BUCKET"
GCP_PROJECT: "$PROJECT_ID"
TTL_DAYS: "1"
RUNTIME_SERVICE_ACCOUNT: "$SERVICE_ACCOUNT"
ALLOWED_ORIGINS: "$ALLOWED_ORIGINS"
"@ | Out-File -Encoding ascii -FilePath $SIGN_ENV_FILE

# Chat env (YAML)
@"
FILES_BUCKET: "$BUCKET"
GCP_PROJECT: "$PROJECT_ID"
ORCH_IPC_MODE: "base64"
GEMINI_FUSED: "1"
RUNTIME_SERVICE_ACCOUNT: "$SERVICE_ACCOUNT"
ALLOWED_ORIGINS: "$ALLOWED_ORIGINS"
"@ | Out-File -Encoding ascii -FilePath $CHAT_ENV_FILE

# ======================================
# Secret Manager access for chat (Gemini)
# ======================================
gcloud secrets add-iam-policy-binding GEMINI_API_KEY `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/secretmanager.secretAccessor" `
  --project=$PROJECT_ID

# ============================================
# Deploy Functions Gen2: sign-upload-url (HTTP)
# ============================================
# Auth flag (set ALLOW_UNAUTHENTICATED=1 in env for dev convenience)
$AUTH_FLAG = if ($env:ALLOW_UNAUTHENTICATED -eq "1") { "--allow-unauthenticated" } else { "" }

gcloud functions deploy sign-upload-url `
  --gen2 `
  --runtime=python312 `
  --region=$REGION `
  --source="$SRC_FN_SIGN" `
  --entry-point="sign_upload_url" `
  --trigger-http `
  $AUTH_FLAG `
  --service-account="$SERVICE_ACCOUNT" `
  --env-vars-file="$SIGN_ENV_FILE"

# ======================================
# Deploy Functions Gen2: chat (SSE HTTP)
# ======================================
gcloud functions deploy chat `
  --gen2 `
  --runtime=python312 `
  --region=$REGION `
  --source="$SRC_FN_ORCH" `
  --entry-point="chat" `
  --trigger-http `
  $AUTH_FLAG `
  --service-account="$SERVICE_ACCOUNT" `
  --memory=512Mi `
  --env-vars-file="$CHAT_ENV_FILE" `
  --set-secrets="GEMINI_API_KEY=GEMINI_API_KEY:latest"

# ====================
# Grab service URLs
# ====================
$SIGN_URL = gcloud functions describe sign-upload-url --gen2 --region=$REGION --format="value(url)"
$CHAT_URL = gcloud functions describe chat --gen2 --region=$REGION --format="value(url)"

# Trim URLs
$SIGN_URL = ($SIGN_URL | Out-String).Trim()
$CHAT_URL = ($CHAT_URL | Out-String).Trim()

Write-Host "sign-upload-url: $SIGN_URL"
Write-Host "chat (SSE):     $CHAT_URL"
</file>

<file path="deploy-preprocess.ps1">
# =========
# Settings (Preprocess Stage Only)
# =========
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

gcloud config set project $PROJECT_ID | Out-Null

# Script-relative paths
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$ROOT_DIR   = Split-Path -Parent $SCRIPT_DIR
$SRC_RUN     = Join-Path $SCRIPT_DIR "run-preprocess"

# =====================
# Enable required APIs
# =====================
gcloud services enable `
  run.googleapis.com `
  eventarc.googleapis.com `
  cloudbuild.googleapis.com `
  artifactregistry.googleapis.com `
  firestore.googleapis.com `
  storage.googleapis.com `
  pubsub.googleapis.com

# ===========================
# Project number and SA vars
# ===========================
$PROJECT_NUMBER = gcloud projects describe $PROJECT_ID --format="value(projectNumber)"
$SERVICE_ACCOUNT = "$PROJECT_NUMBER-compute@developer.gserviceaccount.com"

# ======================================================
# Deploy Cloud Run: preprocess-svc (from run-preprocess/)
# ======================================================
gcloud run deploy preprocess-svc `
  --region=$REGION `
  --source="$SRC_RUN" `
  --service-account="$SERVICE_ACCOUNT" `
  --set-build-env-vars="GOOGLE_PYTHON_VERSION=3.12" `
  --cpu=2 `
  --memory=2Gi `
  --concurrency=10 `
  --set-env-vars="FILES_BUCKET=$BUCKET,GCP_PROJECT=$PROJECT_ID,TTL_DAYS=1,PREPROCESS_ENGINE=polars" `
  --no-allow-unauthenticated

# ==================================================
# IAM: baseline access for preprocess + artifacts
# ==================================================
# Firestore (read/write)
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/datastore.user"

# Storage (read raw + write artifacts) - scoped to the bucket
gcloud storage buckets add-iam-policy-binding gs://$BUCKET `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/storage.objectAdmin"

# =============================
# GCS lifecycle: users/ → 1 day
# =============================
$lifecycleFile = Join-Path $env:TEMP "gcs_lifecycle_users.json"
@'
{
  "rule": [
    {
      "action": {"type": "Delete"},
      "condition": {"age": 1, "matchesPrefix": ["users/"]}
    }
  ]
}
'@ | Out-File -FilePath $lifecycleFile -Encoding ascii -NoNewline
try {
  gsutil lifecycle set $lifecycleFile gs://$BUCKET
  Write-Host "Applied GCS lifecycle rule: delete users/ after 1 day."
} catch {
  Write-Host "Warning: failed to set lifecycle:" $_.Exception.Message
} finally {
  if (Test-Path $lifecycleFile) { Remove-Item $lifecycleFile -Force }
}

# ==================================================
# Eventarc: permissions + trigger for GCS Object Finalize
# ==================================================
# Allow SA to receive events and invoke the service
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/eventarc.eventReceiver"

# Grant Pub/Sub Publisher to the Cloud Storage service account so GCS can
# publish CloudEvents to Pub/Sub in this project (required by Eventarc).
$GCS_SA = "service-$PROJECT_NUMBER@gs-project-accounts.iam.gserviceaccount.com"
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$GCS_SA" `
  --role="roles/pubsub.publisher"

# Verify service exists before binding/creating trigger
$RUN_URL_CHECK = gcloud run services describe preprocess-svc --region=$REGION --format="value(status.url)"
if ($RUN_URL_CHECK) {
  gcloud run services add-iam-policy-binding preprocess-svc `
    --region=$REGION `
    --member="serviceAccount:$SERVICE_ACCOUNT" `
    --role="roles/run.invoker"

  # Ensure trigger (GCS -> Cloud Run) exists; update if present
  $TRIGGER_NAME = "preprocess-trigger"
  $TRIGGER_EXISTS = gcloud eventarc triggers describe $TRIGGER_NAME --location=$REGION --format="value(name)" 2>$null
  if ($TRIGGER_EXISTS) {
    gcloud eventarc triggers update $TRIGGER_NAME `
      --location=$REGION `
      --event-filters="type=google.cloud.storage.object.v1.finalized" `
      --event-filters="bucket=$BUCKET" `
      --destination-run-service="preprocess-svc" `
      --destination-run-path="/eventarc" `
      --destination-run-region=$REGION `
      --service-account="$SERVICE_ACCOUNT"
  } else {
    gcloud eventarc triggers create $TRIGGER_NAME `
      --location=$REGION `
      --event-filters="type=google.cloud.storage.object.v1.finalized" `
      --event-filters="bucket=$BUCKET" `
      --destination-run-service="preprocess-svc" `
      --destination-run-path="/eventarc" `
      --destination-run-region=$REGION `
      --service-account="$SERVICE_ACCOUNT"
  }

  # Verify trigger
  gcloud eventarc triggers describe $TRIGGER_NAME --location=$REGION
} else {
  Write-Host "Cloud Run service preprocess-svc not found; skipping Eventarc IAM/trigger setup."
}

# ====================
# Output service URL
# ====================
$RUN_URL  = gcloud run services describe preprocess-svc --region=$REGION --format="value(status.url)"
$RUN_URL  = ($RUN_URL  | Out-String).Trim()
Write-Host "preprocess-svc: $RUN_URL"
</file>

<file path="docs/api.md">
# Backend API (Draft v1)

This document describes the backend endpoints and SSE event contract. All endpoints are in region `europe-west4`.

## Authentication

- For simplicity in the draft, examples omit auth. In production, attach a user identity (e.g., Firebase Auth ID) and pass `uid` and `sid` (session ID) via headers or cookies.
- Example headers (subject to change):
  - `X-User-Id: <uid>`
  - `X-Session-Id: <sid>`

---

## 1) GET `/api/sign-upload-url`

Issue a signed URL for direct browser PUT to GCS.

Query parameters:
- `filename`: string (required)
- `size`: integer bytes (required, ≤ 20MB)
- `type`: mime type, e.g., `text/csv` or `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet` (required)

Headers:
- `X-User-Id`: uid
- `X-Session-Id`: sid

Response 200 JSON:
```json
{
  "url": "https://storage.googleapis.com/...",
  "datasetId": "<uuid>",
  "storagePath": "users/<uid>/sessions/<sid>/datasets/<datasetId>/raw/input.csv"
}
```

Notes:
- Signed URL method: PUT
- Required headers when uploading: `Content-Type: <type>`
- CORS: configured on bucket `ai-data-analyser-files` for `http://localhost:3000` (prod domain added later)

---

## 2) POST `/api/chat` (SSE)

Start an analysis step; streams progress and results. LLM (Gemini 2.5 Flash) generates
Python that operates on the dataset with Pandas/Numpy. Code is validated and executed
in a sandboxed child process with a hard 60s timeout.

Request JSON:
```json
{
  "uid": "<uid>",
  "sessionId": "<sid>",
  "datasetId": "<datasetId>",
  "question": "Find trends in revenue by region"
}
```

Headers:
- `X-User-Id: <uid>` (optional if provided in body)
- `X-Session-Id: <sid>` (optional if provided in body)

SSE events (examples):
```json
{"type":"received","data":{"sessionId":"<sid>","datasetId":"<datasetId>"}}
{"type":"validating"}
{"type":"generating_code"}
{"type":"running_fast"}
{"type":"summarizing"}
{"type":"persisting"}
{"type":"done","data":{
  "messageId":"<uuid>",
  "chartData": {"kind":"bar","labels":["A","B"],"series":[{"label":"Value","data":[1,2]}]},
  "tableSample": [{"category":"A","value":1},{"category":"B","value":2}],
  "uris":{
    "table":"gs://ai-data-analyser-files/users/.../results/<messageId>/table.json",
    "metrics":"gs://.../metrics.json",
    "chartData":"gs://.../chart_data.json",
    "summary":"gs://.../summary.json"
  }
}}
{"type":"ping"}
```

Chart data schema (backend → frontend/Chart.js):
```json
{
  "kind": "bar" | "line" | "pie",
  "labels": ["x1", "x2", "x3"],
  "series": [
    { "label": "Series A", "data": [1, 2, 3] }
  ],
  "options": { /* optional hints */ }
}
```

Behavior:
- Heartbeat: `{ "type": "ping" }` about every 20–25 seconds.
- Hard timeout: 60s. On expiry emits `{"type":"error","data":{"code":"TIMEOUT_HARD"}}`.
- Soft timeout: currently logs only (no partial-return yet).

Error codes (non-exhaustive):
- `MISSING_PARQUET` – cleaned.parquet not found
- `DOWNLOAD_FAILED` – failed to download dataset artifacts
- `CODEGEN_FAILED` – LLM code generation failure
- `CODE_VALIDATION_FAILED` – AST/allowlist validation failure
- `EXEC_FAILED` – sandboxed execution failed (stderr included)
- `BAD_RESULT` – sandbox output not valid JSON/shape
- `PERSIST_FAILED` – failed to write results to GCS

---

## 3) POST `/api/session/:id/close`

Immediately purge all artifacts for a session and delete Firestore docs.

Response 202 JSON:
```json
{"status":"accepted"}
```

Notes:
- GCS prefix-scope TTL (1 day on `users/`) remains as safety net.

---

## 4) Eventarc → Cloud Run `/eventarc` (Preprocess)

HTTP target for GCS-object-finalize events. Expects CloudEvent-like body with at least:
```json
{
  "data": {
    "bucket": "ai-data-analyser-files",
    "name": "users/<uid>/sessions/<sid>/datasets/<datasetId>/raw/input.csv"
  },
  "id": "...",
  "source": "...",
  "type": "google.cloud.storage.object.v1.finalized",
  "time": "..."
}
```

Response:
- `204 No Content` on success/ignored events.

Processing:
- Download raw → run pipeline → write `cleaned.parquet`, `payload.json`, `cleaning_report.json` → update Firestore dataset doc.
</file>

<file path="docs/ctx_contract.md">
# ctx Contract

This document specifies the `ctx` object passed into the user-generated analysis function `def run(df, ctx):`.

The goals for `ctx` are:
- Provide enough context to generate compliant, efficient code.
- Enforce budget and safety constraints (time, memory, charts).
- Keep content compact and deterministic.

## Top-level structure

```json
{
  "dataset": {
    "rows": 0,
    "columns": 0,
    "column_names": ["..."],
    "dtypes": {"col": "dtype"}
  },
  "limits": {
    "maxCharts": 3,
    "timeBudgetSec": 10,
    "memoryMB": 512,
    "sampleRowsForDisplay": 50
  },
  "allowlist": [
    "pandas", "numpy", "matplotlib", "seaborn",
    "math", "statistics", "json", "io", "itertools", "functools", "collections", "re", "datetime"
  ],
  "provenance": {
    "sessionId": "...",
    "datasetId": "...",
    "messageId": "..."
  },
  "hazards": [
    "free_text:notes",
    "high_cardinality:segment"
  ],
  "chartability": {
    "numeric": ["revenue", "users"],
    "categorical_low_card": ["region"]
  },
  "cost": {
    "size_grade": "S",
    "est_rows_for_fast": 12500,
    "heavy_ops_caveat": ""
  },
  "seed": 42,
  "notes": [
    "No network access or file writes allowed.",
    "Prefer vectorized operations over loops.",
    "Avoid long-running operations."
  ]
}
```

## Field reference

- **dataset**
  - **rows**: Total number of rows in the cleaned dataset.
  - **columns**: Total number of columns in the cleaned dataset.
  - **column_names**: Ordered list of column names for the DataFrame.
  - **dtypes**: Mapping of column name to dtype string (e.g., `float64`, `int64`, `object`).

- **limits**
  - **maxCharts**: Maximum number of charts to produce (images returned as base64 PNG).
  - **timeBudgetSec**: Soft wall-clock budget; executors enforce hard timeouts.
  - **memoryMB**: Approximate memory budget for fast vs complex executors.
  - **sampleRowsForDisplay**: Maximum row count to include in small display tables.

- **allowlist**
  - Libraries and stdlib modules that may be imported by generated code. Any import outside this list fails validation.

- **provenance**
  - Identifiers useful for logging and correlating results.

- **hazards** (optional)
  - Flags derived from payload analysis (e.g., free-text columns, high cardinality) to steer safer code paths.

- **chartability** (optional)
  - Suggested columns that make good candidates for basic charts.

- **cost** (optional)
  - Hints about dataset size and expected fast-path capacity.

- **seed**
  - Fixed seed (e.g., 42) to ensure deterministic sampling and plotting randomness.

- **notes**
  - Additional guardrails or instructions for generated code.

## Example usage in `run(df, ctx)`

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def run(df, ctx):
    # Respect limits
    max_charts = ctx.get("limits", {}).get("maxCharts", 3)

    # Simple summary
    summary = f"Rows: {ctx['dataset']['rows']}, Cols: {ctx['dataset']['columns']}"

    # Example table (head limited by display budget)
    display_n = ctx.get("limits", {}).get("sampleRowsForDisplay", 50)
    table = df.head(min(5, display_n)).to_dict(orient="records")

    charts = []
    if max_charts > 0:
        # Example histogram if a numeric column exists
        numeric_cols = [c for c, t in ctx["dataset"]["dtypes"].items() if t.startswith("int") or t.startswith("float")]
        if numeric_cols:
            col = numeric_cols[0]
            import io, base64
            fig, ax = plt.subplots(figsize=(5, 3))
            df[col].dropna().hist(ax=ax, bins=20)
            ax.set_title(f"Histogram of {col}")
            buf = io.BytesIO()
            fig.tight_layout()
            fig.savefig(buf, format="png")
            plt.close(fig)
            png_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
            charts.append({"png_base64": png_b64, "caption": f"Histogram of {col}"})

    return {
        "summary": summary,
        "tables": [table],
        "charts": charts,
        "notes": []
    }
```
</file>

<file path="docs/ops_checklist.md">
# Ops Checklist (GCS, Eventarc, Firestore TTL)

Project: `ai-data-analyser`  •  Region: `europe-west4`  •  Bucket: `ai-data-analyser-files`

## 1) GCS CORS for signed URL uploads

Create `cors.json` locally:
```json
[
  {
    "origin": ["http://localhost:3000"],
    "method": ["PUT", "GET", "HEAD"],
    "responseHeader": ["Content-Type", "Authorization", "x-goog-meta-*"],
    "maxAgeSeconds": 3600
  }
]
```

Apply CORS:
```bash
gsutil cors set cors.json gs://ai-data-analyser-files
gsutil cors get gs://ai-data-analyser-files
```

## 2) GCS lifecycle: prefix-scoped 1-day TTL

Create `lifecycle.json`:
```json
{
  "rule": [
    {
      "action": {"type": "Delete"},
      "condition": {"age": 1, "matchesPrefix": ["users/"]}
    }
  ]
}
```

Apply lifecycle:
```bash
gsutil lifecycle set lifecycle.json gs://ai-data-analyser-files
gsutil lifecycle get gs://ai-data-analyser-files
```

## 3) Eventarc trigger → Cloud Run preprocess

Example (adjust service account as needed):
```bash
gcloud eventarc triggers create preprocess-trigger \
  --location=europe-west4 \
  --event-filters="type=google.cloud.storage.object.v1.finalized" \
  --event-filters="bucket=ai-data-analyser-files" \
  --destination-run-service=preprocess-svc \
  --destination-run-region=europe-west4 \
  --service-account=preprocess-svc@ai-data-analyser.iam.gserviceaccount.com
```

Verify trigger:
```bash
gcloud eventarc triggers describe preprocess-trigger --location=europe-west4
```

## 4) Firestore TTL (1 day)

Enable TTL on the following collection groups via Console (recommended) or API:
- `users/*/sessions/*`  → field: `ttlAt`
- `users/*/sessions/*/datasets/*`  → field: `ttlAt`
- `users/*/sessions/*/messages/*`  → field: `ttlAt`

Write policy:
- On session creation, set `ttlAt = createdAt + 1 day`.
- On session close, set `ttlAt = now` before deletion (belt-and-suspenders).

## 5) Secret Manager: Gemini API key

Create or update:
```bash
gcloud secrets versions add GEMINI_API_KEY --data-file=gemini_key.txt \
  --project=ai-data-analyser
```

Grant access to service accounts (orchestrator, executors):
```bash
gcloud secrets add-iam-policy-binding GEMINI_API_KEY \
  --member=serviceAccount:orchestrator-func@ai-data-analyser.iam.gserviceaccount.com \
  --role=roles/secretmanager.secretAccessor \
  --project=ai-data-analyser
```

## 6) Budget alerts and Monitoring (high level)

- Set a budget in Cloud Billing with alerts at 70% and 90%.
- Create dashboards for:
  - Error rates by stage (preprocess, classify, validate, run)
  - p95 latencies
  - Complex-path ratio and failover counts

## 7) Scheduler → usage-watcher Function

Create scheduler job (every 10 minutes):
```bash
gcloud scheduler jobs create http usage-watcher \
  --schedule="*/10 * * * *" \
  --uri="https://<cloudfunctions-url>/usage-watcher" \
  --http-method=POST \
  --oauth-service-account-email=usage-watcher-func@ai-data-analyser.iam.gserviceaccount.com \
  --location=europe-west4
```
</file>

<file path="docs/payload_schema.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "payload",
  "$comment": "This schema validates the v1 payload (dataset, columns, sample_rows, cleaning_report, mode, version). Additional v2 fields are additive and may be present but are not validated here: schema_version, header_info, analysis_hints, dataset_summary.",
  "type": "object",
  "required": ["dataset", "columns", "mode", "version"],
  "properties": {
    "dataset": {
      "type": "object",
      "required": ["rows", "columns", "column_names", "dtypes"],
      "properties": {
        "rows": { "type": "integer", "minimum": 0 },
        "columns": { "type": "integer", "minimum": 0 },
        "column_names": {
          "type": "array",
          "items": { "type": "string" }
        },
        "dtypes": {
          "type": "object",
          "additionalProperties": { "type": "string" }
        }
      }
    },
    "columns": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "required": ["type", "null_pct", "unique_pct"],
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "integer",
              "float",
              "percentage",
              "currency",
              "date",
              "categorical",
              "text",
              "id"
            ]
          },
          "null_pct": { "type": "number", "minimum": 0, "maximum": 100 },
          "unique_pct": { "type": "number", "minimum": 0, "maximum": 100 },
          "top_values": {
            "type": "array",
            "items": {
              "type": "object",
              "required": ["value", "count"],
              "properties": {
                "value": { },
                "count": { "type": "integer", "minimum": 0 }
              }
            },
            "maxItems": 5
          },
          "datetime_format": { "type": ["string", "null"] },
          "is_potential_dimension": { "type": "boolean" }
        }
      }
    },
    "sample_rows": {
      "type": "array",
      "items": { "type": "object" },
      "maxItems": 50
    },
    "excelInfo": {
      "type": "object",
      "properties": {
        "sheet_used": { "type": "integer", "minimum": 0 },
        "sheet_name": { "type": ["string", "null"] },
        "sheets_total": { "type": ["integer", "null"], "minimum": 1 }
      }
    },
    "cleaning_report": { "type": "object" },
    "mode": { "type": "string", "enum": ["full", "schema_only"] },
    "version": { "type": "string", "enum": ["1"] }
  }
}
</file>

<file path="docs/payload_v1_vs_v2.md">
# Payload examples: v1 (baseline) vs v2 (additive)

Below are minimal examples illustrating the additive v2 fields. The schema at `backend/docs/payload_schema.json` validates v1 only; v2 fields are optional and ignored by v1 validators.

## v1 (current schema)
```json
{
  "dataset": { "rows": 123, "columns": 4, "column_names": ["Metric", "2023", "2024", "2025"], "dtypes": {"Metric": "string", "2023": "float64", "2024": "float64", "2025": "float64"} },
  "columns": {
    "Metric": { "type": "text", "null_pct": 0.0, "unique_pct": 100.0, "top_values": [] },
    "2023": { "type": "float", "null_pct": 0.0, "unique_pct": 95.0, "top_values": [] }
  },
  "sample_rows": [ { "Metric": "Cash", "2023": 1990.0, "2024": 3389.0 } ],
  "cleaning_report": { "header_row": 1, "renamed_columns": {}, "numeric_columns": ["2023", "2024"], "rows_before": 130, "rows_after": 123, "file_kind": "csv" },
  "mode": "full",
  "version": "1"
}
```

## v2 (additive, compact)
```json
{
  "dataset": { "rows": 123, "columns": 4, "column_names": ["Metric", "2023", "2024", "2025"], "dtypes": {"Metric": "string", "2023": "float64", "2024": "float64", "2025": "float64"} },
  "columns": {
    "Metric": { "type": "text", "null_pct": 0.0, "unique_pct": 100.0, "top_values": [] },
    "2023": { "type": "float", "null_pct": 0.0, "unique_pct": 95.0, "top_values": [] }
  },
  "sample_rows": [ { "Metric": "Cash", "2023": 1990.0, "2024": 3389.0 } ],
  "cleaning_report": { "header_row": 1, "renamed_columns": {}, "numeric_columns": ["2023", "2024"], "rows_before": 130, "rows_after": 123, "file_kind": "csv" },
  "mode": "full",
  "version": "1",

  "schema_version": "2.0",
  "header_info": {
    "method": "auto_detected",
    "header_row_index": 1,
    "confidence": 0.78,
    "original_headers": ["Metric", "2023", "2024", "2025"],
    "final_headers": ["Metric", "2023", "2024", "2025"],
    "is_transposed": false
  },
  "analysis_hints": {
    "detected_header_row": 1,
    "header_confidence": 0.78,
    "pct_missing_total": 0.02,
    "first_column_type": "dimension",
    "likely_pivoted": true,
    "temporal_columns": [1, 2, 3],
    "numeric_columns": [1, 2, 3]
  },
  "dataset_summary": "Dataset has 123 rows and 4 columns. Header row 1 detected (confidence 0.78). Structure: first column dimension, pivoted=true."
}
```
</file>

<file path="docs/runtime_flags.md">
# Runtime Flags (Firestore: `config/runtime_flags`)

Default document:
```json
{
  "allowComplex": true,
  "perHourLimit": 10,
  "perDayLimit": 60,
  "sampleRowsForLLM": 50,
  "metricRenameHeuristic": false,
  "maxCharts": 3
}
```

Notes:
- `allowComplex` is toggled by the Scheduler-driven `usage-watcher` function.
- `metricRenameHeuristic=false` preserves original column names; the pipeline emits `is_potential_dimension` flags instead.
- `sampleRowsForLLM` sets payload sample size and the `ctx.limits.sampleRowsForDisplay` hint.
</file>

<file path="env.chat.env">
FILES_BUCKET=ai-data-analyser-files
GCP_PROJECT=ai-data-analyser
ORCH_IPC_MODE=base64
GEMINI_FUSED=1
RUNTIME_SERVICE_ACCOUNT=1012295827257-compute@developer.gserviceaccount.com
ALLOWED_ORIGINS=http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com
</file>

<file path="env.chat.yaml">
FILES_BUCKET: "ai-data-analyser-files"
GCP_PROJECT: "ai-data-analyser"
ORCH_IPC_MODE: "base64"
GEMINI_FUSED: "1"
RUNTIME_SERVICE_ACCOUNT: "1012295827257-compute@developer.gserviceaccount.com"
ALLOWED_ORIGINS: "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com"
</file>

<file path="env.sign-upload-url.env">
FILES_BUCKET=ai-data-analyser-files
GCP_PROJECT=ai-data-analyser
TTL_DAYS=1
RUNTIME_SERVICE_ACCOUNT=1012295827257-compute@developer.gserviceaccount.com
ALLOWED_ORIGINS=http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com
</file>

<file path="env.sign-upload-url.yaml">
FILES_BUCKET: "ai-data-analyser-files"
GCP_PROJECT: "ai-data-analyser"
TTL_DAYS: "1"
RUNTIME_SERVICE_ACCOUNT: "1012295827257-compute@developer.gserviceaccount.com"
ALLOWED_ORIGINS: "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com"
</file>

<file path="functions/orchestrator/gemini_client.py">
from __future__ import annotations

import os
import re
from typing import Any, Dict, List

import google.generativeai as genai

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

_MODEL_NAME = "gemini-2.5-flash"  # As requested
_API_KEY = os.getenv("GEMINI_API_KEY", "")

_MAX_TOKENS = 4096
_TEMPERATURE = 0.2

_configured = False
_model = None


def _ensure_model():
    """Ensure the Gemini client is configured and return the model."""
    global _configured, _model
    if not _configured:
        if not _API_KEY:
            raise RuntimeError("GEMINI_API_KEY not set")
        genai.configure(api_key=_API_KEY)
        _model = genai.GenerativeModel(_MODEL_NAME)
        _configured = True
    return _model


# ---------------------------------------------------------------------------
# Core: Generate Analysis Code
# ---------------------------------------------------------------------------

def generate_analysis_code(
    question: str,
    schema_snippet: str,
    sample_rows: list[dict],
    row_limit: int = 200
) -> str:
    """
    Ask Gemini to produce Python code, with a one-time auto-repair attempt.
    This function is primarily used for the repair loop.
    """
    model = _ensure_model()
    sample_preview = sample_rows[: min(len(sample_rows), 10)]

    prompt = (
        "You are an expert Python data analyst. Write a single Python function "
        "`def run(df, ctx):` to answer the user's question about the dataset.\n\n"
        "OUTPUT RULES (STRICT):\n"
        "- Return ONLY one fenced Python code block starting with ```python.\n"
        "- The function MUST return a dict with EXACT keys: 'table' (list of dict rows answering the question),\n"
        "  'metrics' (dict of key figures), and 'chartData' (object with keys: 'kind', 'labels', 'series' = list of {label, data}).\n"
        "- Do NOT return 'tables' or 'charts' keys. Use 'table' and 'chartData' only.\n"
        "- Respect ctx.get('row_limit', 200) when returning 'table'.\n"
        "- Use robust numeric handling: prefer pd.to_numeric(..., errors='coerce') and select_dtypes(include=[np.number]) for stats.\n"
        "- Never use complex dtype or astype(complex).\n"
        "- If a chart is appropriate, populate 'chartData' with non-empty labels and numeric series; else return empty {}.\n"
        "- Allowed imports: pandas, numpy, matplotlib, seaborn, math, statistics, json, io, "
        "itertools, functools, collections, re, datetime, base64.\n\n"
        f"SCHEMA:\n{schema_snippet}\n\n"
        f"SAMPLE ROWS:\n{sample_preview}\n\n"
        f"USER QUESTION:\n\"{question}\"\n\n"
        "Return only the fenced Python code block now."
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "max_output_tokens": _MAX_TOKENS,
            "temperature": _TEMPERATURE,
        },
    )

    text = _safe_response_text(resp)
    code = _extract_code_block(text)

    # Accept 'def run (df, ctx):' with arbitrary whitespace
    if not code or not re.search(r"def\s+run\s*\(", code):
        raise RuntimeError("CODEGEN_FAILED: Missing valid 'def run(df, ctx):' implementation.")
    return code


# ---------------------------------------------------------------------------
# Generate Summary
# ---------------------------------------------------------------------------

def generate_summary(
    question: str,
    table_head: list[dict],
    metrics: dict,
    code: str | None = None
) -> str:
    """Generate a concise, data-driven summary from analysis results."""
    model = _ensure_model()
    preview = table_head[: min(len(table_head), 5)]

    prompt = (
        "You are a financial data analyst. Interpret the analysis results below. "
        "Focus on trends, anomalies, or key figures; do NOT describe the code.\n\n"
        f"USER QUESTION: \"{question}\"\n\n"
        f"TABLE PREVIEW:\n{preview}\n\n"
        f"KEY METRICS:\n{metrics}\n\n"
        "Now write a one-paragraph interpretation of the data:"
    )

    try:
        resp = model.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": _MAX_TOKENS,
                "temperature": _TEMPERATURE,
            },
        )
        text = _safe_response_text(resp).strip()
        if text:
            return text
    except Exception:
        # Fallthrough to generate a fallback summary if API fails
        pass

    # Fallback: minimal textual info if API fails or returns empty
    parts = []
    if question: parts.append(f"Question: {question}")
    if metrics: parts.append(f"Metrics: {list(metrics.keys())[:5]}")
    return " ".join(parts) or "No textual summary available."


# ---------------------------------------------------------------------------
# Fused: Generate Code + Summary in a Single Call
# ---------------------------------------------------------------------------

def generate_code_and_summary(
    question: str,
    schema_snippet: str,
    sample_rows: list[dict],
    row_limit: int = 200
) -> tuple[str, str]:
    """
    Return (code, summary) using a single Gemini call, with a one-time repair loop.
    """
    fused = os.getenv("GEMINI_FUSED", "0").lower() not in ("0", "false", "no")
    if not fused:
        code = generate_analysis_code(question, schema_snippet, sample_rows, row_limit=row_limit)
        return code, "Analysis planned. Executed results will follow."

    model = _ensure_model()
    sample_preview = sample_rows[: min(len(sample_rows), 10)]

    prompt = (
        "You are an expert Python data analyst.\n\n"
        "Provide one fenced Python code block implementing `def run(df, ctx):`.\n\n"
        "CODE REQUIREMENTS (STRICT):\n"
        "- MUST return a dict with EXACT keys: 'table' (list[dict]), 'metrics' (dict), 'chartData' (kind, labels, series).\n"
        "- Do NOT return 'tables' or 'charts'.\n"
        "- Respect ctx.get('row_limit', 200). Use robust numeric handling; avoid complex dtype.\n"
        "- Allowed imports: pandas, numpy, matplotlib, seaborn, math, statistics, json, io, "
        "itertools, functools, collections, re, datetime, base64.\n\n"
        f"--- DATA CONTEXT ---\nSchema: {schema_snippet}\nSample rows: {sample_preview}\n\n"
        f"--- QUESTION ---\n\"{question}\"\n\n"
        "Return only the Python code block."
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "max_output_tokens": _MAX_TOKENS,
            "temperature": _TEMPERATURE,
        },
    )

    text = _safe_response_text(resp)
    code = _extract_code_block(text)
    
    # One-time repair attempt if initial extraction fails
    if not code:
        feedback_prompt = (
            "Your previous response was not formatted correctly. "
            f"Please regenerate the response for the following question: \"{question}\"\n\n"
            "Return a one-sentence summary, then a single, valid, fenced Python code block "
            "defining `def run(df, ctx):`."
        )
        retry_resp = model.generate_content(
            feedback_prompt,
            generation_config={
                "max_output_tokens": _MAX_TOKENS,
                "temperature": 0.0, # Use 0 temp for deterministic repair
            },
        )
        text = _safe_response_text(retry_resp)
        code = _extract_code_block(text)

    # If code extraction still fails, return the raw text for the orchestrator's repair loop.
    if not code:
        return "", text

    summary = "Analysis planned. Executed results will follow."
    return code, summary


# ---------------------------------------------------------------------------
# Repair Code given Runtime Error
# ---------------------------------------------------------------------------

def repair_code(
    question: str,
    schema_snippet: str,
    sample_rows: list[dict],
    previous_code: str,
    runtime_error: str,
    row_limit: int = 200,
) -> str:
    """Ask Gemini to repair previously generated code given a runtime error message."""
    model = _ensure_model()
    sample_preview = sample_rows[: min(len(sample_rows), 10)]

    prompt = (
        "You previously wrote Python analysis code which raised a runtime error. "
        "Repair the code. Maintain the same intent, and follow these strict rules.\n\n"
        "RULES (STRICT):\n"
        "- Implement `def run(df, ctx):` and return a dict with EXACT keys: 'table' (list[dict]), 'metrics' (dict), 'chartData'.\n"
        "- Do NOT return 'tables' or 'charts'. Use 'table' and 'chartData' only.\n"
        "- Respect ctx.get('row_limit', 200) for the size of 'table'.\n"
        "- Use robust numeric handling: prefer pd.to_numeric(..., errors='coerce'), select_dtypes(include=[np.number]).\n"
        "- Never use complex dtype or astype(complex).\n"
        "- Allowed imports: pandas, numpy, matplotlib, seaborn, math, statistics, json, io, "
        "itertools, functools, collections, re, datetime, base64.\n\n"
        f"RUNTIME ERROR: {runtime_error}\n\n"
        f"PREVIOUS CODE:\n```python\n{previous_code}\n```\n\n"
        f"SCHEMA:\n{schema_snippet}\n\n"
        f"SAMPLE ROWS:\n{sample_preview}\n\n"
        f"USER QUESTION:\n\"{question}\"\n\n"
        "Return only the repaired Python code block."
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "max_output_tokens": _MAX_TOKENS,
            "temperature": 0.0,
        },
    )
    text = _safe_response_text(resp)
    code = _extract_code_block(text)
    if not code or not re.search(r"def\s+run\s*\(", code):
        raise RuntimeError("REPAIR_FAILED: Missing valid 'def run(df, ctx):' implementation.")
    return code


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _safe_response_text(resp) -> str:
    """Safely extract text from Gemini responses, handling potential exceptions."""
    try:
        if getattr(resp, "text", None):
            return resp.text or ""
        if getattr(resp, "candidates", None):
            for c in resp.candidates:
                content = getattr(c, "content", None)
                parts = getattr(content, "parts", None) if content else None
                if parts:
                    return "".join(p.text for p in parts if hasattr(p, "text"))
    except Exception:
        return ""
    return ""


def _extract_code_block(text: str) -> str:
    """Extract a Python code block robustly from a response."""
    # 1. Prefer fenced python block
    m = re.search(r"```python\s*(.*?)```", text, flags=re.DOTALL | re.IGNORECASE)
    if m:
        code = m.group(1).strip()
        if re.search(r"def\s+run\s*\(", code):
            return code

    # 2. Fallback to any fenced block
    m = re.search(r"```(.*?)```", text, flags=re.DOTALL)
    if m:
        code = m.group(1).strip()
        if re.search(r"def\s+run\s*\(", code):
            return code

    # 3. Heuristic fallback: find the start of the function definition
    m = re.search(r"(def\s+run\s*\(.*)", text, flags=re.DOTALL)
    if m:
        return m.group(1).strip()

    return ""
</file>

<file path="functions/orchestrator/main.py">
import json
import os
import time
import uuid
import subprocess
import sys
from datetime import datetime, timezone, timedelta
from typing import Generator, Iterable

import functions_framework
from flask import Request, Response
from google.cloud import firestore
from google.cloud import storage
import pandas as pd
import base64
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout
import pyarrow as pa  # type: ignore
import pyarrow.parquet as pq  # type: ignore

import firebase_admin
from firebase_admin import auth as fb_auth
import google.auth
from google.auth import impersonated_credentials
import google.auth.transport.requests

import gemini_client
import sandbox_runner
from google.api_core import exceptions as gax_exceptions  # type: ignore

# Configuration
PROJECT_ID = os.getenv("GCP_PROJECT", "ai-data-analyser")
FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PING_INTERVAL_SECONDS = int(os.getenv("SSE_PING_INTERVAL_SECONDS", "22"))
HARD_TIMEOUT_SECONDS = int(os.getenv("CHAT_HARD_TIMEOUT_SECONDS", "60"))
REPAIR_TIMEOUT_SECONDS = int(os.getenv("CHAT_REPAIR_TIMEOUT_SECONDS", "30"))
ORCH_IPC_MODE = os.getenv("ORCH_IPC_MODE", "base64").lower()
RUNTIME_SERVICE_ACCOUNT = os.getenv("RUNTIME_SERVICE_ACCOUNT")

ALLOWED_ORIGINS = {
    o.strip()
    for o in (os.getenv(
        "ALLOWED_ORIGINS",
        "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com",
    ) or "").split(",")
    if o.strip()
}

# Firebase Admin SDK Initialization
try:
    firebase_admin.get_app()
except ValueError:
    firebase_admin.initialize_app()


def _origin_allowed(origin: str | None) -> bool:
    return origin in ALLOWED_ORIGINS if origin else False


_CACHED_SIGNING_CREDS = None
_CACHED_EXPIRES_AT = 0.0

def _impersonated_signing_credentials(sa_email: str | None):
    """Creates and caches impersonated credentials for signing URLs."""
    global _CACHED_SIGNING_CREDS, _CACHED_EXPIRES_AT
    now = time.time()
    if _CACHED_SIGNING_CREDS and now < _CACHED_EXPIRES_AT:
        return _CACHED_SIGNING_CREDS

    source_creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    if not sa_email:
        creds = source_creds
    else:
        creds = impersonated_credentials.Credentials(
            source_credentials=source_creds,
            target_principal=sa_email,
            target_scopes=["https://www.googleapis.com/auth/cloud-platform"],
            lifetime=3600,
        )
    _CACHED_SIGNING_CREDS = creds
    _CACHED_EXPIRES_AT = now + 3300  # ~55m
    return _CACHED_SIGNING_CREDS


def _sign_gs_uri(gs_uri: str, minutes: int = 15) -> str:
    """Returns a signed HTTPS URL for a gs:// URI."""
    if not gs_uri or not gs_uri.startswith("gs://"):
        return gs_uri
    try:
        bucket_name, blob_path = gs_uri[5:].split("/", 1)
        storage_client = storage.Client(project=PROJECT_ID)
        blob = storage_client.bucket(bucket_name).blob(blob_path)
        signing_creds = _impersonated_signing_credentials(RUNTIME_SERVICE_ACCOUNT)
        return blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=minutes),
            method="GET",
            credentials=signing_creds,
        )
    except Exception:
        return gs_uri


def _sse_format(obj: dict) -> str:
    """Formats a dictionary as a Server-Sent Event string."""
    return f"data: {json.dumps(obj, ensure_ascii=False)}\n\n"


def _events(session_id: str, dataset_id: str, uid: str, question: str) -> Iterable[str]:
    """Generator function for the main SSE event stream."""
    yield _sse_format({"type": "received", "data": {"sessionId": session_id, "datasetId": dataset_id}})

    # Setup GCS and Firestore clients
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(FILES_BUCKET)
    fs = firestore.Client(project=PROJECT_ID)

    # Fetch payload.json for schema and sample data
    payload_obj = {}
    try:
        payload_gcs_path = f"users/{uid}/sessions/{session_id}/datasets/{dataset_id}/metadata/payload.json"
        payload_blob = bucket.blob(payload_gcs_path)
        payload_obj = json.loads(payload_blob.download_as_text())
    except Exception as e:
        yield _sse_format({"type": "error", "data": {"code": "PAYLOAD_READ_FAILED", "message": f"Could not read metadata: {e}"}})
        return

    schema_snippet = json.dumps(payload_obj.get("columns", {}))[:1000]
    sample_rows = payload_obj.get("sample_rows", [])[:10]

    # --- Main Generation and Validation Loop ---
    yield _sse_format({"type": "generating_code"})
    code, is_valid, validation_errors, warnings = "", False, ["Code generation failed."], []
    
    max_retries = 2
    for attempt in range(max_retries):
        try:
            raw_code, llm_response_text = gemini_client.generate_code_and_summary(question, schema_snippet, sample_rows)
            
            if not raw_code:
                # If code extraction fails, use the raw response for the repair prompt
                validation_errors = [f"LLM did not return a valid code block. Response: {llm_response_text[:200]}"]
                question = f"The previous attempt failed. Please fix it. The error was: {validation_errors[0]}. Original question: {question}"
                continue # Retry

            # ✅ FIX 1: Unpack all three return values from the validator
            is_valid, validation_errors, warnings = sandbox_runner.validate_code(raw_code)
            
            if is_valid:
                code = raw_code
                break # Success
            else:
                # If validation fails, use the errors for the repair prompt
                question = f"The previous code failed validation. Please fix it. Errors: {'; '.join(validation_errors)}. Original question: {question}"

        except Exception as e:
            validation_errors = [f"An unexpected error occurred during code generation: {e}"]

    if not is_valid or not code:
        yield _sse_format({"type": "error", "data": {"code": "CODE_VALIDATION_FAILED", "message": "; ".join(validation_errors)}})
        return
    
    # --- Emit the validated code so the UI can display it (even if execution fails) ---
    try:
        yield _sse_format({
            "type": "code",
            "data": {
                "language": "python",
                "text": code,
                "warnings": (warnings or [])
            }
        })
    except Exception:
        # Non-fatal: continue workflow even if emitting this event fails
        pass

    # --- Execute the validated code (with one-time repair on failure) ---
    yield _sse_format({"type": "running_fast"})
    parquet_gcs_path = f"users/{uid}/sessions/{session_id}/datasets/{dataset_id}/cleaned/cleaned.parquet"
    try:
        parquet_blob = bucket.blob(parquet_gcs_path)
        parquet_bytes = parquet_blob.download_as_bytes()
        parquet_b64 = base64.b64encode(parquet_bytes).decode("ascii")
    except Exception as e:
        yield _sse_format({"type": "error", "data": {"code": "DATA_READ_FAILED", "message": str(e)}})
        return

    def _run_once(code_to_run: str) -> dict:
        worker_path = os.path.join(os.path.dirname(__file__), "worker.py")
        proc = subprocess.run(
            [sys.executable, worker_path],
            input=json.dumps({
                "code": code_to_run,
                "parquet_b64": parquet_b64,
                "ctx": {"question": question, "row_limit": 200},
            }).encode("utf-8"),
            capture_output=True,
            timeout=HARD_TIMEOUT_SECONDS,
        )
        if proc.returncode != 0:
            raise RuntimeError(f"Worker process failed: {proc.stderr.decode('utf-8', errors='ignore')}")
        return json.loads(proc.stdout)

    tried_repair = False
    try:
        result = _run_once(code)
        if result.get("error"):
            raise RuntimeError(f"Execution error: {result['error']}")
    except subprocess.TimeoutExpired:
        yield _sse_format({"type": "error", "data": {"code": "TIMEOUT_HARD", "message": f"Execution timed out after {HARD_TIMEOUT_SECONDS}s"}})
        return
    except Exception as e_first:
        # Attempt a single repair using the runtime error
        try:
            tried_repair = True
            yield _sse_format({"type": "repairing"})
            # Bound the repair step to avoid indefinite hangs
            with ThreadPoolExecutor(max_workers=1) as ex:
                future = ex.submit(gemini_client.repair_code, question, schema_snippet, sample_rows, code, str(e_first))
                try:
                    repaired = future.result(timeout=REPAIR_TIMEOUT_SECONDS)
                except FuturesTimeout:
                    yield _sse_format({"type": "error", "data": {"code": "REPAIR_TIMEOUT", "message": f"Repair step timed out after {REPAIR_TIMEOUT_SECONDS}s"}})
                    return
            ok2, errs2, warns2 = sandbox_runner.validate_code(repaired)
            if not ok2:
                yield _sse_format({"type": "error", "data": {"code": "CODE_VALIDATION_FAILED", "message": "; ".join(errs2)}})
                return
            code = repaired
            warnings = warns2
            # Emit updated code for the UI
            try:
                yield _sse_format({
                    "type": "code",
                    "data": {"language": "python", "text": code, "warnings": (warnings or [])}
                })
            except Exception:
                pass
            # Re-run once
            yield _sse_format({"type": "running_fast"})
            result = _run_once(code)
            if result.get("error"):
                raise RuntimeError(f"Execution error: {result['error']}")
        except subprocess.TimeoutExpired:
            yield _sse_format({"type": "error", "data": {"code": "TIMEOUT_HARD", "message": f"Execution timed out after {HARD_TIMEOUT_SECONDS}s"}})
            return
        except Exception as e_second:
            # Final failure after repair attempt
            yield _sse_format({"type": "error", "data": {"code": "EXEC_FAILED", "message": str(e_second)}})
            return

    # ✅ FIX 2: Correct key names (singular, not plural)
    message_id = str(uuid.uuid4())
    table = result.get("table", [])  # "table" not "tables"
    chart_data = result.get("chartData", {})  # "chartData" not "charts"
    metrics = result.get("metrics", {})
    
    yield _sse_format({"type": "summarizing"})
    summary = result.get("summary") or gemini_client.generate_summary(question, table[:5], metrics)
    
    # ✅ FIX 3: Add actual persistence logic
    yield _sse_format({"type": "persisting"})
    
    results_prefix = f"users/{uid}/sessions/{session_id}/results/{message_id}"
    table_path = f"{results_prefix}/table.json"
    metrics_path = f"{results_prefix}/metrics.json"
    chart_path = f"{results_prefix}/chart_data.json"
    summary_path = f"{results_prefix}/summary.json"
    
    try:
        table_blob = bucket.blob(table_path)
        metrics_blob = bucket.blob(metrics_path)
        chart_blob = bucket.blob(chart_path)
        summary_blob = bucket.blob(summary_path)
        
        table_data = json.dumps({"rows": table}, ensure_ascii=False).encode("utf-8")
        metrics_data = json.dumps(metrics, ensure_ascii=False).encode("utf-8")
        chart_data_json = json.dumps(chart_data, ensure_ascii=False).encode("utf-8")
        summary_data = json.dumps({"text": summary}, ensure_ascii=False).encode("utf-8")
        
        # Upload in parallel
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [
                executor.submit(table_blob.upload_from_string, table_data, content_type="application/json"),
                executor.submit(metrics_blob.upload_from_string, metrics_data, content_type="application/json"),
                executor.submit(chart_blob.upload_from_string, chart_data_json, content_type="application/json"),
                executor.submit(summary_blob.upload_from_string, summary_data, content_type="application/json"),
            ]
            for f in futures:
                f.result()
        
        # Generate signed URLs for frontend
        table_url = _sign_gs_uri(f"gs://{FILES_BUCKET}/{table_path}")
        metrics_url = _sign_gs_uri(f"gs://{FILES_BUCKET}/{metrics_path}")
        chart_url = _sign_gs_uri(f"gs://{FILES_BUCKET}/{chart_path}")
        summary_url = _sign_gs_uri(f"gs://{FILES_BUCKET}/{summary_path}")
        
    except Exception as e:
        yield _sse_format({"type": "error", "data": {"code": "PERSIST_FAILED", "message": str(e)}})
        return
    
    # Final 'done' event with URLs
    yield _sse_format({
        "type": "done",
        "data": {
            "messageId": message_id,
            "summary": summary,
            "tableSample": table[:50],  # Now works correctly
            "chartData": chart_data,
            "metrics": metrics,
            "uris": {
                "table": table_url,
                "metrics": metrics_url,
                "chartData": chart_url,
                "summary": summary_url
            }
        }
    })


@functions_framework.http
def chat(request: Request) -> Response:
    """HTTP entry point for the chat Cloud Function."""
    origin = request.headers.get("Origin") or ""
    if request.method == "OPTIONS":
        if not _origin_allowed(origin): return ("Origin not allowed", 403)
        headers = {
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Session-Id",
            "Access-Control-Max-Age": "3600",
        }
        return ("", 204, headers)

    try:
        if not _origin_allowed(origin):
            return Response(json.dumps({"error": "origin not allowed"}), 403, mimetype="application/json")

        auth_header = request.headers.get("Authorization", "")
        token = auth_header.split(" ", 1)[1] if auth_header.lower().startswith("bearer ") else None
        if not token:
            return Response(json.dumps({"error": "missing token"}), 401, mimetype="application/json")
        
        decoded = fb_auth.verify_id_token(token)
        uid = decoded["uid"]
        
        payload = request.get_json(silent=True) or {}
        session_id = payload.get("sessionId")
        dataset_id = payload.get("datasetId")
        question = payload.get("question", "")
        
        if not all([session_id, dataset_id, uid]):
            return Response(json.dumps({"error": "missing sessionId or datasetId"}), 400, mimetype="application/json")

        headers = {
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": origin,
        }
        return Response(_events(session_id, dataset_id, uid, question), headers=headers)

    except Exception as e:
        return Response(json.dumps({"error": "internal error", "detail": str(e)}), 500, mimetype="application/json")
</file>

<file path="functions/orchestrator/requirements.txt">
functions-framework==3.5.0
google-cloud-firestore==2.16.0
google-cloud-storage==2.16.0
google-cloud-secret-manager==2.20.2
flask==3.0.3
pandas==2.2.2
numpy==2.0.2
pyarrow==16.1.0
google-generativeai==0.7.2
firebase-admin==6.5.0
matplotlib==3.8.4
seaborn==0.13.2
</file>

<file path="functions/orchestrator/sandbox_runner.py">
"""
AST-based validation for LLM-generated analysis code.

This module statically validates dynamically generated Python analysis code to
enforce security and sandbox constraints before execution.

Checks performed:
- The presence of a required function signature: `def run(df, ctx):`
- Imports restricted to an allowlist (configurable via SANDBOX_MODE).
- Disallowed function calls (e.g., exec, open, eval).
- Disallowed attribute or name access (dunder methods, globals).
- Optional loop-depth and import-count limits (prevent runaway code).

Environment variable:
    SANDBOX_MODE = "restricted" | "rich"   (default: restricted)
"""

from __future__ import annotations
import ast
import os
from typing import Iterable, Tuple, Dict, List

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

_SANDBOX_MODE = os.getenv("SANDBOX_MODE", "restricted").lower()

# Minimal baseline imports for restricted environments
ALLOWED_IMPORTS_BASE = {"pandas", "numpy", "math", "json"}

# Rich mode adds analysis and visualization support
ALLOWED_IMPORTS_RICH = {
    "matplotlib", "seaborn", "statistics", "io", "itertools", "functools",
    "collections", "re", "datetime", "base64",
}

ALLOWED_IMPORTS = set(ALLOWED_IMPORTS_BASE)
if _SANDBOX_MODE in ("rich", "extended"):
    ALLOWED_IMPORTS.update(ALLOWED_IMPORTS_RICH)

# Dangerous constructs and module prefixes
FORBIDDEN_NAMES = {"exec", "eval", "compile", "open", "__import__", "globals", "locals", "input"}

FORBIDDEN_MODULE_PREFIXES = {
    "os", "sys", "subprocess", "socket", "asyncio", "multiprocessing",
    "threading", "ctypes", "pathlib", "importlib", "pdb", "pickle",
    "dill", "requests", "urllib",
}

# Safety thresholds
MAX_IMPORTS = 12
MAX_LOOP_DEPTH = 4


# ---------------------------------------------------------------------------
# AST Validator
# ---------------------------------------------------------------------------

class _Validator(ast.NodeVisitor):
    """AST walker enforcing import, call, and naming safety rules."""

    def __init__(self, allowlist: Iterable[str]):
        super().__init__()
        self.allowlist = set(allowlist)
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.import_count = 0
        self.loop_depth = 0

    def _err(self, msg: str) -> None:
        self.errors.append(msg)

    def _warn(self, msg: str) -> None:
        self.warnings.append(msg)

    # -------------------------------
    # Helpers: complex dtype detection
    # -------------------------------
    def _node_contains_complex(self, node: ast.AST) -> bool:
        """Return True if the AST node subtree refers to complex dtype.

        Matches any of the following patterns:
        - Name 'complex' (built-in complex)
        - Attribute whose attr startswith 'complex' (e.g., np.complex, complex64, complex128)
        - Constant string containing 'complex' (e.g., "complex128")
        """
        for sub in ast.walk(node):
            if isinstance(sub, ast.Name) and sub.id == "complex":
                return True
            if isinstance(sub, ast.Attribute) and isinstance(sub.attr, str) and sub.attr.lower().startswith("complex"):
                return True
            if isinstance(sub, ast.Constant) and isinstance(sub.value, str) and "complex" in sub.value.lower():
                return True
        return False

    # -------------------------------
    # Import Validation
    # -------------------------------
    def visit_Import(self, node: ast.Import) -> None:
        self.import_count += 1
        for alias in node.names:
            root = (alias.name or "").split(".")[0]
            if root not in self.allowlist:
                self._err(f"Import not allowed: {alias.name}")
            if root in FORBIDDEN_MODULE_PREFIXES:
                self._err(f"Forbidden import: {alias.name}")
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        self.import_count += 1
        mod = node.module or ""
        root = mod.split(".")[0]
        if root not in self.allowlist:
            self._err(f"Import from not allowed: {mod}")
        if any(root == p or root.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
            self._err(f"Forbidden import from: {mod}")
        self.generic_visit(node)

    # -------------------------------
    # Function & Call Validation
    # -------------------------------
    def visit_Call(self, node: ast.Call) -> None:
        # Forbid dangerous builtins
        if isinstance(node.func, ast.Name) and node.func.id in FORBIDDEN_NAMES:
            self._err(f"Forbidden call: {node.func.id}")

        # Forbid direct usage of complex() builtin or np.complex* as a callable
        if (isinstance(node.func, ast.Name) and node.func.id == "complex") or (
            isinstance(node.func, ast.Attribute) and isinstance(node.func.attr, str) and node.func.attr.lower().startswith("complex")
        ):
            self._err("Complex dtype is not allowed. Use float via pd.to_numeric(..., errors='coerce') instead.")

        # Forbid dtype=complex or astype(complex)
        # 1) dtype keyword anywhere
        for kw in getattr(node, "keywords", []) or []:
            if kw.arg == "dtype" and kw.value is not None and self._node_contains_complex(kw.value):
                self._err("Complex dtype is not allowed (dtype=complex).")

        # 2) astype(complex) or astype(np.complex*) patterns
        if isinstance(node.func, ast.Attribute) and node.func.attr == "astype":
            # check positional first arg
            if node.args:
                if self._node_contains_complex(node.args[0]):
                    self._err("Complex dtype is not allowed (astype(complex)).")
            # or keyword dtype
            for kw in getattr(node, "keywords", []) or []:
                if kw.arg == "dtype" and kw.value is not None and self._node_contains_complex(kw.value):
                    self._err("Complex dtype is not allowed (astype(dtype=complex)).")
        self.generic_visit(node)

    def visit_Attribute(self, node: ast.Attribute) -> None:
        if node.attr.startswith("__") and node.attr.endswith("__"):
            self._err("Use of dunder attributes is not allowed")
        self.generic_visit(node)

    def visit_Name(self, node: ast.Name) -> None:
        if node.id.startswith("__") and node.id.endswith("__"):
            self._err("Use of dunder names is not allowed")
        self.generic_visit(node)

    # -------------------------------
    # Structural Safety Checks
    # -------------------------------
    def visit_For(self, node: ast.For) -> None:
        self.loop_depth += 1
        if self.loop_depth > MAX_LOOP_DEPTH:
            self._warn(f"Deeply nested loop detected (depth {self.loop_depth})")
        self.generic_visit(node)
        self.loop_depth -= 1

    def visit_While(self, node: ast.While) -> None:
        self.loop_depth += 1
        if self.loop_depth > MAX_LOOP_DEPTH:
            self._warn(f"Deeply nested loop detected (depth {self.loop_depth})")
        self.generic_visit(node)
        self.loop_depth -= 1


# ---------------------------------------------------------------------------
# Public Validation API
# ---------------------------------------------------------------------------

def validate_code(
    code: str,
    allowlist: Iterable[str] | None = None
) -> Tuple[bool, List[str], List[str]]:
    """
    Validate Python code against structural and security rules.

    Returns:
        (is_valid, errors, warnings)
    """
    if not code or not isinstance(code, str):
        return False, ["Empty or invalid code string."], []

    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return False, [f"SyntaxError: {e}"], []

    # Ensure required entrypoint exists
    has_run_func = False
    for node in tree.body:
        if isinstance(node, ast.FunctionDef) and node.name == "run":
            args = node.args.args
            if len(args) >= 2 and args[0].arg == "df" and args[1].arg == "ctx":
                has_run_func = True
                break
    if not has_run_func:
        return False, ["Missing required function: def run(df, ctx):"], []

    # Determine which allowlist to apply
    allowlist_to_use = set(allowlist) if allowlist else set(ALLOWED_IMPORTS)
    validator = _Validator(allowlist_to_use)
    validator.visit(tree)

    # Apply import/loop sanity warnings
    if validator.import_count > MAX_IMPORTS:
        validator._warn(f"Too many imports ({validator.import_count} > {MAX_IMPORTS})")

    ok = len(validator.errors) == 0
    return ok, validator.errors, validator.warnings


def structured_validate(code: str) -> Dict[str, any]:
    """
    Return a structured dict for downstream use (e.g., LLM repair loop).
    """
    ok, errors, warnings = validate_code(code)
    return {
        "ok": ok,
        "errors": errors,
        "warnings": warnings,
        "mode": _SANDBOX_MODE,
        "allowed_imports": sorted(list(ALLOWED_IMPORTS)),
    }


# ---------------------------------------------------------------------------
# Stub Execution (compatibility placeholder)
# ---------------------------------------------------------------------------

def run_user_code_stub() -> dict:
    """Simple placeholder result for systems not yet executing code."""
    return {
        "table": [{"category": "A", "value": 1}, {"category": "B", "value": 2}],
        "metrics": {},
        "chartData": {
            "kind": "bar",
            "labels": ["A", "B"],
            "series": [{"label": "Value", "data": [1, 2]}],
        },
        "message": f"Sandbox validation ready (mode={_SANDBOX_MODE}). Execution not yet implemented.",
    }
</file>

<file path="functions/orchestrator/worker.py">
#!/usr/bin/env python3
"""
Worker process to safely execute LLM-generated analysis code in a sandboxed environment.

This script is launched as a subprocess by the orchestrator. It receives JSON via stdin:
{
  "code": "<python source>",
  "parquet_b64": "<base64 bytes>" | optional,
  "arrow_ipc_b64": "<base64 bytes>" | optional,
  "parquet_path": "/tmp/cleaned.parquet" | optional,
  "ctx": { ... }
}

Steps:
1. Validate code with sandbox_runner.
2. Load the dataset into a DataFrame.
3. Execute the validated run(df, ctx) safely.
4. Sanitize and normalize the result for JSON output.

It always prints a JSON payload to stdout and exits with 0 unless the payload is malformed.
"""
from __future__ import annotations

import io
import json
import base64
import sys
import signal
import traceback
import os
from typing import Any

import pandas as pd
import numpy as np
import pyarrow as pa  # type: ignore

import matplotlib
matplotlib.use("Agg")  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

# Try to import sandbox validator (preferred)
try:
    from sandbox_runner import structured_validate, ALLOWED_IMPORTS as SANDBOX_ALLOWED_IMPORTS
except Exception:
    SANDBOX_ALLOWED_IMPORTS = {
        "pandas", "numpy", "matplotlib", "seaborn",
        "math", "statistics", "json", "io", "itertools", "functools",
        "collections", "re", "datetime", "base64"
    }
    structured_validate = None  # fallback


# --------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------
ALLOWED_IMPORTS = set(SANDBOX_ALLOWED_IMPORTS)
CODE_TIMEOUT = int(os.getenv("CODE_TIMEOUT", "60"))
MAX_MEMORY_BYTES = int(os.getenv("CODE_MAX_MEMORY_BYTES", str(512 * 1024 * 1024)))  # 512MB
try:
    import resource
except Exception:
    resource = None


# --------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------
def sanitize_for_json(obj: Any) -> Any:
    """Recursively replaces NaN/Inf with None for Firestore/JSON compatibility."""
    if isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [sanitize_for_json(v) for v in obj]
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    return obj


def _safe_import(name, globals=None, locals=None, fromlist=(), level=0):
    root = (name.split(".") or [name])[0]
    if root not in ALLOWED_IMPORTS:
        raise ImportError(f"Import not allowed: {name}")
    return _orig_import(name, globals, locals, fromlist, level)


def _prepare_globals() -> dict:
    """Prepare restricted globals for execution."""
    import builtins as _builtins
    safe_builtins = {
        b: getattr(_builtins, b)
        for b in [
            "abs", "all", "any", "bool", "dict", "enumerate", "filter",
            "float", "int", "len", "list", "map", "max", "min", "pow",
            "range", "round", "set", "slice", "sorted", "str", "sum",
            "zip", "print", "isinstance", "getattr", "hasattr", "type",
        ]
        if hasattr(_builtins, b)
    }
    safe_builtins["__import__"] = _safe_import
    return {"__builtins__": safe_builtins, "pd": pd, "np": np, "plt": plt, "sns": sns, "RESULT": None}


def _set_resource_limits():
    """Apply memory and CPU limits (POSIX only)."""
    if resource is None:
        return
    try:
        resource.setrlimit(resource.RLIMIT_AS, (MAX_MEMORY_BYTES, MAX_MEMORY_BYTES))
    except Exception:
        pass
    try:
        resource.setrlimit(resource.RLIMIT_CPU, (CODE_TIMEOUT + 5, CODE_TIMEOUT + 5))
    except Exception:
        pass


class _TimeoutException(Exception):
    pass


def _timeout_handler(signum, frame):
    raise _TimeoutException("User code timed out")


def _load_dataframe(payload: dict) -> pd.DataFrame:
    """Load df from base64 Parquet/Arrow or path."""
    if payload.get("arrow_ipc_b64"):
        ipc_bytes = base64.b64decode(payload["arrow_ipc_b64"])
        with pa.ipc.open_stream(io.BytesIO(ipc_bytes)) as reader:
            table = reader.read_all()
        return table.to_pandas()
    if payload.get("parquet_b64"):
        data = base64.b64decode(payload["parquet_b64"])
        return pd.read_parquet(io.BytesIO(data))
    if payload.get("parquet_path"):
        return pd.read_parquet(payload["parquet_path"])
    raise ValueError("Missing data payload: no parquet_b64, arrow_ipc_b64, or parquet_path provided")


def _fallback_result(df: pd.DataFrame, ctx: dict) -> dict:
    """Fallback minimal result when code fails."""
    row_limit = int((ctx or {}).get("row_limit", 200))
    return {
        "table": df.head(row_limit).to_dict(orient="records"),
        "metrics": {"rows": len(df), "columns": len(df.columns)},
        "chartData": {},
        "message": "Fallback result generated due to code execution failure."
    }


# --------------------------------------------------------------------------
# Main Execution
# --------------------------------------------------------------------------
def main() -> int:
    global _orig_import
    import builtins as _builtins
    _orig_import = _builtins.__import__

    # Step 1: Read payload
    try:
        payload = json.load(sys.stdin)
        code = payload.get("code", "")
        ctx = payload.get("ctx", {}) or {}
        if not code:
            raise ValueError("Missing 'code' field in payload")
    except Exception as e:
        sys.stderr.write(f"Invalid input payload: {e}\n")
        return 1

    # Step 2: Validate code via sandbox_runner
    try:
        if structured_validate:
            validation = structured_validate(code)
            if not validation.get("ok", False):
                output = {
                    "table": [],
                    "metrics": {},
                    "chartData": {},
                    "error": "Validation failed",
                    "validation": validation,
                }
                print(json.dumps(output, ensure_ascii=False))
                return 0
    except Exception as e:
        output = {
            "table": [],
            "metrics": {},
            "chartData": {},
            "error": f"Validator error: {e}",
        }
        print(json.dumps(output, ensure_ascii=False))
        return 0

    # Step 3: Load DataFrame
    try:
        df = _load_dataframe(payload)
    except Exception as e:
        output = {"table": [], "metrics": {}, "chartData": {}, "error": f"Failed to load data: {e}"}
        print(json.dumps(output, ensure_ascii=False))
        return 0

    # Step 4: Execute code safely
    globs = _prepare_globals()
    locs: dict = {}

    _set_resource_limits()
    old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
    signal.alarm(CODE_TIMEOUT)

    try:
        compiled = compile(code, filename="<user_code>", mode="exec")
        exec(compiled, globs, locs)

        run_func = locs.get("run") or globs.get("run")
        if not callable(run_func):
            raise RuntimeError("Missing required function: def run(df, ctx):")

        result = run_func(df, ctx)
        if result is None:
            result = globs.get("RESULT")

        # Normalize
        if isinstance(result, pd.DataFrame):
            result = {"table": result.to_dict(orient="records"), "metrics": {}, "chartData": {}}
        elif isinstance(result, list):
            result = {"table": result, "metrics": {}, "chartData": {}}
        elif not isinstance(result, dict):
            result = _fallback_result(df, ctx)

        # Map plural keys to canonical ones when needed
        if isinstance(result, dict):
            # tables -> table (choose first reasonable table)
            if "table" not in result and "tables" in result:
                tables = result.get("tables")
                table_rows = None
                try:
                    if isinstance(tables, list) and len(tables) > 0:
                        first = tables[0]
                        if isinstance(first, pd.DataFrame):
                            table_rows = first.to_dict(orient="records")
                        elif isinstance(first, list):
                            table_rows = first
                        elif isinstance(first, dict):
                            table_rows = [first]
                    elif isinstance(tables, dict) and len(tables) > 0:
                        for v in tables.values():
                            if isinstance(v, pd.DataFrame):
                                table_rows = v.to_dict(orient="records")
                                break
                            elif isinstance(v, list):
                                table_rows = v
                                break
                            elif isinstance(v, dict):
                                table_rows = [v]
                                break
                except Exception:
                    table_rows = None
                if table_rows is not None:
                    result["table"] = table_rows

            # charts -> chartData (choose first chart-like dict)
            if "chartData" not in result and "charts" in result:
                charts = result.get("charts")
                chosen = None
                if isinstance(charts, dict):
                    chosen = charts
                elif isinstance(charts, list) and len(charts) > 0:
                    chosen = charts[0]
                if isinstance(chosen, dict):
                    result["chartData"] = chosen

        # Ensure required keys
        result.setdefault("table", df.head(int(ctx.get("row_limit", 200))).to_dict(orient="records"))
        result.setdefault("metrics", {"rows": len(df), "columns": len(df.columns)})
        result.setdefault("chartData", {})

        sanitized = sanitize_for_json(result)
        print(json.dumps(sanitized, ensure_ascii=False))
        return 0

    except _TimeoutException:
        output = {"table": [], "metrics": {}, "chartData": {}, "error": f"Execution timed out after {CODE_TIMEOUT}s."}
        print(json.dumps(output, ensure_ascii=False))
        return 0

    except Exception as e:
        tb = traceback.format_exc(limit=8)
        output = {
            "table": [],
            "metrics": {},
            "chartData": {},
            "error": f"Runtime error: {e}",
            "traceback": tb,
        }
        print(json.dumps(output, ensure_ascii=False))
        return 0

    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)


if __name__ == "__main__":
    raise SystemExit(main())
</file>

<file path="functions/sign_upload_url/main.py">
import os
import json
import uuid
from datetime import datetime, timedelta, timezone
from typing import Tuple
import time
import re

from google.cloud import storage
from google.cloud import firestore
import google.auth
from google.auth import impersonated_credentials
import google.auth.transport.requests
import firebase_admin
from firebase_admin import auth as fb_auth

MAX_FILE_BYTES = 20 * 1024 * 1024  # 20 MB
FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PROJECT_ID = os.getenv("GCP_PROJECT", os.getenv("GOOGLE_CLOUD_PROJECT", "ai-data-analyser"))
TTL_DAYS = int(os.getenv("TTL_DAYS", "1"))
RUNTIME_SERVICE_ACCOUNT = os.getenv("RUNTIME_SERVICE_ACCOUNT")
SIGNING_CREDS_TTL_SECONDS = int(os.getenv("SIGNING_CREDS_TTL_SECONDS", "3300"))  # ~55m

ALLOWED_ORIGINS = {
    o.strip()
    for o in re.split(r'[,;]', os.getenv(
        "ALLOWED_ORIGINS",
        "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com",
    ) or "")
    if o.strip()
}

ALLOWED_MIME = {
    "text/csv": ".csv",
    "application/vnd.ms-excel": ".xls",
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",
}
ALLOWED_EXT = {".csv", ".xls", ".xlsx"}


def _ext_from_filename_or_type(filename: str, mime: str) -> str:
    ext = os.path.splitext(filename)[1].lower() if filename else ""
    if ext in ALLOWED_EXT:
        return ext
    return ALLOWED_MIME.get(mime, "")

def _origin_allowed(origin: str | None) -> bool:
    if not origin:
        return False
    # Normalize the origin by removing any trailing slashes before checking
    normalized_origin = origin.rstrip('/')
    return normalized_origin in ALLOWED_ORIGINS


def _require_session_id(request) -> str:
    sid = request.headers.get("X-Session-Id") or request.args.get("sessionId")
    if not sid:
        raise ValueError("Missing X-Session-Id header")
    return sid


_CACHED_SIGNING_CREDS = None
_CACHED_EXPIRES_AT = 0.0


def _impersonated_signing_credentials(sa_email: str):
    global _CACHED_SIGNING_CREDS, _CACHED_EXPIRES_AT
    now = time.time()
    if _CACHED_SIGNING_CREDS is not None and now < _CACHED_EXPIRES_AT:
        return _CACHED_SIGNING_CREDS
    if not sa_email:
        creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
        _CACHED_SIGNING_CREDS = creds
        _CACHED_EXPIRES_AT = now + SIGNING_CREDS_TTL_SECONDS
        return _CACHED_SIGNING_CREDS
    source_creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    if getattr(source_creds, "token", None) is None:
        source_creds.refresh(google.auth.transport.requests.Request())
    _CACHED_SIGNING_CREDS = impersonated_credentials.Credentials(
        source_credentials=source_creds,
        target_principal=sa_email,
        target_scopes=["https://www.googleapis.com/auth/cloud-platform"],
        lifetime=3600,
    )
    _CACHED_EXPIRES_AT = now + SIGNING_CREDS_TTL_SECONDS
    return _CACHED_SIGNING_CREDS


try:
    firebase_admin.get_app()
except ValueError:
    firebase_admin.initialize_app()


def sign_upload_url(request):
    # --- DIAGNOSTIC LOGS START ---
    origin = request.headers.get("Origin") or ""
    print("--- DIAGNOSTIC LOGS ---")
    print(f"Received Origin Header: '{origin}'")
    print(f"Parsed Allowed Origins Set: {ALLOWED_ORIGINS}")
    print(f"Origin Check Result: {_origin_allowed(origin)}")
    print("--- DIAGNOSTIC LOGS END ---")
    # --- DIAGNOSTIC LOGS END ---
    
    if request.method == "OPTIONS":
        # CORS preflight
        if not _origin_allowed(origin):
            return ("Origin not allowed", 403, {"Content-Type": "text/plain"})
        headers = {
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, content-type, Authorization, authorization, X-Session-Id, x-session-id",
            "Access-Control-Max-Age": "3600",
        }
        return ("", 204, headers)

    try:
        # Origin allowlist
        if not _origin_allowed(origin):
            return (json.dumps({"error": "origin not allowed"}), 403, {"Content-Type": "application/json"})

        # ... (rest of the function is the same)
        auth_header = request.headers.get("Authorization", "")
        token = auth_header.split(" ", 1)[1] if auth_header.lower().startswith("bearer ") else None
        if not token:
            return (json.dumps({"error": "missing Authorization Bearer token"}), 401, {"Content-Type": "application/json"})
        try:
            decoded = fb_auth.verify_id_token(token)
            uid = decoded.get("uid")
        except Exception as e:
            return (json.dumps({"error": "invalid token", "detail": str(e)[:200]}), 401, {"Content-Type": "application/json"})

        sid = _require_session_id(request)
        filename = request.args.get("filename", "")
        size = int(request.args.get("size", "0"))
        mime = request.args.get("type", "")

        if not filename or not mime:
            return (json.dumps({"error": "filename and type are required"}), 400, {"Content-Type": "application/json"})
        if size <= 0 or size > MAX_FILE_BYTES:
            return (json.dumps({"error": "file too large (max 20MB)"}), 400, {"Content-Type": "application/json"})

        ext = _ext_from_filename_or_type(filename, mime)
        if ext not in ALLOWED_EXT:
            return (json.dumps({"error": "unsupported file type"}), 400, {"Content-Type": "application/json"})

        dataset_id = str(uuid.uuid4())
        object_path = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}/raw/input{ext}"

        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(FILES_BUCKET)
        blob = bucket.blob(object_path)

        signing_creds = _impersonated_signing_credentials(RUNTIME_SERVICE_ACCOUNT)

        url = blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=15),
            method="PUT",
            content_type=mime,
            credentials=signing_creds,
        )

        fs = firestore.Client(project=PROJECT_ID)
        ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
        fs.document("users", uid, "sessions", sid, "datasets", dataset_id).set(
            {
                "status": "awaiting_upload",
                "rawUri": f"gs://{FILES_BUCKET}/{object_path}",
                "createdAt": datetime.now(timezone.utc),
                "updatedAt": datetime.now(timezone.utc),
                "ttlAt": ttl_at,
            },
            merge=True,
        )

        resp = {
            "url": url,
            "datasetId": dataset_id,
            "storagePath": object_path,
        }
        headers = {
            "Content-Type": "application/json",
            "Access-Control-Allow-Origin": origin,
        }
        return (json.dumps(resp), 200, headers)

    except ValueError as ve:
        return (json.dumps({"error": str(ve)}), 400, {"Content-Type": "application/json"})
    except Exception as e:
        return (json.dumps({"error": "internal error", "detail": str(e)[:500]}), 500, {"Content-Type": "application/json"})
</file>

<file path="functions/sign_upload_url/requirements.txt">
functions-framework==3.5.0
google-cloud-storage==2.16.0
google-cloud-firestore==2.16.0
firebase-admin==6.5.0
</file>

<file path="run-preprocess/header_utils.py">
"""
Header detection and metadata helpers (simple, pragmatic).

Provides:
- detect_header_row_simple(df, lookahead=12) -> (idx, confidence)
- finalize_headers(raw_headers) -> (final_headers, issues)
- build_analysis_hints(df_after_header, final_headers, header_row_index, confidence) -> (hints, dataset_summary)

Design goals:
- Minimal signals: alpha_ratio, non_empty_ratio, distinctness, next_row_numeric.
- Year-like used only as a tie-breaker.
- Confidence normalized to 0..1 (average of signals).
- Keep payload lightweight; no per-signal dumps in payload.
"""
from __future__ import annotations

import re
from typing import Any, Dict, List, Tuple

import pandas as pd
from pandas.api import types as ptypes

YEAR_RE = re.compile(r"^(19|20)\d{2}$")
CURRENCY_PREFIX_RE = re.compile(r"^[\$€£¥]")
GENERIC_COL_RE = re.compile(r"^(col|column|unnamed)_?\d*$", re.IGNORECASE)


def _is_empty_token(s: str) -> bool:
    if s is None:
        return True
    s2 = str(s).strip().lower()
    return s2 in ("", "none", "nan", "null", "unnamed")


def is_numeric_string(s: str) -> bool:
    if s is None:
        return False
    t = str(s).strip()
    if t == "":
        return False
    # Tolerate commas, currency symbols, parentheses, percent
    t = (
        t.replace(",", "")
        .replace("(", "-")
        .replace(")", "")
        .replace("%", "")
        .replace("$", "")
        .replace("€", "")
        .replace("£", "")
        .replace("¥", "")
    )
    try:
        float(t)
        return True
    except Exception:
        return False


def _row_metrics(vals: List[str]) -> Tuple[float, float, float]:
    # alpha_ratio, distinctness, non_empty_ratio (returned separately for clarity)
    if not vals:
        return 0.0, 0.0, 0.0
    non_empty = [v for v in vals if v != ""]
    if not non_empty:
        return 0.0, 0.0, 0.0
    alpha_ratio = sum(1 for v in non_empty if re.search(r"[A-Za-z]", v)) / len(non_empty)
    distinctness = len(set(non_empty)) / len(non_empty)
    non_empty_ratio = len(non_empty) / max(1, len(vals))
    return float(alpha_ratio), float(distinctness), float(non_empty_ratio)


def detect_header_row_simple(df: pd.DataFrame, lookahead: int = 12) -> Tuple[int, float]:
    """Return (best_row_index, confidence) using 4 simple signals.

    Signals per candidate row r (first `lookahead` rows):
    - alpha_ratio: share of non-empty cells containing letters
    - non_empty_ratio: share of non-empty cells
    - distinctness: unique/total among non-empty
    - next_row_numeric: share of non-empty cells in row r+1 that are numeric-like

    Score is the average of the four signals (0..1). Year-like content is used
    only as a tie-breaker when scores are nearly equal.
    """
    max_r = min(len(df), max(1, lookahead))
    best_idx = 0
    best_eff_score = -1.0
    best_conf_score = 0.0
    best_year_ratio = -1.0

    lower_empty_tokens = {"", "none", "nan", "null"}
    for r in range(max_r):
        # fillna before string cast to avoid literal 'None'
        row = df.iloc[r].fillna("").astype(str).str.strip()
        vals_raw = row.tolist()
        vals = ["" if str(v).strip().lower() in lower_empty_tokens else str(v).strip() for v in vals_raw]
        alpha_ratio, distinctness, non_empty_ratio = _row_metrics(vals)
        non_empty_count = sum(1 for v in vals if v != "")
        if non_empty_ratio < 0.5 or non_empty_count < 2:
            continue  # skip mostly empty rows

        next_row_numeric = 0.0
        if r + 1 < len(df):
            nxt = df.iloc[r + 1].fillna("").astype(str).str.strip()
            nxt_vals = [x for x in nxt.tolist() if str(x).strip().lower() not in lower_empty_tokens]
            if nxt_vals:
                next_row_numeric = sum(1 for v in nxt_vals if is_numeric_string(v)) / len(nxt_vals)

        score_raw = (alpha_ratio + non_empty_ratio + distinctness + next_row_numeric) / 4.0

        # Heuristic boost/penalty layer (does not affect confidence, only selection)
        ne = [v for v in vals if v != ""]
        year_like_ratio = 0.0
        if ne:
            year_like_ratio = sum(1 for v in ne if YEAR_RE.match(v)) / len(ne)

        # Prefer a year-header pattern: majority of columns from col1 are years,
        # and the first column is empty or non-numeric (dimension/title)
        from_c1 = [v for v in vals[1:] if v != ""] if len(vals) > 1 else []
        year_like_from_c1 = 0.0
        if from_c1:
            year_like_from_c1 = sum(1 for v in from_c1 if YEAR_RE.match(v)) / len(from_c1)
        first_tok = vals[0] if vals else ""
        first_tok_numeric = is_numeric_string(first_tok)
        year_header_candidate = bool(year_like_from_c1 >= 0.6 and (first_tok == "" or not first_tok_numeric))

        # Penalize currency-like data rows: many columns look like currency values
        currency_ratio_from_c1 = 0.0
        if from_c1:
            currency_ratio_from_c1 = sum(1 for v in from_c1 if CURRENCY_PREFIX_RE.match(v)) / len(from_c1)
        currency_row_candidate = bool(currency_ratio_from_c1 >= 0.5 and first_tok != "")

        boost = 0.4 if year_header_candidate else 0.0
        penalty = 0.4 if currency_row_candidate else 0.0
        score_eff = max(0.0, min(1.0, score_raw + boost - penalty))

        # Selection uses effective score; confidence remains based on raw score
        if score_eff > best_eff_score:
            best_idx, best_eff_score, best_conf_score, best_year_ratio = r, score_eff, score_raw, year_like_ratio
        elif abs(score_eff - best_eff_score) <= 0.02 and year_like_ratio > best_year_ratio:
            best_idx, best_eff_score, best_conf_score, best_year_ratio = r, score_eff, score_raw, year_like_ratio

        # End candidate loop

    confidence = max(0.0, min(1.0, float(best_conf_score if best_eff_score >= 0.0 else 0.0)))
    return int(best_idx), float(confidence)


def finalize_headers(raw_headers: List[Any]) -> Tuple[List[str], List[Dict[str, Any]]]:
    """Replace empty/generic with col_i, ensure uniqueness, return issues list.
    Issues only for replacements of empty/generic headers.
    """
    final: List[str] = []
    issues: List[Dict[str, Any]] = []
    for i, h in enumerate(raw_headers):
        s = None if h is None else str(h).strip()
        if not s or _is_empty_token(s) or GENERIC_COL_RE.match(s):
            assigned = f"col_{i+1}"
            issues.append({"col_index": i, "reason": "empty_or_generic_header", "original": h, "assigned": assigned})
            final.append(assigned)
        else:
            final.append(s)

    # enforce uniqueness with suffixes
    seen: Dict[str, int] = {}
    uniq: List[str] = []
    for h in final:
        cnt = seen.get(h, 0)
        if cnt == 0:
            uniq.append(h)
        else:
            uniq.append(f"{h}_{cnt+1}")
        seen[h] = cnt + 1
    return uniq, issues


def _is_year_or_numeric_header(h: Any) -> bool:
    if h is None:
        return False
    s = str(h).strip()
    if s == "" or _is_empty_token(s):
        return False
    return bool(YEAR_RE.match(s) or is_numeric_string(s))


def build_analysis_hints(
    df: pd.DataFrame,
    final_headers: List[str],
    header_row_index: int,
    confidence: float,
) -> Tuple[Dict[str, Any], str]:
    """Compute compact analysis hints and a one-liner summary.

    - df: cleaned DataFrame with final headers already assigned
    """
    n_rows, n_cols = df.shape
    total_cells = max(1, n_rows * max(1, n_cols))
    n_missing_total = int(df.isna().sum().sum())
    pct_missing_total = float(n_missing_total / total_cells)

    # numeric columns by dtype or convertible ratio > 0.6
    numeric_columns: List[int] = []
    for i, c in enumerate(df.columns):
        ser = df[c]
        is_num = ptypes.is_numeric_dtype(ser)
        if not is_num:
            sample = ser.dropna().astype(str).head(50)
            if not sample.empty:
                ratio = sample.apply(is_numeric_string).mean()
                is_num = bool(ratio and float(ratio) > 0.6)
        if is_num:
            numeric_columns.append(i)

    # temporal columns: header name year-like or series parseable ratio > 0.6
    temporal_columns: List[int] = []
    for i, name in enumerate(final_headers[: len(df.columns)]):
        is_temporal = bool(YEAR_RE.match(str(name)))
        if not is_temporal:
            ser = df.iloc[:, i]
            try:
                parsed = pd.to_datetime(ser.dropna().astype(str).head(50), errors="coerce")
                if not parsed.empty and parsed.notna().mean() > 0.6:
                    is_temporal = True
            except Exception:
                is_temporal = False
        if is_temporal:
            temporal_columns.append(i)

    # first column type heuristic
    first_column_type = "data"
    if n_cols > 0:
        first_ser = df.iloc[:, 0]
        first_is_textual = not ptypes.is_numeric_dtype(first_ser)
        numeric_others = [i for i in range(1, n_cols) if i in numeric_columns]
        if first_is_textual and len(numeric_others) >= max(1, int(0.5 * max(0, n_cols - 1))):
            first_column_type = "dimension"

    likely_pivoted = bool(first_column_type == "dimension" and len(numeric_columns) >= max(1, int(0.5 * max(0, n_cols - 1))))

    hints: Dict[str, Any] = {
        "detected_header_row": int(header_row_index),
        "header_confidence": float(confidence),
        "pct_missing_total": float(pct_missing_total),
        "first_column_type": first_column_type,
        "likely_pivoted": likely_pivoted,
        "temporal_columns": temporal_columns,
        "numeric_columns": numeric_columns,
    }

    # concise summary
    dataset_summary = (
        f"Dataset has {n_rows} rows and {n_cols} columns. "
        f"Header row {header_row_index} detected (confidence {confidence:.2f}). "
        f"Structure: first column {first_column_type}, pivoted={likely_pivoted}."
    )

    return hints, dataset_summary
</file>

<file path="run-preprocess/main.py">
import os
import json
import logging
import base64
import io
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta, timezone
from typing import Optional, Tuple

from fastapi import FastAPI, Request, Response

# Optional: import google clients (not used until wired)
try:
    from google.cloud import storage  # type: ignore
    from google.cloud import firestore  # type: ignore
    import google.cloud.logging as cloud_logging  # type: ignore
except Exception:  # pragma: no cover - allow local runs without GCP libs installed
    storage = None
    firestore = None
    cloud_logging = None  # type: ignore

app = FastAPI(title="Preprocess Service", version="0.1.0")

FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PROJECT_ID = os.getenv("GCP_PROJECT", os.getenv("GOOGLE_CLOUD_PROJECT", "ai-data-analyser"))
TTL_DAYS = int(os.getenv("TTL_DAYS", "1"))

def _setup_cloud_logging() -> None:
    """Configure Google Cloud Logging handler if available.

    In Cloud Run, this attaches a handler to the root logger, enabling structured
    logs in Cloud Logging. Fallback to std logging if the client is unavailable.
    """
    if cloud_logging is None:
        return
    try:
        client = cloud_logging.Client(project=PROJECT_ID)
        client.setup_logging()  # attaches handler to root logger
        logging.getLogger().setLevel(logging.INFO)
        logging.info("cloud-logging: configured")
    except Exception as e:
        logging.warning("cloud-logging: setup failed: %s", e)


# Configure logging at startup time
@app.on_event("startup")
async def _on_startup() -> None:  # pragma: no cover
    _setup_cloud_logging()

"""Lazy import loader for pipeline_adapter.

We avoid importing heavy data libs (pandas/pyarrow) at module import time so
the Cloud Run container can start and bind to PORT quickly. The actual import
occurs on first use inside the request handler.
"""
_process_file_to_artifacts = None  # type: ignore
_process_bytes_to_artifacts = None  # type: ignore

def _get_engine() -> str:
    eng = os.getenv("PREPROCESS_ENGINE", "polars").strip().lower()
    return "polars" if eng == "polars" else "pandas"

def get_process_file_to_artifacts():
    """Import and cache process_file_to_artifacts lazily.

    Tries package-relative import first, then absolute import for script mode.
    Raises the underlying exception if imports fail, and logs for visibility.
    """
    global _process_file_to_artifacts
    if _process_file_to_artifacts is not None:
        return _process_file_to_artifacts
    engine = _get_engine()
    # Prefer polars adapter when requested, else default to pandas adapter
    try:
        if engine == "polars":
            try:
                from .pipeline_adapter_polars import process_file_to_artifacts as _fn  # type: ignore
            except Exception:
                from pipeline_adapter_polars import process_file_to_artifacts as _fn  # type: ignore
            logging.info("preprocess engine: polars")
        else:
            raise ImportError("force_pandas")
    except Exception:
        # Fallback to pandas adapter
        try:
            from .pipeline_adapter import process_file_to_artifacts as _fn  # type: ignore
        except Exception:
            from pipeline_adapter import process_file_to_artifacts as _fn  # type: ignore
        logging.info("preprocess engine: pandas (fallback)")
    _process_file_to_artifacts = _fn
    return _fn


def get_process_bytes_to_artifacts():
    """Import and cache process_bytes_to_artifacts lazily."""
    global _process_bytes_to_artifacts
    if _process_bytes_to_artifacts is not None:
        return _process_bytes_to_artifacts
    engine = _get_engine()
    try:
        if engine == "polars":
            try:
                from .pipeline_adapter_polars import process_bytes_to_artifacts as _fn  # type: ignore
            except Exception:
                from pipeline_adapter_polars import process_bytes_to_artifacts as _fn  # type: ignore
            logging.info("preprocess engine: polars")
        else:
            raise ImportError("force_pandas")
    except Exception:
        try:
            from .pipeline_adapter import process_bytes_to_artifacts as _fn  # type: ignore
        except Exception:
            from pipeline_adapter import process_bytes_to_artifacts as _fn  # type: ignore
        logging.info("preprocess engine: pandas (fallback)")
    _process_bytes_to_artifacts = _fn
    return _fn


@app.get("/healthz")
def healthz() -> dict:
    return {"status": "ok", "bucket": FILES_BUCKET, "project": PROJECT_ID}


def _parse_object_path(name: str) -> Optional[Tuple[str, str, str]]:
    """Parse gs object path to (uid, sid, datasetId).
    Expected structure:
    users/{uid}/sessions/{sid}/datasets/{datasetId}/raw/input.csv|xlsx
    """
    try:
        parts = name.split("/")
        i = parts.index("users")
        uid = parts[i + 1]
        assert parts[i + 2] == "sessions"
        sid = parts[i + 3]
        assert parts[i + 4] == "datasets"
        dataset_id = parts[i + 5]
        assert parts[i + 6] == "raw"
        return uid, sid, dataset_id
    except Exception:
        return None


@app.post("/eventarc")
async def handle_eventarc(request: Request) -> Response:
    """Handle Eventarc GCS finalize events.

    Accepts CloudEvent-like body with `data.bucket` and `data.name`.
    Ignores events that are not under `/raw/` or are not CSV/XLSX.
    """
    # Read raw body and attempt to support multiple delivery shapes:
    # 1) CloudEvents structured: { "data": { "bucket": ..., "name": ... }, ... }
    # 2) CloudEvents binary: body is the data object itself
    # 3) Pub/Sub push: { "message": { "data": base64(json) }, ... }
    # 4) GCS notification compatibility: top-level { "bucket": ..., "name": ... }
    envelope: dict = {}
    try:
        body_bytes = await request.body()
        body_text = body_bytes.decode("utf-8") if body_bytes else "{}"
        envelope = json.loads(body_text or "{}") if body_text else {}
    except Exception as e:
        logging.warning("Invalid JSON for Eventarc request: %s", e)
        envelope = {}

    data = None
    t0 = time.monotonic()

    # Structured CloudEvent
    if isinstance(envelope, dict) and "data" in envelope and isinstance(envelope["data"], dict):
        data = envelope["data"]

    # Pub/Sub push format
    if data is None and isinstance(envelope, dict) and "message" in envelope:
        try:
            msg = envelope.get("message", {})
            b64 = msg.get("data")
            if isinstance(b64, str):
                decoded = base64.b64decode(b64 + "==")
                inner = json.loads(decoded.decode("utf-8"))
                data = inner.get("data") if isinstance(inner, dict) and "data" in inner else inner
        except Exception as e:
            logging.warning("Failed to decode Pub/Sub envelope: %s", e)

    # GCS notification compatibility or binary CE body
    if data is None and isinstance(envelope, dict):
        if "bucket" in envelope or "name" in envelope or "objectId" in envelope:
            data = envelope

    if not isinstance(data, dict):
        logging.warning("Event parsing failed; envelope keys=%s headers.ce-type=%s", list(envelope.keys()) if isinstance(envelope, dict) else type(envelope), request.headers.get("ce-type"))
        return Response(status_code=204)

    bucket = data.get("bucket")
    name = data.get("name") or data.get("objectId") or data.get("object")

    if not bucket or not name:
        logging.warning("Missing bucket/name after normalization: %s", data)
        return Response(status_code=204)

    # Only react to raw uploads (csv/xlsx)
    is_raw = "/raw/" in name
    is_csv = name.lower().endswith(".csv")
    is_xlsx = name.lower().endswith(".xlsx") or name.lower().endswith(".xls")
    if not (is_raw and (is_csv or is_xlsx)):
        return Response(status_code=204)

    ids = _parse_object_path(name)
    if not ids:
        logging.warning("Object path does not match expected pattern: %s", name)
        return Response(status_code=204)

    uid, sid, dataset_id = ids
    logging.info("Preprocess trigger: bucket=%s name=%s uid=%s sid=%s dataset=%s", bucket, name, uid, sid, dataset_id)
    # Initialize clients lazily
    if storage is None or firestore is None:
        logging.error("GCP clients not available in runtime environment")
        return Response(status_code=500)

    storage_client = storage.Client(project=PROJECT_ID)
    firestore_client = firestore.Client(project=PROJECT_ID)

    try:
        # 1) Download raw into memory
        raw_blob = storage_client.bucket(bucket).blob(name)
        raw_bytes = raw_blob.download_as_bytes()
        kind = "excel" if is_xlsx else "csv"
        t_download = time.monotonic()

        # 2) Optional manual header-row override from GCS metadata (preferred) or env fallback
        header_row_override = None
        try:
            prefix = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}"
            override_blob = storage_client.bucket(bucket).blob(f"{prefix}/metadata/preprocess_overrides.json")
            if override_blob.exists(storage_client):
                override_obj = json.loads(override_blob.download_as_text())
                val = override_obj.get("header_row_index")
                if isinstance(val, int):
                    header_row_override = val
        except Exception as e:
            logging.warning("override_read_failed: %s", e)

        if header_row_override is None:
            try:
                env_val = os.getenv("PREPROCESS_HEADER_ROW_OVERRIDE")
                if env_val is not None and str(env_val).strip() != "":
                    header_row_override = int(str(env_val).strip())
            except Exception:
                pass

        # 3) Run pipeline adapter (bytes variant; lazy import on first use)
        result = get_process_bytes_to_artifacts()( 
            raw_bytes,
            kind,
            sample_rows_for_llm=50,
            metric_rename_heuristic=False,
            header_row_override=header_row_override,
        )
        t_process = time.monotonic()

        # 3) Write artifacts to GCS under dataset prefix (in parallel)
        prefix = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}"
        cleaned_path = f"{prefix}/cleaned/cleaned.parquet"
        payload_path = f"{prefix}/metadata/payload.json"
        report_path = f"{prefix}/reports/cleaning_report.json"

        # Build in-memory parquet
        parquet_buf = io.BytesIO()
        result.cleaned_df.to_parquet(parquet_buf, engine="pyarrow", index=False)
        parquet_size = parquet_buf.getbuffer().nbytes
        parquet_buf.seek(0)
        t_build = time.monotonic()

        payload_data = json.dumps(result.payload, ensure_ascii=False).encode("utf-8")
        report_data = json.dumps(result.cleaning_report, ensure_ascii=False).encode("utf-8")

        bkt = storage_client.bucket(bucket)
        cleaned_blob = bkt.blob(cleaned_path)
        payload_blob = bkt.blob(payload_path)
        report_blob = bkt.blob(report_path)

        # Upload three artifacts in parallel
        with ThreadPoolExecutor(max_workers=3) as ex:
            futs = []
            futs.append(ex.submit(cleaned_blob.upload_from_file, parquet_buf, size=parquet_size, content_type="application/octet-stream"))
            futs.append(ex.submit(payload_blob.upload_from_string, payload_data, content_type="application/json; charset=utf-8"))
            futs.append(ex.submit(report_blob.upload_from_string, report_data, content_type="application/json; charset=utf-8"))
            for f in futs:
                f.result()
        t_uploads = time.monotonic()

        cleaned_uri = f"gs://{bucket}/{cleaned_path}"
        payload_uri = f"gs://{bucket}/{payload_path}"
        report_uri = f"gs://{bucket}/{report_path}"

        # 4) Update Firestore dataset doc
        ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
        doc_ref = firestore_client.document(
            "users", uid, "sessions", sid, "datasets", dataset_id
        )
        doc_ref.set(
            {
                "rawUri": f"gs://{bucket}/{name}",
                "cleanedUri": cleaned_uri,
                "payloadUri": payload_uri,
                "reportUri": report_uri,
                "rows": result.rows,
                "columns": result.columns,
                "status": "ready",
                "updatedAt": datetime.now(timezone.utc),
                "ttlAt": ttl_at,
            },
            merge=True,
        )
        t_firestore = time.monotonic()

        logging.info(json.dumps({
            "event": "preprocess_complete",
            "uid": uid,
            "sid": sid,
            "datasetId": dataset_id,
            "rows": result.rows,
            "columns": result.columns,
            "cleanedUri": cleaned_uri,
            "payloadUri": payload_uri,
            "reportUri": report_uri,
        }))

        # header detection telemetry (v2 fields are additive; guard if missing)
        try:
            header_info = (result.payload or {}).get("header_info", {})
            conf = float(header_info.get("confidence") or 0.0)
            method = header_info.get("method") or "unknown"
            hrow = int(header_info.get("header_row_index") or -1)
            is_transposed = bool(header_info.get("is_transposed") or False)
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            low_thr = float(os.getenv("HEADER_LOW_CONFIDENCE_THRESHOLD", "0.4"))
            low_conf = conf < low_thr
            logging.info(json.dumps({
                "event": "header_detection",
                "uid": uid,
                "sid": sid,
                "datasetId": dataset_id,
                "engine": _get_engine(),
                "header_row_index": hrow,
                "confidence": round(conf, 3),
                "method": method,
                "is_transposed": is_transposed,
                "lookahead": lookahead,
                "low_confidence": low_conf,
            }))
        except Exception as e:
            logging.warning("header_detection_log_failed: %s", e)
        # timings
        timings = {
            "event": "preprocess_timings",
            "uid": uid,
            "sid": sid,
            "datasetId": dataset_id,
            "download_s": round(t_download - t0, 3),
            "process_s": round(t_process - t_download, 3),
            "build_parquet_s": round(t_build - t_process, 3),
            "uploads_s": round(t_uploads - t_build, 3),
            "firestore_s": round(t_firestore - t_uploads, 3),
            "total_s": round(t_firestore - t0, 3),
        }
        logging.info(json.dumps(timings))
        return Response(status_code=204)

    except Exception as e:  # noqa: BLE001 (broad for last-resort error path)
        logging.exception("Preprocess failed: %s", e)
        try:
            ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
            firestore_client.document(
                "users", uid, "sessions", sid, "datasets", dataset_id
            ).set(
                {
                    "status": "error",
                    "errorMessage": str(e)[:2000],
                    "updatedAt": datetime.now(timezone.utc),
                    "ttlAt": ttl_at,
                },
                merge=True,
            )
        except Exception:
            logging.warning("Failed to write error status to Firestore")
        return Response(status_code=500)

if __name__ == "__main__":  # pragma: no cover
    # This block is for local development or when run directly.
    # Cloud Run will provide the PORT environment variable.
    import uvicorn

    port = int(os.getenv("PORT", "8080"))
    uvicorn.run(app, host="0.0.0.0", port=port)
</file>

<file path="run-preprocess/pipeline_adapter_polars.py">
"""
Polars-backed adapter for the preprocess service.

CSV paths use Polars for faster load and normalization, then convert to pandas
for payload construction that reuses pandas-based helpers (type inference).
Excel remains handled by pandas for compatibility.
"""
from __future__ import annotations

import io
import re
import math
from typing import Any, Dict, List, Optional

import polars as pl  # type: ignore
import pandas as pd
import os

# Reuse shared types and helpers with robust import (package/script modes)
try:
    from .pipeline_adapter import (  # type: ignore
        ProcessResult,
        NULL_TOKENS,
        KMB_PATTERN,
        CURRENCY_PATTERN,
        PARENS_PATTERN,
        PERCENT_PATTERN,
        infer_column_type,
    )
except Exception:  # pragma: no cover
    from pipeline_adapter import (  # type: ignore
        ProcessResult,
        NULL_TOKENS,
        KMB_PATTERN,
        CURRENCY_PATTERN,
        PARENS_PATTERN,
        PERCENT_PATTERN,
        infer_column_type,
    )


def _pl_read_csv(source: Any) -> pl.DataFrame:
    """Read CSV via Polars with no header (robust settings)."""
    return pl.read_csv(
        source,
        has_header=False,
        infer_schema_length=2048,
        ignore_errors=True,
        try_parse_dates=False,
    )


def _drop_fully_blank_rows_pl(df: pl.DataFrame) -> pl.DataFrame:
    cols = df.columns
    blank_exprs = []
    for c in cols:
        s = pl.col(c).cast(pl.Utf8).str.strip_chars().str.to_lowercase()
        blank_exprs.append(pl.col(c).is_null() | s.is_in(list(NULL_TOKENS)))
    all_blank = pl.all_horizontal(blank_exprs)
    return df.filter(~all_blank)


def _detect_header_row_pl(df: pl.DataFrame, lookahead: int = 5) -> int:
    max_r = min(df.height, lookahead)
    best_row = 0
    best_score = float("-inf")
    for r in range(max_r):
        row = df.row(r)
        vals = [str(v).strip() for v in row]
        non_empty_ratio = sum(1 for v in vals if v != "") / max(1, len(vals))
        alpha_ratio = sum(1 for v in vals if re.search(r"[A-Za-z]", v or "")) / max(1, len(vals))
        score = alpha_ratio - (0.5 if non_empty_ratio < 0.7 else 0)
        if score > best_score:
            best_score = score
            best_row = r
    return best_row


def _build_headers(vals: list[str]) -> list[str]:
    headers: list[str] = []
    for v in vals:
        s = re.sub(r"\s+", " ", str(v).strip())
        headers.append(s if s else "Column")
    seen: Dict[str, int] = {}
    dedup: list[str] = []
    for h in headers:
        n = h
        if n in seen:
            seen[n] += 1
            n = f"{n}_{seen[n]}"
        else:
            seen[n] = 1
        dedup.append(n)
    return dedup


def _numeric_expr_for(col: str) -> pl.Expr:
    s = pl.col(col).cast(pl.Utf8)
    s = s.str.replace("\u2212", "-")
    s = s.str.replace("\u00a0", " ")
    s = s.str.replace(r"[\u2000-\u200B]", " ", literal=False)
    s = s.str.strip_chars()

    lower = s.str.to_lowercase()
    s = pl.when(lower.is_in(list(NULL_TOKENS))).then(None).otherwise(s)

    # Detect negatives via parentheses, then drop parentheses for casting
    mask_paren = s.str.contains(PARENS_PATTERN.pattern, literal=False)
    s = s.str.replace_all(r"[()]", "", literal=False)
    mask_trail = s.str.ends_with("-")
    s = s.str.replace(r"-$", "", literal=False)

    # K/M/B
    suffix = s.str.extract(KMB_PATTERN.pattern, group_index=1)
    mult = (
        pl.when(suffix.str.to_lowercase() == "k").then(1e3)
        .when(suffix.str.to_lowercase() == "m").then(1e6)
        .when(suffix.str.to_lowercase() == "b").then(1e9)
        .otherwise(1.0)
    )
    s = s.str.replace(KMB_PATTERN.pattern, "", literal=False)

    # Currency and separators
    s = s.str.replace(CURRENCY_PATTERN.pattern, "", literal=False)
    s = s.str.replace(",", "")
    s = s.str.replace(" ", "")

    # Percent
    percent_mask = s.str.contains(PERCENT_PATTERN.pattern, literal=False)
    s = s.str.replace(PERCENT_PATTERN.pattern, "", literal=False)

    nums = s.cast(pl.Float64, strict=False) * mult
    nums = pl.when(mask_paren | mask_trail).then(-nums).otherwise(nums)
    nums = pl.when(percent_mask).then(nums / 100.0).otherwise(nums)
    return nums


def _apply_numeric_normalization(df: pl.DataFrame) -> tuple[pl.DataFrame, list[str]]:
    numeric_cols: list[str] = []
    out = df
    for c in df.columns:
        expr = _numeric_expr_for(c)
        ratio = out.select(expr.is_not_null().mean()).item()
        if ratio is not None and float(ratio) >= 0.6:
            out = out.with_columns(expr.alias(c))
            numeric_cols.append(c)
    return out, numeric_cols


def process_file_to_artifacts(
    local_path: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    # Decide file kind by extension
    ext = local_path.lower().rsplit(".", 1)[-1]
    if ext in ("xlsx", "xls"):
        # Excel via pandas
        raw_df = pd.read_excel(local_path, sheet_name=0, header=None, dtype=object)
        file_kind = "excel"
    else:
        # CSV via Polars → native cleaning → convert to pandas for payload
        df_pl = _pl_read_csv(local_path)
        rows_before = df_pl.height
        work = _drop_fully_blank_rows_pl(df_pl)
        # Lazy import header helpers (package/script modes)
        try:
            from .header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore
        except Exception:  # pragma: no cover
            from header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore

        if header_row_override is not None:
            hdr_row = int(header_row_override)
            header_confidence = 1.0
            method = "override"
        else:
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            pdf = work.head(lookahead).to_pandas()
            hdr_row, header_confidence = detect_header_row_simple(pdf, lookahead=lookahead)
            method = "auto_detected"

        raw_headers = [str(x) for x in work.row(hdr_row)] if work.height > hdr_row else [None] * work.width
        headers, header_issues = finalize_headers(raw_headers)
        body = work.slice(hdr_row + 1)
        # Rename first N columns to headers
        rename_map = {old: headers[i] for i, old in enumerate(body.columns[: len(headers)])}
        body = body.rename(rename_map)
        # Remove repeated header rows
        eq_exprs = [(pl.col(h).cast(pl.Utf8).str.strip_chars() == headers[i]) for i, h in enumerate(headers)]
        is_hdr_row = pl.all_horizontal(eq_exprs)
        body = body.filter(~is_hdr_row)
        # Numeric normalization
        body, numeric_cols = _apply_numeric_normalization(body)
        # Convert to pandas for payload/type inference
        df = body.to_pandas()
        # Build payload pieces
        columns_meta: Dict[str, Any] = {}
        for col in df.columns:
            col_type, dt_fmt = infer_column_type(df[col], str(col))
            col_series = df[col]
            nn = col_series.dropna()
            null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
            unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
            vc = nn.value_counts().head(5)
            top_values = []
            for v, c in vc.items():
                val: Any = v
                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                    continue
                if isinstance(v, (float, int)):
                    val = float(v)
                if isinstance(v, str) and len(v) > 300:
                    val = v[:300]
                top_values.append({"value": val, "count": int(c)})
            entry: Dict[str, Any] = {
                "type": col_type,
                "null_pct": round(float(null_pct), 2),
                "unique_pct": round(float(unique_pct), 2),
                "top_values": top_values,
            }
            if col_type == "date":
                entry["datetime_format"] = dt_fmt
            entry["is_potential_dimension"] = False
            columns_meta[str(col)] = entry

        # Heuristic for potential dimension
        if len(df.columns) > 0:
            first = str(df.columns[0])
            first_type = columns_meta[first]["type"]
            numeric_others = [c for c in list(df.columns)[1:] if columns_meta[str(c)]["type"] in ("integer", "float", "percentage", "currency")]
            if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
                columns_meta[first]["is_potential_dimension"] = True

        dataset_meta = {
            "rows": int(df.shape[0]),
            "columns": int(df.shape[1]),
            "column_names": [str(x) for x in df.columns.tolist()],
            "dtypes": {str(c): str(df[c].dtype) for c in df.columns},
        }

        n = min(sample_rows_for_llm, len(df))
        sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)
        sample_rows: list[dict] = []
        for _, r in sample.iterrows():
            obj: Dict[str, Any] = {}
            for k, v in r.to_dict().items():
                if isinstance(v, str) and len(v) > 300:
                    obj[str(k)] = v[:300]
                elif isinstance(v, (float, int)):
                    obj[str(k)] = float(v)
                else:
                    obj[str(k)] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
            sample_rows.append(obj)

        # Build v2 analysis hints and dataset summary
        hints, dataset_summary = build_analysis_hints(df, headers, hdr_row, float(header_confidence))
        # Minimal header_info with transposed signal
        is_transposed = False
        non_empty_final = [h for h in headers if str(h).strip() != ""]
        if non_empty_final:
            is_transposed = all(is_numeric_string(h) for h in non_empty_final)

        header_info = {
            "method": method,
            "header_row_index": int(hdr_row),
            "confidence": float(header_confidence),
            "original_headers": [str(x) if x is not None else None for x in raw_headers],
            "final_headers": headers,
            "is_transposed": bool(is_transposed),
        }
        if header_issues:
            header_info["issues"] = header_issues

        payload: Dict[str, Any] = {
            "dataset": dataset_meta,
            "columns": columns_meta,
            "sample_rows": sample_rows,
            "cleaning_report": {
                "header_row": int(hdr_row),
                "renamed_columns": {},
                "numeric_columns": numeric_cols,
                "rows_before": int(rows_before),
                "rows_after": int(df.shape[0]),
                "file_kind": "csv",
            },
            "mode": "full",
            "version": "1",
            # v2 additive fields
            "schema_version": "2.0",
            "header_info": header_info,
            "analysis_hints": hints,
            "dataset_summary": dataset_summary,
        }

        return ProcessResult(
            cleaned_df=df,
            payload=payload,
            cleaning_report=payload["cleaning_report"],
            rows=int(df.shape[0]),
            columns=int(df.shape[1]),
        )


def process_bytes_to_artifacts(
    data: bytes,
    kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    if kind not in ("csv", "excel"):
        raise ValueError("kind must be 'csv' or 'excel'")

    if kind == "excel":
        # Excel via pandas
        raw_df = pd.read_excel(io.BytesIO(data), sheet_name=0, header=None, dtype=object)
        # Defer to shared pandas path by converting directly
        # Import locally to avoid circulars
        try:
            from .pipeline_adapter import process_df_to_artifacts as _p  # type: ignore
        except Exception:
            from pipeline_adapter import process_df_to_artifacts as _p  # type: ignore
        return _p(raw_df, "excel", sample_rows_for_llm=sample_rows_for_llm, metric_rename_heuristic=metric_rename_heuristic, header_row_override=header_row_override)
    else:
        # CSV via Polars (same as file path case)
        tmp_path = io.BytesIO(data)
        df_pl = _pl_read_csv(tmp_path)
        rows_before = df_pl.height
        work = _drop_fully_blank_rows_pl(df_pl)
        # Lazy import header helpers
        try:
            from .header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore
        except Exception:  # pragma: no cover
            from header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore

        if header_row_override is not None:
            hdr_row = int(header_row_override)
            header_confidence = 1.0
            method = "override"
        else:
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            pdf = work.head(lookahead).to_pandas()
            hdr_row, header_confidence = detect_header_row_simple(pdf, lookahead=lookahead)
            method = "auto_detected"

        raw_headers = [str(x) for x in work.row(hdr_row)] if work.height > hdr_row else [None] * work.width
        headers, header_issues = finalize_headers(raw_headers)
        body = work.slice(hdr_row + 1)
        rename_map = {old: headers[i] for i, old in enumerate(body.columns[: len(headers)])}
        body = body.rename(rename_map)
        eq_exprs = [(pl.col(h).cast(pl.Utf8).str.strip_chars() == headers[i]) for i, h in enumerate(headers)]
        is_hdr_row = pl.all_horizontal(eq_exprs)
        body = body.filter(~is_hdr_row)
        body, numeric_cols = _apply_numeric_normalization(body)
        df = body.to_pandas()

        columns_meta: Dict[str, Any] = {}
        for col in df.columns:
            col_type, dt_fmt = infer_column_type(df[col], str(col))
            col_series = df[col]
            nn = col_series.dropna()
            null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
            unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
            vc = nn.value_counts().head(5)
            top_values = []
            for v, c in vc.items():
                val: Any = v
                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                    continue
                if isinstance(v, (float, int)):
                    val = float(v)
                if isinstance(v, str) and len(v) > 300:
                    val = v[:300]
                top_values.append({"value": val, "count": int(c)})
            entry: Dict[str, Any] = {
                "type": col_type,
                "null_pct": round(float(null_pct), 2),
                "unique_pct": round(float(unique_pct), 2),
                "top_values": top_values,
            }
            if col_type == "date":
                entry["datetime_format"] = dt_fmt
            entry["is_potential_dimension"] = False
            columns_meta[str(col)] = entry

        if len(df.columns) > 0:
            first = str(df.columns[0])
            first_type = columns_meta[first]["type"]
            numeric_others = [c for c in list(df.columns)[1:] if columns_meta[str(c)]["type"] in ("integer", "float", "percentage", "currency")]
            if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
                columns_meta[first]["is_potential_dimension"] = True

        dataset_meta = {
            "rows": int(df.shape[0]),
            "columns": int(df.shape[1]),
            "column_names": [str(x) for x in df.columns.tolist()],
            "dtypes": {str(c): str(df[c].dtype) for c in df.columns},
        }
        n = min(sample_rows_for_llm, len(df))
        sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)
        sample_rows: list[dict] = []
        for _, r in sample.iterrows():
            obj: Dict[str, Any] = {}
            for k, v in r.to_dict().items():
                if isinstance(v, str) and len(v) > 300:
                    obj[str(k)] = v[:300]
                elif isinstance(v, (float, int)):
                    obj[str(k)] = float(v)
                else:
                    obj[str(k)] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
            sample_rows.append(obj)

        # Build v2 analysis hints and dataset summary
        hints, dataset_summary = build_analysis_hints(df, headers, hdr_row, float(header_confidence))
        # Minimal header_info with transposed signal
        is_transposed = False
        non_empty_final = [h for h in headers if str(h).strip() != ""]
        if non_empty_final:
            is_transposed = all(is_numeric_string(h) for h in non_empty_final)

        header_info = {
            "method": method,
            "header_row_index": int(hdr_row),
            "confidence": float(header_confidence),
            "original_headers": [str(x) if x is not None else None for x in raw_headers],
            "final_headers": headers,
            "is_transposed": bool(is_transposed),
        }
        if header_issues:
            header_info["issues"] = header_issues

        payload: Dict[str, Any] = {
            "dataset": dataset_meta,
            "columns": columns_meta,
            "sample_rows": sample_rows,
            "cleaning_report": {
                "header_row": int(hdr_row),
                "renamed_columns": {},
                "numeric_columns": numeric_cols,
                "rows_before": int(rows_before),
                "rows_after": int(df.shape[0]),
                "file_kind": "csv",
            },
            "mode": "full",
            "version": "1",
            # v2 additive fields
            "schema_version": "2.0",
            "header_info": header_info,
            "analysis_hints": hints,
            "dataset_summary": dataset_summary,
        }

        return ProcessResult(
            cleaned_df=df,
            payload=payload,
            cleaning_report=payload["cleaning_report"],
            rows=int(df.shape[0]),
            columns=int(df.shape[1]),
        )
</file>

<file path="run-preprocess/pipeline_adapter.py">
"""
Pipeline adapter for the preprocess service.

This module loads a raw CSV/XLSX file from a local path, performs light but
robust cleaning, infers lightweight column metadata for LLM prompting
(payload), and returns both the cleaned DataFrame and JSON-serializable
artifacts.

Design goals:
- Preserve original column names (no destructive renames).
- Excel: first sheet only.
- Deterministic sampling for payload (seed=42, up to 50 rows).
- Numeric normalization for %, K/M/B, currency symbols, and common separators.
- Add `datetime_format` where inferred with high confidence.
- Add `is_potential_dimension` instead of renaming first column to Metric.

Note: This is a focused adapter tailored for serverless preprocessing. It is
not a full port of the original repository in data_processing_profiling.md, but
it follows the same principles and payload structure agreed for this project.
"""
from __future__ import annotations

import io
import math
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import os
try:
    from .header_utils import (
        detect_header_row_simple,
        finalize_headers,
        build_analysis_hints,
        is_numeric_string,
    )  # type: ignore
except Exception:  # pragma: no cover
    from header_utils import (
        detect_header_row_simple,
        finalize_headers,
        build_analysis_hints,
        is_numeric_string,
    )  # type: ignore

# -----------------------------
# Constants & helpers
# -----------------------------

NULL_TOKENS = {
    "", "na", "n/a", "none", "null", "nil", "nan", "-", "--", "n\u00a0a",
}
KMB_PATTERN = re.compile(r"\s*([kKmMbB])\s*$")
CURRENCY_PATTERN = re.compile(r"[$€£¥]|USD|EUR|GBP|JPY|TRY|TL|₺")
TRAILING_MINUS_PATTERN = re.compile(r"-$")
PARENS_PATTERN = re.compile(r"^\(.*\)$")
PERCENT_PATTERN = re.compile(r"%")
DATE_REGEXES = [
    (re.compile(r"^\d{4}-\d{2}-\d{2}$"), "%Y-%m-%d"),
    (re.compile(r"^\d{2}/\d{2}/\d{4}$"), "%m/%d/%Y"),
    (re.compile(r"^\d{2}-\d{2}-\d{4}$"), "%m-%d-%Y"),
    (re.compile(r"^\d{2}\.\d{2}\.\d{4}$"), "%m.%d.%Y"),
    (re.compile(r"^\d{1,2}\s+[A-Za-z]{3}\s+\d{4}$"), "%d %b %Y"),
    (re.compile(r"^[A-Za-z]{3}\s+\d{1,2},\s+\d{4}$"), "%b %d, %Y"),
]


@dataclass
class ProcessResult:
    cleaned_df: pd.DataFrame
    payload: Dict[str, Any]
    cleaning_report: Dict[str, Any]
    rows: int
    columns: int


# -----------------------------
# Loading & cleaning
# -----------------------------

def _load_raw(local_path: str) -> Tuple[pd.DataFrame, str]:
    ext = local_path.lower().rsplit(".", 1)[-1]
    if ext in ("xlsx", "xls"):
        df = pd.read_excel(local_path, sheet_name=0, header=None, dtype=object)
        kind = "excel"
    else:
        # CSV fast path: C engine + Arrow-backed dtypes for speed/memory
        df = pd.read_csv(
            local_path,
            header=None,
            engine="c",
            dtype_backend="pyarrow",
        )
        kind = "csv"
    return df, kind


def _drop_fully_blank_rows(df: pd.DataFrame) -> pd.DataFrame:
    is_na = df.isna()
    lowered = df.astype(str).apply(lambda s: s.str.strip().str.lower(), axis=0)
    is_token_blank = lowered.isin(NULL_TOKENS)
    is_blank = is_na | is_token_blank
    keep_mask = ~is_blank.all(axis=1)
    return df.loc[keep_mask]


def _detect_header_row(df: pd.DataFrame, lookahead: int = 5) -> int:
    # Simple heuristic: choose the earliest row where more than half of columns
    # are non-numeric-ish strings, followed by a row with more numeric candidates.
    max_r = min(len(df), lookahead)
    best_row = 0
    best_score = float("-inf")
    for r in range(max_r):
        row = df.iloc[r].astype(str).str.strip()
        non_empty = row.replace({"": np.nan}).notna().mean()
        alpha_ratio = row.str.contains(r"[A-Za-z]", regex=True, na=False).mean()
        # Penalize rows with very low non-empty ratio
        score = alpha_ratio - (0.5 if non_empty < 0.7 else 0)
        if score > best_score:
            best_score = score
            best_row = r
    return best_row


def _build_headers(df: pd.DataFrame, header_row: int) -> Tuple[List[str], int]:
    row = df.iloc[header_row].tolist()
    headers: List[str] = []
    for val in row:
        s = re.sub(r"\s+", " ", str(val).strip())
        headers.append(s if s else "Column")
    # Deduplicate while preserving order
    seen = {}
    dedup: List[str] = []
    for h in headers:
        n = h
        if n in seen:
            seen[n] += 1
            n = f"{n}_{seen[n]}"
        else:
            seen[n] = 1
        dedup.append(n)
    start_row = header_row + 1
    return dedup, start_row


# -----------------------------
# Numeric normalization
# -----------------------------

def _normalize_whitespace_and_minus(series: pd.Series) -> pd.Series:
    s = series.astype(str)
    s = s.str.replace("\u2212", "-", regex=False)  # unicode minus → ASCII
    s = s.str.replace("\u00a0", " ", regex=False)  # NBSP → space
    s = s.str.replace(r"[\u2000-\u200B]", " ", regex=True)  # thin spaces
    return s


def normalize_numeric_series(series: pd.Series) -> pd.Series:
    s = series.astype(object)
    s = s.where(~s.isna(), None)
    s = _normalize_whitespace_and_minus(s).str.strip()

    lower = s.str.lower()
    s = s.mask(lower.isin(NULL_TOKENS), np.nan)
    s = s.fillna("")

    # Detect negative forms and strip wrappers
    mask_paren = s.str.match(PARENS_PATTERN, na=False)
    s = s.mask(mask_paren, s.str.replace(r"^[\(](.*)[\)]$", r"\1", regex=True))
    mask_trail = s.str.endswith("-", na=False)
    s = s.mask(mask_trail, s.str[:-1])
    negative_mask = mask_paren | mask_trail

    # Extract K/M/B multiplier and strip suffix
    m = s.str.extract(KMB_PATTERN)
    suffix = m[0].fillna("") if not m.empty else ""
    mult = pd.Series(1.0, index=s.index, dtype=float)
    mult = mult.mask(suffix.str.lower() == "k", 1e3)
    mult = mult.mask(suffix.str.lower() == "m", 1e6)
    mult = mult.mask(suffix.str.lower() == "b", 1e9)
    s = s.str.replace(KMB_PATTERN.pattern, "", regex=True)

    # Strip currency and common separators
    s = s.str.replace(CURRENCY_PATTERN, "", regex=True)
    s = s.str.replace(",", "", regex=False)
    s = s.str.replace(" ", "", regex=False)

    # Percent
    percent_mask = s.str.contains(PERCENT_PATTERN, na=False)
    s = s.str.replace(PERCENT_PATTERN, "", regex=True)

    nums = pd.to_numeric(s, errors="coerce")
    nums = nums * mult
    nums = nums.mask(negative_mask, -nums)
    nums = nums.mask(percent_mask, nums / 100.0)
    nums = nums.replace([np.inf, -np.inf], np.nan)
    return nums


def is_numeric_candidate(series: pd.Series) -> bool:
    sample = series.dropna()
    if sample.empty:
        return False
    sample = sample.sample(min(100, len(sample)), random_state=42)
    parsed = normalize_numeric_series(sample)
    ratio = parsed.notna().mean()
    return ratio >= 0.6


# -----------------------------
# Type inference & utilities
# -----------------------------

def _detect_datetime_format_from_value(val: str) -> Optional[str]:
    for regex, fmt in DATE_REGEXES:
        if regex.match(val):
            return fmt
    return None


def infer_column_type(series: pd.Series, name: str) -> Tuple[str, Optional[str]]:
    """Return (type, datetime_format)."""
    s = series.dropna().astype(str).str.strip()
    if s.empty:
        return "text", None

    # Try date detection (regex + pandas parse success rate)
    sample_vals = s.sample(min(50, len(s)), random_state=42)
    regex_hits = sum(1 for v in sample_vals if _detect_datetime_format_from_value(v))
    if regex_hits / max(1, len(sample_vals)) >= 0.6:
        parsed = pd.to_datetime(s, errors="coerce")
        if parsed.notna().mean() >= 0.8:
            for v in sample_vals:
                fmt = _detect_datetime_format_from_value(v)
                if fmt:
                    return "date", fmt
            return "date", None

    # Numeric / percentage / currency
    if is_numeric_candidate(series):
        lower_name = name.lower()
        contains_pct = any("%" in str(x) for x in series.dropna().head(50))
        contains_curr = any(CURRENCY_PATTERN.search(str(x) or "") for x in series.dropna().head(50))
        if contains_pct or any(k in lower_name for k in ("percent", "pct", "rate")):
            return "percentage", None
        if contains_curr or any(k in lower_name for k in ("price", "revenue", "amount", "cost", "currency")):
            return "currency", None
        nums = normalize_numeric_series(series)
        if nums.notna().any() and (nums.dropna() % 1 == 0).mean() > 0.95:
            return "integer", None
        return "float", None

    # ID-like
    unique_ratio = series.nunique(dropna=True) / max(1, len(series.dropna()))
    lower = name.lower()
    if unique_ratio > 0.9 and any(k in lower for k in ("id", "uuid", "code", "identifier")):
        return "id", None

    # Categorical vs text
    if series.dropna().nunique() / max(1, len(series.dropna())) <= 0.2:
        return "categorical", None

    return "text", None


# -----------------------------
# Main adapter
# -----------------------------

def process_file_to_artifacts(
    local_path: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,  # kept for compatibility; not used (non-destructive)
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    raw_df, file_kind = _load_raw(local_path)
    return process_df_to_artifacts(
        raw_df,
        file_kind,
        sample_rows_for_llm=sample_rows_for_llm,
        metric_rename_heuristic=metric_rename_heuristic,
        header_row_override=header_row_override,
    )


def process_bytes_to_artifacts(
    data: bytes,
    kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    """In-memory variant of process_file_to_artifacts.

    Parameters
    - data: raw file bytes
    - kind: "csv" or "excel"
    - sample_rows_for_llm: number of sampled rows to include in payload.sample_rows
    - metric_rename_heuristic: kept for API compatibility
    """
    if kind not in ("csv", "excel"):
        raise ValueError("kind must be 'csv' or 'excel'")
    if kind == "excel":
        raw_df = pd.read_excel(io.BytesIO(data), sheet_name=0, header=None, dtype=object)
        file_kind = "excel"
    else:
        # CSV fast path from bytes
        raw_df = pd.read_csv(
            io.BytesIO(data),
            header=None,
            engine="c",
            dtype_backend="pyarrow",
        )
        file_kind = "csv"
    return process_df_to_artifacts(
        raw_df,
        file_kind,
        sample_rows_for_llm=sample_rows_for_llm,
        metric_rename_heuristic=metric_rename_heuristic,
        header_row_override=header_row_override,
    )


def process_df_to_artifacts(
    raw_df: pd.DataFrame,
    file_kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    rows_before = raw_df.shape[0]

    work = _drop_fully_blank_rows(raw_df).reset_index(drop=True)
    # Determine header row (override or auto-detect)
    if header_row_override is not None:
        header_row = int(header_row_override)
        header_confidence = 1.0
        method = "override"
    else:
        lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
        header_row, header_confidence = detect_header_row_simple(work, lookahead=lookahead)
        method = "auto_detected"

    raw_headers = work.iloc[header_row].tolist() if len(work) > header_row else [None] * work.shape[1]
    headers, header_issues = finalize_headers(raw_headers)
    start_row = header_row + 1

    body = work.iloc[start_row:].reset_index(drop=True)
    body.columns = headers

    # Remove rows that fully match headers (repeated header lines)
    hdr_norm = [str(h).strip() for h in headers]
    keep_mask = []
    for _, r in body.iterrows():
        vals = [str(x).strip() for x in r.tolist()]
        keep_mask.append(vals != hdr_norm)
    df = body.loc[keep_mask].reset_index(drop=True)

    # Normalize numeric columns (best-effort)
    numeric_cols: List[str] = []
    for col in df.columns:
        if is_numeric_candidate(df[col]):
            df[col] = normalize_numeric_series(df[col])
            numeric_cols.append(col)

    # Type inference & column summaries
    columns_meta: Dict[str, Any] = {}
    for col in df.columns:
        col_type, dt_fmt = infer_column_type(df[col], col)
        col_series = df[col]
        nn = col_series.dropna()
        null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
        unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
        vc = nn.value_counts().head(5)
        top_values = []
        for v, c in vc.items():
            val: Any = v
            if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                continue
            if isinstance(v, (np.floating, np.integer)):
                val = float(v)
            if isinstance(v, str) and len(v) > 300:
                val = v[:300]
            top_values.append({"value": val, "count": int(c)})

        entry: Dict[str, Any] = {
            "type": col_type,
            "null_pct": round(float(null_pct), 2),
            "unique_pct": round(float(unique_pct), 2),
            "top_values": top_values,
        }
        if col_type == "date":
            entry["datetime_format"] = dt_fmt
        entry["is_potential_dimension"] = False
        columns_meta[col] = entry

    # Heuristic for potential dimension in the first column
    if len(df.columns) > 0:
        first = df.columns[0]
        first_type = columns_meta[first]["type"]
        numeric_others = [c for c in df.columns[1:] if columns_meta[c]["type"] in ("integer", "float", "percentage", "currency")]
        if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
            columns_meta[first]["is_potential_dimension"] = True

    dataset_meta = {
        "rows": int(df.shape[0]),
        "columns": int(df.shape[1]),
        "column_names": list(map(str, df.columns.tolist())),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # Deterministic sample rows
    n = min(sample_rows_for_llm, len(df))
    sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)

    sample_rows: List[Dict[str, Any]] = []
    for _, r in sample.iterrows():
        obj = {}
        for k, v in r.to_dict().items():
            if isinstance(v, str) and len(v) > 300:
                obj[k] = v[:300]
            elif isinstance(v, (np.floating, np.integer)):
                obj[k] = float(v)
            else:
                obj[k] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
        sample_rows.append(obj)

    # Build v2 analysis hints and dataset summary
    hints, dataset_summary = build_analysis_hints(df, headers, header_row, float(header_confidence))
    # Minimal header_info with transposed signal
    is_transposed = False
    non_empty_final = [h for h in headers if str(h).strip() != ""]
    if non_empty_final:
        is_transposed = all(is_numeric_string(h) for h in non_empty_final)

    header_info = {
        "method": method,
        "header_row_index": int(header_row),
        "confidence": float(header_confidence),
        "original_headers": [str(x).strip() if x is not None else None for x in raw_headers],
        "final_headers": headers,
        "is_transposed": bool(is_transposed),
    }
    if header_issues:
        header_info["issues"] = header_issues

    payload: Dict[str, Any] = {
        "dataset": dataset_meta,
        "columns": columns_meta,
        "sample_rows": sample_rows,
        "cleaning_report": {
            "header_row": int(header_row),
            "renamed_columns": {},
            "numeric_columns": numeric_cols,
            "rows_before": int(rows_before),
            "rows_after": int(df.shape[0]),
            "file_kind": "xlsx" if file_kind == "excel" else "csv",
        },
        "mode": "full",
        "version": "1",
        # v2 additive fields (compact)
        "schema_version": "2.0",
        "header_info": header_info,
        "analysis_hints": hints,
        "dataset_summary": dataset_summary,
    }

    if file_kind == "excel":
        payload["excelInfo"] = {"sheet_used": 0, "sheet_name": None, "sheets_total": None}

    return ProcessResult(
        cleaned_df=df,
        payload=payload,
        cleaning_report=payload["cleaning_report"],
        rows=int(df.shape[0]),
        columns=int(df.shape[1]),
    )
</file>

<file path="run-preprocess/Procfile">
web: gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app
</file>

<file path="run-preprocess/project.toml">
# File: run-preprocess/project.toml

[build]
# Pin Python to a stable version with wheels for pyarrow/numpy
[[build.env]]
name = "GOOGLE_PYTHON_VERSION"
value = "3.12"
</file>

<file path="run-preprocess/README.md">
# Preprocess Service (Cloud Run)

Region: `europe-west4`  •  Project: `ai-data-analyser`  •  Bucket: `ai-data-analyser-files`

This service is the HTTP target of an Eventarc trigger for `google.cloud.storage.object.v1.finalized` events. When a raw file is uploaded under the `users/{uid}/sessions/{sid}/datasets/{datasetId}/raw/` prefix, it:

1. Downloads the raw CSV/XLSX bytes into memory (no `/tmp` in the happy path).
2. Runs the pipeline adapter (Polars CSV by default; Excel via pandas) to clean + profile.
3. Writes:
   - `cleaned/cleaned.parquet`
   - `metadata/payload.json`
   - `reports/cleaning_report.json`
4. Updates Firestore dataset doc with URIs, rows, columns, `status=ready`, and `ttlAt`.

## Run locally

```bash
pip install -r requirements.txt
export FILES_BUCKET=ai-data-analyser-files
export GCP_PROJECT=ai-data-analyser
uvicorn main:app --reload --port 8080
```

Health check:
```
curl http://localhost:8080/healthz
```

## Deploy (sketch)

From the repository root, you can deploy just this service via:

```
./backend/deploy-preprocess.ps1
```

The script enables required APIs (once), deploys Cloud Run `preprocess-svc`, and ensures the Eventarc trigger. After deployment, run `./backend/test.ps1` for a quick smoke test.

If you prefer a unified flow for all backend components, use `./backend/deploy.ps1`.

## Notes

- Excel policy: first sheet only; payload includes `excelInfo`.
- No column renames; instead flag potential dimensions in the payload.
- Deterministic sample of 50 rows for payload; truncate long strings.
- Parquet written with `pyarrow` and compression.
- Polars string API: we use `str.strip_chars()` (not `str.strip()`) for whitespace trimming in expressions to match the deployed Polars version. If you upgrade Polars, re-verify string namespace compatibility in `pipeline_adapter_polars.py`.

### Engine selection

- Default engine for CSV is Polars. Set via env var `PREPROCESS_ENGINE=polars|pandas` (default: `polars`).
- Excel files are handled by pandas (first sheet) regardless of engine.
- Core cleaning/payload logic is centralized in `pipeline_adapter.process_df_to_artifacts()` and shared by both adapters.

---

## Operational Runbook

### Verify Cloud Run health

1) Get service URL

```
gcloud run services describe preprocess-svc \
  --region=europe-west4 --format="value(status.url)"
```

2) Health check endpoint

```
curl -s "$(gcloud run services describe preprocess-svc \
  --region=europe-west4 --format='value(status.url)')/healthz"
```

### Verify Eventarc trigger

```
gcloud eventarc triggers describe preprocess-trigger \
  --location=europe-west4
```

Confirm:

- `eventFilters`: `type=google.cloud.storage.object.v1.finalized`, `bucket=ai-data-analyser-files`
- `destination`: run service `preprocess-svc`, path `/eventarc`, region `europe-west4`

### End-to-end smoke test

Prereqs: `functions/sign-upload-url` deployed and publicly reachable.

1) Request signed URL (replace values as needed)

```
SIGN_URL=$(gcloud functions describe sign-upload-url \
  --gen2 --region=europe-west4 --format='value(url)')

UID=demo-uid
SID=demo-sid
FILE=test_files/basic.csv
MIME=text/csv

FILENAME=$(basename "$FILE")
curl -s -H "X-User-Id: $UID" -H "X-Session-Id: $SID" \
  "$SIGN_URL?filename=$(python -c "import urllib.parse,sys;print(urllib.parse.quote(sys.argv[1]))" "$FILENAME")&size=$(stat -c%s "$FILE")&type=$(python -c "import urllib.parse;print(urllib.parse.quote('$MIME'))")" \
  | tee /tmp/sign_resp.json

DATASET=$(jq -r .datasetId </tmp/sign_resp.json)
PUT_URL=$(jq -r .url </tmp/sign_resp.json)
```

2) Upload file to signed URL

```
curl -X PUT -H "Content-Type: $MIME" --data-binary @"$FILE" "$PUT_URL"
```

3) Wait and list artifacts

```
sleep 8
gcloud storage ls \
  "gs://ai-data-analyser-files/users/$UID/sessions/$SID/datasets/$DATASET/**"
```

4) Inspect Firestore doc (from Console or gcloud) at:

`users/{uid}/sessions/{sid}/datasets/{datasetId}`

### Troubleshooting

- Container failed to start (PORT): ensure the service is launched with a PORT-aware entrypoint.
  - We use `GOOGLE_ENTRYPOINT=python main.py` and `main.py` starts uvicorn honoring `$PORT`.
- Build fails compiling `pyarrow`: ensure Python 3.12 wheels are used.
  - `project.toml` sets `GOOGLE_PYTHON_VERSION=3.12` and `deploy.ps1` passes the same build env var.
</file>

<file path="run-preprocess/requirements-dev.txt">
pytest==8.2.1
pytest-cov==5.0.0
</file>

<file path="run-preprocess/requirements.txt">
fastapi==0.111.0
uvicorn[standard]==0.30.0
gunicorn==22.0.0

# Pandas/Numpy/Arrow/Polars: keep pinned versions for Python <3.13 (deployment parity)
# and allow newer versions for Python >=3.13 (local dev on latest Python)
pandas==2.2.2; python_version < "3.13"
pandas>=2.2.3,<3; python_version >= "3.13"

numpy==2.0.2; python_version < "3.13"
numpy>=2.1.0,<3; python_version >= "3.13"

pyarrow==16.1.0; python_version < "3.13"
pyarrow>=16.1.0,<19; python_version >= "3.13"

openpyxl==3.1.5

polars[xlsx]==1.7.1; python_version < "3.13"
polars[xlsx]>=1.7.1,<2; python_version >= "3.13"

google-cloud-storage==2.16.0
google-cloud-firestore==2.16.0
google-cloud-logging==3.10.0
</file>

<file path="run-preprocess/runtime.txt">
3.12.*
</file>

<file path="run-preprocess/tests/fixtures/balance_sheet.csv">
Company ABC Balance Sheet,,,
Item,2023,2024,2025
Cash & Equivalents,1990,3389,4123
Short-Term Investments,19218,9907,10234
Accounts Receivable,850,910,1005
</file>

<file path="run-preprocess/tests/fixtures/multi_row_header.csv">
Region,Sales,Sales,Profit
Region,2023,2024,Profit
North,100,120,20
South,80,95,15
</file>

<file path="run-preprocess/tests/fixtures/title_rows.csv">
Consolidated Balance Sheet,,
Metric,2023,2024
Cash & Equivalents,1990,3389
Short-Term Investments,19218,9907
</file>

<file path="run-preprocess/tests/test_header_detection_simple.py">
import pandas as pd

try:
    from ..header_utils import detect_header_row_simple, finalize_headers, build_analysis_hints  # type: ignore
except Exception:  # pragma: no cover
    from header_utils import detect_header_row_simple, finalize_headers, build_analysis_hints  # type: ignore


def test_detect_header_with_title_rows():
    rows = [
        ["Consolidated Balance Sheet", "", ""],
        ["Metric", "2023", "2024"],
        ["Cash & Equivalents", "1990", "3389"],
        ["Short-Term Investments", "19218", "9907"],
    ]
    df = pd.DataFrame(rows)
    idx, conf = detect_header_row_simple(df, lookahead=5)
    assert idx == 1
    assert conf >= 0.3
    raw_headers = df.iloc[idx].tolist()
    final_headers, issues = finalize_headers(raw_headers)
    assert final_headers[0].lower().startswith("metric")
    data = df.iloc[idx + 1 :].reset_index(drop=True).copy()
    data.columns = final_headers
    hints, summary = build_analysis_hints(data, final_headers, idx, conf)
    assert isinstance(hints, dict)
    assert summary and "Header row" in summary or "Header row" in summary.capitalize()


def test_negative_numeric_row_not_header():
    rows = [
        ["100", "200", "300"],
        ["Name", "Value", "Score"],
        ["A", "1", "2"],
        ["B", "3", "4"],
    ]
    df = pd.DataFrame(rows)
    idx, conf = detect_header_row_simple(df, lookahead=4)
    assert idx == 1  # should prefer the textual header row
    assert conf > 0.3
</file>

<file path="run-preprocess/tests/test_integration_polars_balance_sheet.py">
from pathlib import Path

import os

try:
    from ..pipeline_adapter_polars import process_file_to_artifacts  # type: ignore
except Exception:  # pragma: no cover
    from pipeline_adapter_polars import process_file_to_artifacts  # type: ignore


def test_polars_integration_balance_sheet(tmp_path):
    # Ensure lookahead default is present for deterministic behavior
    os.environ.setdefault("PREPROCESS_HEADER_LOOKAHEAD", "12")

    # Locate fixture next to this test
    csv_path = Path(__file__).resolve().parent / "fixtures" / "balance_sheet.csv"
    assert csv_path.exists(), f"fixture missing: {csv_path}"

    result = process_file_to_artifacts(str(csv_path))
    payload = result.payload

    # v2 fields present
    assert payload.get("schema_version") == "2.0"
    header_info = payload.get("header_info") or {}
    hints = payload.get("analysis_hints") or {}
    assert isinstance(header_info, dict) and isinstance(hints, dict)

    # Header detection: expect row 1 (0-based) due to title row at 0
    assert header_info.get("header_row_index") == 1
    conf = float(header_info.get("confidence") or 0.0)
    assert 0.0 <= conf <= 1.0

    final_headers = header_info.get("final_headers") or []
    assert isinstance(final_headers, list) and len(final_headers) >= 2
    assert str(final_headers[0]).lower() in ("item", "metric")

    # Hints sanity
    assert isinstance(hints.get("numeric_columns"), list)
    assert isinstance(hints.get("temporal_columns"), list)
    assert hints.get("first_column_type") in ("dimension", "data")
    # Balance sheet-like: first column is label/dimension, many numeric columns
    assert hints.get("likely_pivoted") in (True, False)  # don't force value, just ensure present
</file>

<file path="run-preprocess/tests/test_polars_helpers.py">
import polars as pl
import math

# Import functions under test
try:
    from ..pipeline_adapter_polars import _drop_fully_blank_rows_pl, _detect_header_row_pl, _numeric_expr_for, NULL_TOKENS
except Exception:  # fallback for direct invocation
    from pipeline_adapter_polars import _drop_fully_blank_rows_pl, _detect_header_row_pl, _numeric_expr_for, NULL_TOKENS  # type: ignore


def test_drop_fully_blank_rows_pl():
    df = pl.DataFrame({
        "a": [None, " ", "value", None],
        "b": [None, "\t", "123", "-"],
    })
    # '-' is a NULL_TOKEN per shared constants
    cleaned = _drop_fully_blank_rows_pl(df)
    # Expect rows with any non-null/non-null-token value to remain
    # Row 0: all null -> drop
    # Row 1: whitespace only -> drop
    # Row 2: has values -> keep
    # Row 3: a=None, b='-' (null token) -> drop
    assert cleaned.height == 1
    assert cleaned.select(pl.first()).to_series()[0] == "value"


def test_detect_header_row_pl():
    # First row looks like header (alpha), second more numeric
    df = pl.DataFrame({
        "c0": ["Product", "A", "B"],
        "c1": ["Revenue", "100", "200"],
        "c2": ["Qty", "1", "3"],
    })
    # The adapter reads CSVs without header; emulate that by providing rows as values
    # Here, row 0 is header-like
    row_like = pl.DataFrame(
        [df.select("c0").to_series(), df.select("c1").to_series(), df.select("c2").to_series()],
    ).transpose(include_header=False)
    hdr_row = _detect_header_row_pl(row_like)
    assert hdr_row == 0


def test_numeric_expr_for_parsing():
    df = pl.DataFrame({
        "x": [
            "1,234",        # separators
            "(12.5)",       # parens negative
            "50%",          # percent
            "$3,000",       # currency
            "1.2k",         # K suffix
            "-",            # null token -> None
            None,
        ]
    })
    expr = _numeric_expr_for("x").alias("y")
    out = df.select(expr)
    got = out["y"].to_list()

    assert got[0] == 1234.0
    assert math.isclose(got[1], -12.5)
    assert math.isclose(got[2], 0.5)
    assert got[3] == 3000.0
    assert math.isclose(got[4], 1200.0)
    assert got[5] is None
    assert got[6] is None
</file>

<file path="shared/ast_validator.py">
"""
AST-based validation for user-provided analysis code.

Enforces:
- Only "def run(df, ctx):" is allowed as an entrypoint.
- Disallow exec/eval, __*__ dunders, attribute access to private attrs.
- Restrict imports to an allowlist.
- Forbid file/network/process/thread/system access.

This is a conservative validator intended for fast/complex executors.
"""
from __future__ import annotations

import ast
from typing import Iterable, Set


FORBIDDEN_NAMES: Set[str] = {
    "exec",
    "eval",
    "compile",
    "open",
    "__import__",
}

FORBIDDEN_MODULE_PREFIXES: Set[str] = {
    "os",
    "sys",
    "subprocess",
    "socket",
    "asyncio",
    "multiprocessing",
    "threading",
    "ctypes",
    "pathlib",
    "importlib",
    "pdb",
    "pickle",
    "dill",
    "requests",
    "urllib",
}

ALLOWED_IMPORTS_FAST: Set[str] = {
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    # stdlib
    "math",
    "statistics",
    "json",
    "io",
    "itertools",
    "functools",
    "collections",
    "re",
    "datetime",
}


class _Validator(ast.NodeVisitor):
    def __init__(self, allowlist: Iterable[str]) -> None:
        super().__init__()
        self.allowlist = set(allowlist)
        self.errors: list[str] = []

    def _err(self, msg: str) -> None:
        self.errors.append(msg)

    # Imports
    def visit_Import(self, node: ast.Import) -> None:  # noqa: N802
        for alias in node.names:
            name = (alias.asname or alias.name).split(".")[0]
            if not any(name == a or name.startswith(a + ".") for a in self.allowlist):
                self._err(f"Import not allowed: {alias.name}")
            if any(name == p or name.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
                self._err(f"Forbidden import: {alias.name}")
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:  # noqa: N802
        mod = node.module or ""
        root = mod.split(".")[0]
        if not any(root == a or root.startswith(a + ".") for a in self.allowlist):
            self._err(f"Import from not allowed: {mod}")
        if any(root == p or root.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
            self._err(f"Forbidden import from: {mod}")
        self.generic_visit(node)

    # Calls
    def visit_Call(self, node: ast.Call) -> None:  # noqa: N802
        # Detect forbidden builtins like eval/exec/open
        if isinstance(node.func, ast.Name):
            if node.func.id in FORBIDDEN_NAMES:
                self._err(f"Forbidden call: {node.func.id}")
        self.generic_visit(node)

    # Attributes
    def visit_Attribute(self, node: ast.Attribute) -> None:  # noqa: N802
        if node.attr.startswith("__") and node.attr.endswith("__"):
            self._err("Use of dunder attributes is not allowed")
        self.generic_visit(node)

    # Name access
    def visit_Name(self, node: ast.Name) -> None:  # noqa: N802
        if node.id.startswith("__") and node.id.endswith("__"):
            self._err("Use of dunder names is not allowed")
        self.generic_visit(node)


def validate(code: str, allowlist: Iterable[str] = ALLOWED_IMPORTS_FAST) -> tuple[bool, list[str]]:
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return False, [f"SyntaxError: {e}"]

    # Must define a top-level function run(df, ctx)
    has_run = False
    for node in tree.body:
        if isinstance(node, ast.FunctionDef) and node.name == "run":
            args = node.args.args
            if len(args) >= 2 and args[0].arg == "df" and args[1].arg == "ctx":
                has_run = True
                break
    if not has_run:
        return False, ["Missing required function: def run(df, ctx):"]

    v = _Validator(allowlist)
    v.visit(tree)
    return len(v.errors) == 0, v.errors
</file>

<file path="test.ps1">
# Config
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

$UID = "demo-uid"
$SID = "demo-sid"
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$ROOT_DIR   = Split-Path -Parent $SCRIPT_DIR
$FILE = (Join-Path $ROOT_DIR "test_files\basic.csv")
$MIME = "text/csv"

# 1) Cloud Run health (service is public right now)
$PROJECT_NUMBER = (gcloud projects describe $PROJECT_ID --format "value(projectNumber)" | Out-String).Trim()

# Prefer URL reported by Cloud Run; fallback to canonical project-number URL
$RUN_URL_SVC = (gcloud run services describe preprocess-svc --region=$REGION --format "value(status.url)" | Out-String).Trim()
if ($RUN_URL_SVC -and ($RUN_URL_SVC -match '^https?://')) {
  $RUN_URL = $RUN_URL_SVC
} else {
  $RUN_URL = "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app"
}
Write-Host "RUN_URL: $RUN_URL"

# Try health; if it fails, try alternate once, then continue without throwing
$healthOk = $false
try {
  $health = Invoke-RestMethod -Uri "$RUN_URL/healthz" -Method GET -TimeoutSec 10
  Write-Host "Health: $($health | ConvertTo-Json -Compress)"
  $healthOk = $true
} catch {
  Write-Host "Health check warning:" $_.Exception.Message
  # Pick alternate URL
  if ($RUN_URL -like "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app") {
    $RUN_URL_ALT = $RUN_URL_SVC
  } else {
    $RUN_URL_ALT = "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app"
  }
  if ($RUN_URL_ALT -and ($RUN_URL_ALT -match '^https?://') -and ($RUN_URL_ALT -ne $RUN_URL)) {
    Write-Host "Retrying health on alternate URL: $RUN_URL_ALT"
    try {
      $health2 = Invoke-RestMethod -Uri "$RUN_URL_ALT/healthz" -Method GET -TimeoutSec 10
      Write-Host "Health (alt): $($health2 | ConvertTo-Json -Compress)"
      $RUN_URL = $RUN_URL_ALT
      $healthOk = $true
    } catch {
      Write-Host "Health check warning (alt):" $_.Exception.Message
    }
  }
}
if (-not $healthOk) {
  Write-Host "Proceeding despite health probe warnings (likely local DNS/routing)."
}

# 2) Get signed URL (unchanged)
$SIGN_URL = (gcloud functions describe sign-upload-url --gen2 --region=$REGION --format "value(url)" | Out-String).Trim()
Write-Host "SIGN_URL: $SIGN_URL"
if (-not $SIGN_URL -or -not ($SIGN_URL -match '^https?://')) { throw "SIGN_URL invalid or empty" }

if (-not (Test-Path $FILE)) { throw "Test file not found at $FILE" }
$SIZE = (Get-Item $FILE).Length
$FILENAME = [System.IO.Path]::GetFileName($FILE)
$encName = [System.Uri]::EscapeDataString($FILENAME)
$encMime = [System.Uri]::EscapeDataString($MIME)
$reqUri = ("{0}?filename={1}&size={2}&type={3}" -f $SIGN_URL, $encName, $SIZE, $encMime)

$headers = @{
  "X-User-Id"    = $UID
  "X-Session-Id" = $SID
}
Write-Host "Signed URL request URI: $reqUri"
try {
  $resp = Invoke-RestMethod -Uri $reqUri -Headers $headers -Method GET
} catch {
  Write-Host "Signed URL request failed:" $_.Exception.Message
  throw
}

Write-Host "DatasetId: $($resp.datasetId)"
if (-not $resp.url) { throw "Signed URL missing in response" }

# 3) Upload
Invoke-WebRequest -Uri $resp.url -Method PUT -InFile $FILE -ContentType $MIME | Out-Null
Write-Host "Upload complete."

# 4) Wait and list artifacts
Start-Sleep -Seconds 30
gcloud storage ls "gs://$BUCKET/users/$UID/sessions/$SID/datasets/$($resp.datasetId)/**"

# 5) Fetch Firestore dataset status (best-effort)
try {
  $TOKEN = (gcloud auth print-access-token | Out-String).Trim()
  $docUrl = "https://firestore.googleapis.com/v1/projects/$PROJECT_ID/databases/(default)/documents/users/$UID/sessions/$SID/datasets/$($resp.datasetId)"
  $doc = Invoke-RestMethod -Uri $docUrl -Headers @{ Authorization = "Bearer $TOKEN" } -Method GET -TimeoutSec 10
  $status = $doc.fields.status.stringValue
  if ($status) {
    Write-Host "Firestore status: $status"
  } else {
    Write-Host "Firestore document retrieved (status field missing):"
  }
} catch {
  Write-Host "Firestore read warning:" $_.Exception.Message
}
# 6) Chat SSE smoke test (best-effort)
$CHAT_URL = (gcloud functions describe chat --gen2 --region=$REGION --format "value(url)" | Out-String).Trim()
Write-Host "CHAT_URL: $CHAT_URL"
if ($CHAT_URL -and ($CHAT_URL -match '^https?://')) {
  # Use a metadata-only question for a fast "done" event
  $body = @{ uid = $UID; sessionId = $SID; datasetId = $resp.datasetId; question = "How many rows?" } | ConvertTo-Json -Compress
  $payloadPath = Join-Path $env:TEMP "chat_payload.json"
  $sseOut = Join-Path $env:TEMP ("chat_sse_{0}.log" -f ([Guid]::NewGuid().ToString('N')))
  $body | Out-File -FilePath $payloadPath -Encoding utf8 -NoNewline
  try {
    & curl.exe -sN `
      -H "Origin: http://localhost:5173" `
      -H "Content-Type: application/json" `
      -H "X-User-Id: $UID" `
      -H "X-Session-Id: $SID" `
      --data-binary "@$payloadPath" `
      --max-time 60 `
      $CHAT_URL 2>&1 | Tee-Object -FilePath $sseOut | Out-Null

    # Try to parse the final done event and verify results in GCS
    try {
      $doneMatch = Select-String -Path $sseOut -Pattern 'data:\s*\{.*"type"\s*:\s*"done"' | Select-Object -Last 1
      if ($doneMatch) {
        $jsonLine = ($doneMatch.Line -replace '^data:\s*','')
        $evt = $jsonLine | ConvertFrom-Json
        $msgId = $evt.data.messageId
        $uris  = $evt.data.uris
        if ($msgId) {
          $resultsPrefix = "users/$UID/sessions/$SID/results/$msgId"
          Write-Host "Verifying analysis artifacts at gs://$BUCKET/$resultsPrefix/"
          gcloud storage ls "gs://$BUCKET/$resultsPrefix/**"
          # Verify HTTPS signed URLs exist and fetch 200
          if ($uris -and $uris.table -and ($uris.table -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.table -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: table.json signed URL fetchable" } catch { Write-Host "Warning: table.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS table URL in done event" }
          if ($uris -and $uris.metrics -and ($uris.metrics -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.metrics -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: metrics.json signed URL fetchable" } catch { Write-Host "Warning: metrics.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS metrics URL in done event" }
          if ($uris -and $uris.chartData -and ($uris.chartData -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.chartData -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: chart_data.json signed URL fetchable" } catch { Write-Host "Warning: chart_data.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS chartData URL in done event" }
        } else {
          Write-Host "SSE parse warning: messageId missing in done event"
        }
      } else {
        Write-Host "SSE parse warning: no done event found"
      }
    } catch {
      Write-Host "SSE parse warning:" $_.Exception.Message
    }
  } catch {
  } finally {
    if (Test-Path $payloadPath) { Remove-Item $payloadPath -Force }
    if (Test-Path $sseOut) { Remove-Item $sseOut -Force }
  }
} else {
  Write-Host "SSE test skipped: CHAT_URL invalid"
}

# =============================
# Optional: XLSX smoke test
# =============================
try {
  $XLSX_FILE = (Join-Path $ROOT_DIR "test_files\test.xlsx")
  if (-not (Test-Path $XLSX_FILE)) {
    $maybe = Get-ChildItem -Path (Join-Path $ROOT_DIR "test_files") -Filter *.xlsx -Recurse -ErrorAction SilentlyContinue | Select-Object -First 1
    if ($maybe) { $XLSX_FILE = $maybe.FullName } else { $XLSX_FILE = $null }
  }

  if ($XLSX_FILE -and (Test-Path $XLSX_FILE)) {
    Write-Host "Running XLSX smoke test with: $XLSX_FILE"
    $MIME2 = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    $SIZE2 = (Get-Item $XLSX_FILE).Length
    $FILENAME2 = [System.IO.Path]::GetFileName($XLSX_FILE)
    $encName2 = [System.Uri]::EscapeDataString($FILENAME2)
    $encMime2 = [System.Uri]::EscapeDataString($MIME2)
    $reqUri2 = ("{0}?filename={1}&size={2}&type={3}" -f $SIGN_URL, $encName2, $SIZE2, $encMime2)

    $headers2 = @{ "X-User-Id" = $UID; "X-Session-Id" = $SID }
    $resp2 = Invoke-RestMethod -Uri $reqUri2 -Headers $headers2 -Method GET
    if (-not $resp2.url) { throw "Signed URL missing for XLSX" }
    Invoke-WebRequest -Uri $resp2.url -Method PUT -InFile $XLSX_FILE -ContentType $MIME2 | Out-Null
    Write-Host "XLSX upload complete."
    Start-Sleep -Seconds 30
    gcloud storage ls "gs://$BUCKET/users/$UID/sessions/$SID/datasets/$($resp2.datasetId)/**"
  } else {
    Write-Host "XLSX smoke test skipped: no .xlsx file found in test_files."
  }
} catch {
  Write-Host "XLSX smoke test warning:" $_.Exception.Message
}
</file>

</files>
