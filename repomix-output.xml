This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
backend/deploy-analysis.ps1
backend/deploy-preprocess.ps1
backend/docs/api.md
backend/docs/ctx_contract.md
backend/docs/ops_checklist.md
backend/docs/payload_schema.json
backend/docs/payload_v1_vs_v2.md
backend/docs/runtime_flags.md
backend/functions/orchestrator/gemini_client.py
backend/functions/orchestrator/main.py
backend/functions/orchestrator/requirements.txt
backend/functions/orchestrator/sandbox_runner.py
backend/functions/orchestrator/worker.py
backend/functions/sign_upload_url/main.py
backend/functions/sign_upload_url/requirements.txt
backend/run-preprocess/header_utils.py
backend/run-preprocess/main.py
backend/run-preprocess/pipeline_adapter_polars.py
backend/run-preprocess/pipeline_adapter.py
backend/run-preprocess/Procfile
backend/run-preprocess/project.toml
backend/run-preprocess/README.md
backend/run-preprocess/requirements-dev.txt
backend/run-preprocess/requirements.txt
backend/run-preprocess/runtime.txt
backend/run-preprocess/tests/fixtures/balance_sheet.csv
backend/run-preprocess/tests/fixtures/multi_row_header.csv
backend/run-preprocess/tests/fixtures/title_rows.csv
backend/run-preprocess/tests/test_header_detection_simple.py
backend/run-preprocess/tests/test_integration_polars_balance_sheet.py
backend/run-preprocess/tests/test_polars_helpers.py
backend/shared/ast_validator.py
backend/test.ps1
frontend/.env.example
frontend/.firebaserc
frontend/.gitignore
frontend/cloudbuild.yaml
frontend/firebase.json
frontend/index.html
frontend/package.json
frontend/postcss.config.js
frontend/src/App.tsx
frontend/src/components/ChatHeader.tsx
frontend/src/components/ChatInput.tsx
frontend/src/components/ChatMessage.tsx
frontend/src/components/ChatSidebar.tsx
frontend/src/components/DarkModeToggle.tsx
frontend/src/components/renderers/ChartRenderer.tsx
frontend/src/components/renderers/TableRenderer.tsx
frontend/src/components/ui/button.tsx
frontend/src/components/ui/scroll-area.tsx
frontend/src/components/ui/textarea.tsx
frontend/src/components/ui/utils.ts
frontend/src/context/AuthContext.tsx
frontend/src/index.css
frontend/src/lib/firebase.ts
frontend/src/main.tsx
frontend/src/services/api.ts
frontend/src/services/firestore.ts
frontend/src/styles/globals.css
frontend/src/styles/tailwind.css
frontend/tailwind.config.js
frontend/vite.config.ts
PROGRESS.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backend/docs/ctx_contract.md">
# ctx Contract

This document specifies the `ctx` object passed into the user-generated analysis function `def run(df, ctx):`.

The goals for `ctx` are:
- Provide enough context to generate compliant, efficient code.
- Enforce budget and safety constraints (time, memory, charts).
- Keep content compact and deterministic.

## Top-level structure

```json
{
  "dataset": {
    "rows": 0,
    "columns": 0,
    "column_names": ["..."],
    "dtypes": {"col": "dtype"}
  },
  "limits": {
    "maxCharts": 3,
    "timeBudgetSec": 10,
    "memoryMB": 512,
    "sampleRowsForDisplay": 50
  },
  "allowlist": [
    "pandas", "numpy", "matplotlib", "seaborn",
    "math", "statistics", "json", "io", "itertools", "functools", "collections", "re", "datetime"
  ],
  "provenance": {
    "sessionId": "...",
    "datasetId": "...",
    "messageId": "..."
  },
  "hazards": [
    "free_text:notes",
    "high_cardinality:segment"
  ],
  "chartability": {
    "numeric": ["revenue", "users"],
    "categorical_low_card": ["region"]
  },
  "cost": {
    "size_grade": "S",
    "est_rows_for_fast": 12500,
    "heavy_ops_caveat": ""
  },
  "seed": 42,
  "notes": [
    "No network access or file writes allowed.",
    "Prefer vectorized operations over loops.",
    "Avoid long-running operations."
  ]
}
```

## Field reference

- **dataset**
  - **rows**: Total number of rows in the cleaned dataset.
  - **columns**: Total number of columns in the cleaned dataset.
  - **column_names**: Ordered list of column names for the DataFrame.
  - **dtypes**: Mapping of column name to dtype string (e.g., `float64`, `int64`, `object`).

- **limits**
  - **maxCharts**: Maximum number of charts to produce (images returned as base64 PNG).
  - **timeBudgetSec**: Soft wall-clock budget; executors enforce hard timeouts.
  - **memoryMB**: Approximate memory budget for fast vs complex executors.
  - **sampleRowsForDisplay**: Maximum row count to include in small display tables.

- **allowlist**
  - Libraries and stdlib modules that may be imported by generated code. Any import outside this list fails validation.

- **provenance**
  - Identifiers useful for logging and correlating results.

- **hazards** (optional)
  - Flags derived from payload analysis (e.g., free-text columns, high cardinality) to steer safer code paths.

- **chartability** (optional)
  - Suggested columns that make good candidates for basic charts.

- **cost** (optional)
  - Hints about dataset size and expected fast-path capacity.

- **seed**
  - Fixed seed (e.g., 42) to ensure deterministic sampling and plotting randomness.

- **notes**
  - Additional guardrails or instructions for generated code.

## Example usage in `run(df, ctx)`

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def run(df, ctx):
    # Respect limits
    max_charts = ctx.get("limits", {}).get("maxCharts", 3)

    # Simple summary
    summary = f"Rows: {ctx['dataset']['rows']}, Cols: {ctx['dataset']['columns']}"

    # Example table (head limited by display budget)
    display_n = ctx.get("limits", {}).get("sampleRowsForDisplay", 50)
    table = df.head(min(5, display_n)).to_dict(orient="records")

    charts = []
    if max_charts > 0:
        # Example histogram if a numeric column exists
        numeric_cols = [c for c, t in ctx["dataset"]["dtypes"].items() if t.startswith("int") or t.startswith("float")]
        if numeric_cols:
            col = numeric_cols[0]
            import io, base64
            fig, ax = plt.subplots(figsize=(5, 3))
            df[col].dropna().hist(ax=ax, bins=20)
            ax.set_title(f"Histogram of {col}")
            buf = io.BytesIO()
            fig.tight_layout()
            fig.savefig(buf, format="png")
            plt.close(fig)
            png_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
            charts.append({"png_base64": png_b64, "caption": f"Histogram of {col}"})

    return {
        "summary": summary,
        "tables": [table],
        "charts": charts,
        "notes": []
    }
```
</file>

<file path="backend/docs/ops_checklist.md">
# Ops Checklist (GCS, Eventarc, Firestore TTL)

Project: `ai-data-analyser`  •  Region: `europe-west4`  •  Bucket: `ai-data-analyser-files`

## 1) GCS CORS for signed URL uploads

Create `cors.json` locally:
```json
[
  {
    "origin": ["http://localhost:3000"],
    "method": ["PUT", "GET", "HEAD"],
    "responseHeader": ["Content-Type", "Authorization", "x-goog-meta-*"],
    "maxAgeSeconds": 3600
  }
]
```

Apply CORS:
```bash
gsutil cors set cors.json gs://ai-data-analyser-files
gsutil cors get gs://ai-data-analyser-files
```

## 2) GCS lifecycle: prefix-scoped 1-day TTL

Create `lifecycle.json`:
```json
{
  "rule": [
    {
      "action": {"type": "Delete"},
      "condition": {"age": 1, "matchesPrefix": ["users/"]}
    }
  ]
}
```

Apply lifecycle:
```bash
gsutil lifecycle set lifecycle.json gs://ai-data-analyser-files
gsutil lifecycle get gs://ai-data-analyser-files
```

## 3) Eventarc trigger → Cloud Run preprocess

Example (adjust service account as needed):
```bash
gcloud eventarc triggers create preprocess-trigger \
  --location=europe-west4 \
  --event-filters="type=google.cloud.storage.object.v1.finalized" \
  --event-filters="bucket=ai-data-analyser-files" \
  --destination-run-service=preprocess-svc \
  --destination-run-region=europe-west4 \
  --service-account=preprocess-svc@ai-data-analyser.iam.gserviceaccount.com
```

Verify trigger:
```bash
gcloud eventarc triggers describe preprocess-trigger --location=europe-west4
```

## 4) Firestore TTL (1 day)

Enable TTL on the following collection groups via Console (recommended) or API:
- `users/*/sessions/*`  → field: `ttlAt`
- `users/*/sessions/*/datasets/*`  → field: `ttlAt`
- `users/*/sessions/*/messages/*`  → field: `ttlAt`

Write policy:
- On session creation, set `ttlAt = createdAt + 1 day`.
- On session close, set `ttlAt = now` before deletion (belt-and-suspenders).

## 5) Secret Manager: Gemini API key

Create or update:
```bash
gcloud secrets versions add GEMINI_API_KEY --data-file=gemini_key.txt \
  --project=ai-data-analyser
```

Grant access to service accounts (orchestrator, executors):
```bash
gcloud secrets add-iam-policy-binding GEMINI_API_KEY \
  --member=serviceAccount:orchestrator-func@ai-data-analyser.iam.gserviceaccount.com \
  --role=roles/secretmanager.secretAccessor \
  --project=ai-data-analyser
```

## 6) Budget alerts and Monitoring (high level)

- Set a budget in Cloud Billing with alerts at 70% and 90%.
- Create dashboards for:
  - Error rates by stage (preprocess, classify, validate, run)
  - p95 latencies
  - Complex-path ratio and failover counts

## 7) Scheduler → usage-watcher Function

Create scheduler job (every 10 minutes):
```bash
gcloud scheduler jobs create http usage-watcher \
  --schedule="*/10 * * * *" \
  --uri="https://<cloudfunctions-url>/usage-watcher" \
  --http-method=POST \
  --oauth-service-account-email=usage-watcher-func@ai-data-analyser.iam.gserviceaccount.com \
  --location=europe-west4
```
</file>

<file path="backend/docs/payload_v1_vs_v2.md">
# Payload examples: v1 (baseline) vs v2 (additive)

Below are minimal examples illustrating the additive v2 fields. The schema at `backend/docs/payload_schema.json` validates v1 only; v2 fields are optional and ignored by v1 validators.

## v1 (current schema)
```json
{
  "dataset": { "rows": 123, "columns": 4, "column_names": ["Metric", "2023", "2024", "2025"], "dtypes": {"Metric": "string", "2023": "float64", "2024": "float64", "2025": "float64"} },
  "columns": {
    "Metric": { "type": "text", "null_pct": 0.0, "unique_pct": 100.0, "top_values": [] },
    "2023": { "type": "float", "null_pct": 0.0, "unique_pct": 95.0, "top_values": [] }
  },
  "sample_rows": [ { "Metric": "Cash", "2023": 1990.0, "2024": 3389.0 } ],
  "cleaning_report": { "header_row": 1, "renamed_columns": {}, "numeric_columns": ["2023", "2024"], "rows_before": 130, "rows_after": 123, "file_kind": "csv" },
  "mode": "full",
  "version": "1"
}
```

## v2 (additive, compact)
```json
{
  "dataset": { "rows": 123, "columns": 4, "column_names": ["Metric", "2023", "2024", "2025"], "dtypes": {"Metric": "string", "2023": "float64", "2024": "float64", "2025": "float64"} },
  "columns": {
    "Metric": { "type": "text", "null_pct": 0.0, "unique_pct": 100.0, "top_values": [] },
    "2023": { "type": "float", "null_pct": 0.0, "unique_pct": 95.0, "top_values": [] }
  },
  "sample_rows": [ { "Metric": "Cash", "2023": 1990.0, "2024": 3389.0 } ],
  "cleaning_report": { "header_row": 1, "renamed_columns": {}, "numeric_columns": ["2023", "2024"], "rows_before": 130, "rows_after": 123, "file_kind": "csv" },
  "mode": "full",
  "version": "1",

  "schema_version": "2.0",
  "header_info": {
    "method": "auto_detected",
    "header_row_index": 1,
    "confidence": 0.78,
    "original_headers": ["Metric", "2023", "2024", "2025"],
    "final_headers": ["Metric", "2023", "2024", "2025"],
    "is_transposed": false
  },
  "analysis_hints": {
    "detected_header_row": 1,
    "header_confidence": 0.78,
    "pct_missing_total": 0.02,
    "first_column_type": "dimension",
    "likely_pivoted": true,
    "temporal_columns": [1, 2, 3],
    "numeric_columns": [1, 2, 3]
  },
  "dataset_summary": "Dataset has 123 rows and 4 columns. Header row 1 detected (confidence 0.78). Structure: first column dimension, pivoted=true."
}
```
</file>

<file path="backend/docs/runtime_flags.md">
# Runtime Flags (Firestore: `config/runtime_flags`)

Default document:
```json
{
  "allowComplex": true,
  "perHourLimit": 10,
  "perDayLimit": 60,
  "sampleRowsForLLM": 50,
  "metricRenameHeuristic": false,
  "maxCharts": 3
}
```

Notes:
- `allowComplex` is toggled by the Scheduler-driven `usage-watcher` function.
- `metricRenameHeuristic=false` preserves original column names; the pipeline emits `is_potential_dimension` flags instead.
- `sampleRowsForLLM` sets payload sample size and the `ctx.limits.sampleRowsForDisplay` hint.
</file>

<file path="backend/run-preprocess/header_utils.py">
"""
Header detection and metadata helpers (simple, pragmatic).

Provides:
- detect_header_row_simple(df, lookahead=12) -> (idx, confidence)
- finalize_headers(raw_headers) -> (final_headers, issues)
- build_analysis_hints(df_after_header, final_headers, header_row_index, confidence) -> (hints, dataset_summary)

Design goals:
- Minimal signals: alpha_ratio, non_empty_ratio, distinctness, next_row_numeric.
- Year-like used only as a tie-breaker.
- Confidence normalized to 0..1 (average of signals).
- Keep payload lightweight; no per-signal dumps in payload.
"""
from __future__ import annotations

import re
from typing import Any, Dict, List, Tuple

import pandas as pd
from pandas.api import types as ptypes

YEAR_RE = re.compile(r"^(19|20)\d{2}$")
CURRENCY_PREFIX_RE = re.compile(r"^[\$€£¥]")
GENERIC_COL_RE = re.compile(r"^(col|column|unnamed)_?\d*$", re.IGNORECASE)


def _is_empty_token(s: str) -> bool:
    if s is None:
        return True
    s2 = str(s).strip().lower()
    return s2 in ("", "none", "nan", "null", "unnamed")


def is_numeric_string(s: str) -> bool:
    if s is None:
        return False
    t = str(s).strip()
    if t == "":
        return False
    # Tolerate commas, currency symbols, parentheses, percent
    t = (
        t.replace(",", "")
        .replace("(", "-")
        .replace(")", "")
        .replace("%", "")
        .replace("$", "")
        .replace("€", "")
        .replace("£", "")
        .replace("¥", "")
    )
    try:
        float(t)
        return True
    except Exception:
        return False


def _row_metrics(vals: List[str]) -> Tuple[float, float, float]:
    # alpha_ratio, distinctness, non_empty_ratio (returned separately for clarity)
    if not vals:
        return 0.0, 0.0, 0.0
    non_empty = [v for v in vals if v != ""]
    if not non_empty:
        return 0.0, 0.0, 0.0
    alpha_ratio = sum(1 for v in non_empty if re.search(r"[A-Za-z]", v)) / len(non_empty)
    distinctness = len(set(non_empty)) / len(non_empty)
    non_empty_ratio = len(non_empty) / max(1, len(vals))
    return float(alpha_ratio), float(distinctness), float(non_empty_ratio)


def detect_header_row_simple(df: pd.DataFrame, lookahead: int = 12) -> Tuple[int, float]:
    """Return (best_row_index, confidence) using 4 simple signals.

    Signals per candidate row r (first `lookahead` rows):
    - alpha_ratio: share of non-empty cells containing letters
    - non_empty_ratio: share of non-empty cells
    - distinctness: unique/total among non-empty
    - next_row_numeric: share of non-empty cells in row r+1 that are numeric-like

    Score is the average of the four signals (0..1). Year-like content is used
    only as a tie-breaker when scores are nearly equal.
    """
    max_r = min(len(df), max(1, lookahead))
    best_idx = 0
    best_eff_score = -1.0
    best_conf_score = 0.0
    best_year_ratio = -1.0

    lower_empty_tokens = {"", "none", "nan", "null"}
    for r in range(max_r):
        # fillna before string cast to avoid literal 'None'
        row = df.iloc[r].fillna("").astype(str).str.strip()
        vals_raw = row.tolist()
        vals = ["" if str(v).strip().lower() in lower_empty_tokens else str(v).strip() for v in vals_raw]
        alpha_ratio, distinctness, non_empty_ratio = _row_metrics(vals)
        non_empty_count = sum(1 for v in vals if v != "")
        if non_empty_ratio < 0.5 or non_empty_count < 2:
            continue  # skip mostly empty rows

        next_row_numeric = 0.0
        if r + 1 < len(df):
            nxt = df.iloc[r + 1].fillna("").astype(str).str.strip()
            nxt_vals = [x for x in nxt.tolist() if str(x).strip().lower() not in lower_empty_tokens]
            if nxt_vals:
                next_row_numeric = sum(1 for v in nxt_vals if is_numeric_string(v)) / len(nxt_vals)

        score_raw = (alpha_ratio + non_empty_ratio + distinctness + next_row_numeric) / 4.0

        # Heuristic boost/penalty layer (does not affect confidence, only selection)
        ne = [v for v in vals if v != ""]
        year_like_ratio = 0.0
        if ne:
            year_like_ratio = sum(1 for v in ne if YEAR_RE.match(v)) / len(ne)

        # Prefer a year-header pattern: majority of columns from col1 are years,
        # and the first column is empty or non-numeric (dimension/title)
        from_c1 = [v for v in vals[1:] if v != ""] if len(vals) > 1 else []
        year_like_from_c1 = 0.0
        if from_c1:
            year_like_from_c1 = sum(1 for v in from_c1 if YEAR_RE.match(v)) / len(from_c1)
        first_tok = vals[0] if vals else ""
        first_tok_numeric = is_numeric_string(first_tok)
        year_header_candidate = bool(year_like_from_c1 >= 0.6 and (first_tok == "" or not first_tok_numeric))

        # Penalize currency-like data rows: many columns look like currency values
        currency_ratio_from_c1 = 0.0
        if from_c1:
            currency_ratio_from_c1 = sum(1 for v in from_c1 if CURRENCY_PREFIX_RE.match(v)) / len(from_c1)
        currency_row_candidate = bool(currency_ratio_from_c1 >= 0.5 and first_tok != "")

        boost = 0.4 if year_header_candidate else 0.0
        penalty = 0.4 if currency_row_candidate else 0.0
        score_eff = max(0.0, min(1.0, score_raw + boost - penalty))

        # Selection uses effective score; confidence remains based on raw score
        if score_eff > best_eff_score:
            best_idx, best_eff_score, best_conf_score, best_year_ratio = r, score_eff, score_raw, year_like_ratio
        elif abs(score_eff - best_eff_score) <= 0.02 and year_like_ratio > best_year_ratio:
            best_idx, best_eff_score, best_conf_score, best_year_ratio = r, score_eff, score_raw, year_like_ratio

        # End candidate loop

    confidence = max(0.0, min(1.0, float(best_conf_score if best_eff_score >= 0.0 else 0.0)))
    return int(best_idx), float(confidence)


def finalize_headers(raw_headers: List[Any]) -> Tuple[List[str], List[Dict[str, Any]]]:
    """Replace empty/generic with col_i, ensure uniqueness, return issues list.
    Issues only for replacements of empty/generic headers.
    """
    final: List[str] = []
    issues: List[Dict[str, Any]] = []
    for i, h in enumerate(raw_headers):
        s = None if h is None else str(h).strip()
        if not s or _is_empty_token(s) or GENERIC_COL_RE.match(s):
            assigned = f"col_{i+1}"
            issues.append({"col_index": i, "reason": "empty_or_generic_header", "original": h, "assigned": assigned})
            final.append(assigned)
        else:
            final.append(s)

    # enforce uniqueness with suffixes
    seen: Dict[str, int] = {}
    uniq: List[str] = []
    for h in final:
        cnt = seen.get(h, 0)
        if cnt == 0:
            uniq.append(h)
        else:
            uniq.append(f"{h}_{cnt+1}")
        seen[h] = cnt + 1
    return uniq, issues


def _is_year_or_numeric_header(h: Any) -> bool:
    if h is None:
        return False
    s = str(h).strip()
    if s == "" or _is_empty_token(s):
        return False
    return bool(YEAR_RE.match(s) or is_numeric_string(s))


def build_analysis_hints(
    df: pd.DataFrame,
    final_headers: List[str],
    header_row_index: int,
    confidence: float,
) -> Tuple[Dict[str, Any], str]:
    """Compute compact analysis hints and a one-liner summary.

    - df: cleaned DataFrame with final headers already assigned
    """
    n_rows, n_cols = df.shape
    total_cells = max(1, n_rows * max(1, n_cols))
    n_missing_total = int(df.isna().sum().sum())
    pct_missing_total = float(n_missing_total / total_cells)

    # numeric columns by dtype or convertible ratio > 0.6
    numeric_columns: List[int] = []
    for i, c in enumerate(df.columns):
        ser = df[c]
        is_num = ptypes.is_numeric_dtype(ser)
        if not is_num:
            sample = ser.dropna().astype(str).head(50)
            if not sample.empty:
                ratio = sample.apply(is_numeric_string).mean()
                is_num = bool(ratio and float(ratio) > 0.6)
        if is_num:
            numeric_columns.append(i)

    # temporal columns: header name year-like or series parseable ratio > 0.6
    temporal_columns: List[int] = []
    for i, name in enumerate(final_headers[: len(df.columns)]):
        is_temporal = bool(YEAR_RE.match(str(name)))
        if not is_temporal:
            ser = df.iloc[:, i]
            try:
                parsed = pd.to_datetime(ser.dropna().astype(str).head(50), errors="coerce")
                if not parsed.empty and parsed.notna().mean() > 0.6:
                    is_temporal = True
            except Exception:
                is_temporal = False
        if is_temporal:
            temporal_columns.append(i)

    # first column type heuristic
    first_column_type = "data"
    if n_cols > 0:
        first_ser = df.iloc[:, 0]
        first_is_textual = not ptypes.is_numeric_dtype(first_ser)
        numeric_others = [i for i in range(1, n_cols) if i in numeric_columns]
        if first_is_textual and len(numeric_others) >= max(1, int(0.5 * max(0, n_cols - 1))):
            first_column_type = "dimension"

    likely_pivoted = bool(first_column_type == "dimension" and len(numeric_columns) >= max(1, int(0.5 * max(0, n_cols - 1))))

    hints: Dict[str, Any] = {
        "detected_header_row": int(header_row_index),
        "header_confidence": float(confidence),
        "pct_missing_total": float(pct_missing_total),
        "first_column_type": first_column_type,
        "likely_pivoted": likely_pivoted,
        "temporal_columns": temporal_columns,
        "numeric_columns": numeric_columns,
    }

    # concise summary
    dataset_summary = (
        f"Dataset has {n_rows} rows and {n_cols} columns. "
        f"Header row {header_row_index} detected (confidence {confidence:.2f}). "
        f"Structure: first column {first_column_type}, pivoted={likely_pivoted}."
    )

    return hints, dataset_summary
</file>

<file path="backend/run-preprocess/Procfile">
web: gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app
</file>

<file path="backend/run-preprocess/project.toml">
# File: run-preprocess/project.toml

[build]
# Pin Python to a stable version with wheels for pyarrow/numpy
[[build.env]]
name = "GOOGLE_PYTHON_VERSION"
value = "3.12"
</file>

<file path="backend/run-preprocess/requirements-dev.txt">
pytest==8.2.1
pytest-cov==5.0.0
</file>

<file path="backend/run-preprocess/runtime.txt">
3.12.*
</file>

<file path="backend/run-preprocess/tests/fixtures/balance_sheet.csv">
Company ABC Balance Sheet,,,
Item,2023,2024,2025
Cash & Equivalents,1990,3389,4123
Short-Term Investments,19218,9907,10234
Accounts Receivable,850,910,1005
</file>

<file path="backend/run-preprocess/tests/fixtures/multi_row_header.csv">
Region,Sales,Sales,Profit
Region,2023,2024,Profit
North,100,120,20
South,80,95,15
</file>

<file path="backend/run-preprocess/tests/fixtures/title_rows.csv">
Consolidated Balance Sheet,,
Metric,2023,2024
Cash & Equivalents,1990,3389
Short-Term Investments,19218,9907
</file>

<file path="backend/run-preprocess/tests/test_header_detection_simple.py">
import pandas as pd

try:
    from ..header_utils import detect_header_row_simple, finalize_headers, build_analysis_hints  # type: ignore
except Exception:  # pragma: no cover
    from header_utils import detect_header_row_simple, finalize_headers, build_analysis_hints  # type: ignore


def test_detect_header_with_title_rows():
    rows = [
        ["Consolidated Balance Sheet", "", ""],
        ["Metric", "2023", "2024"],
        ["Cash & Equivalents", "1990", "3389"],
        ["Short-Term Investments", "19218", "9907"],
    ]
    df = pd.DataFrame(rows)
    idx, conf = detect_header_row_simple(df, lookahead=5)
    assert idx == 1
    assert conf >= 0.3
    raw_headers = df.iloc[idx].tolist()
    final_headers, issues = finalize_headers(raw_headers)
    assert final_headers[0].lower().startswith("metric")
    data = df.iloc[idx + 1 :].reset_index(drop=True).copy()
    data.columns = final_headers
    hints, summary = build_analysis_hints(data, final_headers, idx, conf)
    assert isinstance(hints, dict)
    assert summary and "Header row" in summary or "Header row" in summary.capitalize()


def test_negative_numeric_row_not_header():
    rows = [
        ["100", "200", "300"],
        ["Name", "Value", "Score"],
        ["A", "1", "2"],
        ["B", "3", "4"],
    ]
    df = pd.DataFrame(rows)
    idx, conf = detect_header_row_simple(df, lookahead=4)
    assert idx == 1  # should prefer the textual header row
    assert conf > 0.3
</file>

<file path="backend/run-preprocess/tests/test_integration_polars_balance_sheet.py">
from pathlib import Path

import os

try:
    from ..pipeline_adapter_polars import process_file_to_artifacts  # type: ignore
except Exception:  # pragma: no cover
    from pipeline_adapter_polars import process_file_to_artifacts  # type: ignore


def test_polars_integration_balance_sheet(tmp_path):
    # Ensure lookahead default is present for deterministic behavior
    os.environ.setdefault("PREPROCESS_HEADER_LOOKAHEAD", "12")

    # Locate fixture next to this test
    csv_path = Path(__file__).resolve().parent / "fixtures" / "balance_sheet.csv"
    assert csv_path.exists(), f"fixture missing: {csv_path}"

    result = process_file_to_artifacts(str(csv_path))
    payload = result.payload

    # v2 fields present
    assert payload.get("schema_version") == "2.0"
    header_info = payload.get("header_info") or {}
    hints = payload.get("analysis_hints") or {}
    assert isinstance(header_info, dict) and isinstance(hints, dict)

    # Header detection: expect row 1 (0-based) due to title row at 0
    assert header_info.get("header_row_index") == 1
    conf = float(header_info.get("confidence") or 0.0)
    assert 0.0 <= conf <= 1.0

    final_headers = header_info.get("final_headers") or []
    assert isinstance(final_headers, list) and len(final_headers) >= 2
    assert str(final_headers[0]).lower() in ("item", "metric")

    # Hints sanity
    assert isinstance(hints.get("numeric_columns"), list)
    assert isinstance(hints.get("temporal_columns"), list)
    assert hints.get("first_column_type") in ("dimension", "data")
    # Balance sheet-like: first column is label/dimension, many numeric columns
    assert hints.get("likely_pivoted") in (True, False)  # don't force value, just ensure present
</file>

<file path="backend/shared/ast_validator.py">
"""
AST-based validation for user-provided analysis code.

Enforces:
- Only "def run(df, ctx):" is allowed as an entrypoint.
- Disallow exec/eval, __*__ dunders, attribute access to private attrs.
- Restrict imports to an allowlist.
- Forbid file/network/process/thread/system access.

This is a conservative validator intended for fast/complex executors.
"""
from __future__ import annotations

import ast
from typing import Iterable, Set


FORBIDDEN_NAMES: Set[str] = {
    "exec",
    "eval",
    "compile",
    "open",
    "__import__",
}

FORBIDDEN_MODULE_PREFIXES: Set[str] = {
    "os",
    "sys",
    "subprocess",
    "socket",
    "asyncio",
    "multiprocessing",
    "threading",
    "ctypes",
    "pathlib",
    "importlib",
    "pdb",
    "pickle",
    "dill",
    "requests",
    "urllib",
}

ALLOWED_IMPORTS_FAST: Set[str] = {
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    # stdlib
    "math",
    "statistics",
    "json",
    "io",
    "itertools",
    "functools",
    "collections",
    "re",
    "datetime",
}


class _Validator(ast.NodeVisitor):
    def __init__(self, allowlist: Iterable[str]) -> None:
        super().__init__()
        self.allowlist = set(allowlist)
        self.errors: list[str] = []

    def _err(self, msg: str) -> None:
        self.errors.append(msg)

    # Imports
    def visit_Import(self, node: ast.Import) -> None:  # noqa: N802
        for alias in node.names:
            name = (alias.asname or alias.name).split(".")[0]
            if not any(name == a or name.startswith(a + ".") for a in self.allowlist):
                self._err(f"Import not allowed: {alias.name}")
            if any(name == p or name.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
                self._err(f"Forbidden import: {alias.name}")
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:  # noqa: N802
        mod = node.module or ""
        root = mod.split(".")[0]
        if not any(root == a or root.startswith(a + ".") for a in self.allowlist):
            self._err(f"Import from not allowed: {mod}")
        if any(root == p or root.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
            self._err(f"Forbidden import from: {mod}")
        self.generic_visit(node)

    # Calls
    def visit_Call(self, node: ast.Call) -> None:  # noqa: N802
        # Detect forbidden builtins like eval/exec/open
        if isinstance(node.func, ast.Name):
            if node.func.id in FORBIDDEN_NAMES:
                self._err(f"Forbidden call: {node.func.id}")
        self.generic_visit(node)

    # Attributes
    def visit_Attribute(self, node: ast.Attribute) -> None:  # noqa: N802
        if node.attr.startswith("__") and node.attr.endswith("__"):
            self._err("Use of dunder attributes is not allowed")
        self.generic_visit(node)

    # Name access
    def visit_Name(self, node: ast.Name) -> None:  # noqa: N802
        if node.id.startswith("__") and node.id.endswith("__"):
            self._err("Use of dunder names is not allowed")
        self.generic_visit(node)


def validate(code: str, allowlist: Iterable[str] = ALLOWED_IMPORTS_FAST) -> tuple[bool, list[str]]:
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return False, [f"SyntaxError: {e}"]

    # Must define a top-level function run(df, ctx)
    has_run = False
    for node in tree.body:
        if isinstance(node, ast.FunctionDef) and node.name == "run":
            args = node.args.args
            if len(args) >= 2 and args[0].arg == "df" and args[1].arg == "ctx":
                has_run = True
                break
    if not has_run:
        return False, ["Missing required function: def run(df, ctx):"]

    v = _Validator(allowlist)
    v.visit(tree)
    return len(v.errors) == 0, v.errors
</file>

<file path="frontend/.env.example">
# Frontend environment configuration (example)
# Fill these for local development.

# Backend endpoints (dev). For production, App defaults to /api/* via Hosting rewrites.
VITE_SIGN_URL=http://localhost:8080/sign-upload-url
VITE_CHAT_URL=http://localhost:8081/chat

# Firebase Web App config
VITE_FIREBASE_API_KEY=YOUR_API_KEY
VITE_FIREBASE_AUTH_DOMAIN=your-project.firebaseapp.com
VITE_FIREBASE_PROJECT_ID=your-project-id
VITE_FIREBASE_STORAGE_BUCKET=your-project.appspot.com
VITE_FIREBASE_MESSAGING_SENDER_ID=000000000000
VITE_FIREBASE_APP_ID=1:000000000000:web:abcdefabcdef
</file>

<file path="frontend/.firebaserc">
{
  "projects": {
    "default": "ai-data-analyser"
  }
}
</file>

<file path="frontend/postcss.config.js">
module.exports = {
  plugins: {
    "@tailwindcss/postcss": {},
    autoprefixer: {},
  },
};
</file>

<file path="frontend/src/components/DarkModeToggle.tsx">
import { useEffect, useState } from "react";
import { Moon, Sun } from "lucide-react";
import { Button } from "./ui/button";

export function DarkModeToggle() {
  const [isDark, setIsDark] = useState(false);

  useEffect(() => {
    // Check if dark mode is enabled on mount
    const isDarkMode = document.documentElement.classList.contains("dark");
    setIsDark(isDarkMode);
  }, []);

  const toggleDarkMode = () => {
    const newIsDark = !isDark;
    setIsDark(newIsDark);
    
    if (newIsDark) {
      document.documentElement.classList.add("dark");
    } else {
      document.documentElement.classList.remove("dark");
    }
  };

  return (
    <Button
      variant="ghost"
      size="icon"
      onClick={toggleDarkMode}
      className="rounded-full"
    >
      {isDark ? <Sun className="h-4 w-4" /> : <Moon className="h-4 w-4" />}
    </Button>
  );
}
</file>

<file path="frontend/src/components/renderers/ChartRenderer.tsx">
import React from "react";
import {
  ResponsiveContainer,
  BarChart,
  Bar,
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
} from "recharts";

export interface Series {
  label: string;
  data: number[];
}

export interface ChartData {
  kind: string; // 'bar' | 'line'
  labels: string[];
  series: Series[];
}

export function ChartRenderer({ chartData }: { chartData: ChartData }) {
  if (!chartData || !Array.isArray(chartData.labels)) {
    return <div className="text-sm text-muted-foreground">No chart data.</div>;
  }

  const data = React.useMemo(() => {
    const rows: any[] = [];
    const { labels, series } = chartData;
    const maxLen = Math.max(0, ...series.map((s) => s.data.length));
    const L = Math.max(labels.length, maxLen);
    for (let i = 0; i < L; i++) {
      const row: any = { label: labels[i] ?? String(i) };
      series.forEach((s, idx) => {
        row[`s${idx}`] = typeof s.data[i] === "number" ? s.data[i] : 0;
      });
      rows.push(row);
    }
    return rows;
  }, [chartData]);

  const isBar = (chartData.kind || "bar").toLowerCase() === "bar";

  return (
    <div className="w-full h-72 border rounded-xl p-2 bg-background">
      <ResponsiveContainer width="100%" height="100%">
        {isBar ? (
          <BarChart data={data} margin={{ top: 10, right: 20, left: 0, bottom: 0 }}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis dataKey="label" />
            <YAxis />
            <Tooltip />
            <Legend />
            {chartData.series.map((s, idx) => (
              <Bar key={idx} dataKey={`s${idx}`} name={s.label || `Series ${idx + 1}`} fill={idx === 0 ? "#8884d8" : "#82ca9d"} />
            ))}
          </BarChart>
        ) : (
          <LineChart data={data} margin={{ top: 10, right: 20, left: 0, bottom: 0 }}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis dataKey="label" />
            <YAxis />
            <Tooltip />
            <Legend />
            {chartData.series.map((s, idx) => (
              <Line key={idx} type="monotone" dataKey={`s${idx}`} name={s.label || `Series ${idx + 1}`} stroke={idx === 0 ? "#8884d8" : "#82ca9d"} dot={false} />
            ))}
          </LineChart>
        )}
      </ResponsiveContainer>
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/utils.ts">
import { clsx, type ClassValue } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}
</file>

<file path="frontend/src/styles/tailwind.css">
@import "tailwindcss";
/* Project design tokens and custom layers */
@import "./globals.css";

/* Tell Tailwind v4 where to scan for class usage */
@source "../../index.html";
@source "../**/*.{js,jsx,ts,tsx}";
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  darkMode: 'class',
  content: [
    './index.html',
    './src/**/*.{js,ts,jsx,tsx}',
  ],
  theme: {
    extend: {
      colors: {
        background: 'var(--background)',
        foreground: 'var(--foreground)',
        card: 'var(--card)',
        'card-foreground': 'var(--card-foreground)',
        popover: 'var(--popover)',
        'popover-foreground': 'var(--popover-foreground)',
        primary: 'var(--primary)',
        'primary-foreground': 'var(--primary-foreground)',
        secondary: 'var(--secondary)',
        'secondary-foreground': 'var(--secondary-foreground)',
        muted: 'var(--muted)',
        'muted-foreground': 'var(--muted-foreground)',
        accent: 'var(--accent)',
        'accent-foreground': 'var(--accent-foreground)',
        destructive: 'var(--destructive)',
        'destructive-foreground': 'var(--destructive-foreground)',
        border: 'var(--border)',
        input: 'var(--input)',
        'input-background': 'var(--input-background)',
        'switch-background': 'var(--switch-background)',
        ring: 'var(--ring)',
        sidebar: 'var(--sidebar)',
        'sidebar-foreground': 'var(--sidebar-foreground)',
        'sidebar-primary': 'var(--sidebar-primary)',
        'sidebar-primary-foreground': 'var(--sidebar-primary-foreground)',
        'sidebar-accent': 'var(--sidebar-accent)',
        'sidebar-accent-foreground': 'var(--sidebar-accent-foreground)',
        'sidebar-border': 'var(--sidebar-border)',
        'sidebar-ring': 'var(--sidebar-ring)',
        white: '#ffffff',
      },
      borderRadius: {
        lg: 'var(--radius)',
        md: 'calc(var(--radius) - 2px)',
        sm: 'calc(var(--radius) - 4px)',
      },
    },
  },
  plugins: [],
};
</file>

<file path="backend/docs/api.md">
# Backend API (Draft v1)

This document describes the backend endpoints and SSE event contract. All endpoints are in region `europe-west4`.

## Authentication

- For simplicity in the draft, examples omit auth. In production, attach a user identity (e.g., Firebase Auth ID) and pass `uid` and `sid` (session ID) via headers or cookies.
- Example headers (subject to change):
  - `X-User-Id: <uid>`
  - `X-Session-Id: <sid>`

---

## 1) GET `/api/sign-upload-url`

Issue a signed URL for direct browser PUT to GCS.

Query parameters:
- `filename`: string (required)
- `size`: integer bytes (required, ≤ 20MB)
- `type`: mime type, e.g., `text/csv` or `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet` (required)

Headers:
- `X-User-Id`: uid
- `X-Session-Id`: sid

Response 200 JSON:
```json
{
  "url": "https://storage.googleapis.com/...",
  "datasetId": "<uuid>",
  "storagePath": "users/<uid>/sessions/<sid>/datasets/<datasetId>/raw/input.csv"
}
```

Notes:
- Signed URL method: PUT
- Required headers when uploading: `Content-Type: <type>`
- CORS: configured on bucket `ai-data-analyser-files` for `http://localhost:3000` (prod domain added later)

---

## 2) POST `/api/chat` (SSE)

Start an analysis step; streams progress and results. LLM (Gemini 2.5 Flash) generates
Python that operates on the dataset with Pandas/Numpy. Code is validated and executed
in a sandboxed child process with a hard 60s timeout.

Request JSON:
```json
{
  "uid": "<uid>",
  "sessionId": "<sid>",
  "datasetId": "<datasetId>",
  "question": "Find trends in revenue by region"
}
```

Headers:
- `X-User-Id: <uid>` (optional if provided in body)
- `X-Session-Id: <sid>` (optional if provided in body)

SSE events (examples):
```json
{"type":"received","data":{"sessionId":"<sid>","datasetId":"<datasetId>"}}
{"type":"validating"}
{"type":"generating_code"}
{"type":"running_fast"}
{"type":"summarizing"}
{"type":"persisting"}
{"type":"done","data":{
  "messageId":"<uuid>",
  "chartData": {"kind":"bar","labels":["A","B"],"series":[{"label":"Value","data":[1,2]}]},
  "tableSample": [{"category":"A","value":1},{"category":"B","value":2}],
  "uris":{
    "table":"gs://ai-data-analyser-files/users/.../results/<messageId>/table.json",
    "metrics":"gs://.../metrics.json",
    "chartData":"gs://.../chart_data.json",
    "summary":"gs://.../summary.json"
  }
}}
{"type":"ping"}
```

Chart data schema (backend → frontend/Chart.js):
```json
{
  "kind": "bar" | "line" | "pie",
  "labels": ["x1", "x2", "x3"],
  "series": [
    { "label": "Series A", "data": [1, 2, 3] }
  ],
  "options": { /* optional hints */ }
}
```

Behavior:
- Heartbeat: `{ "type": "ping" }` about every 20–25 seconds.
- Hard timeout: 60s. On expiry emits `{"type":"error","data":{"code":"TIMEOUT_HARD"}}`.
- Soft timeout: currently logs only (no partial-return yet).

Error codes (non-exhaustive):
- `MISSING_PARQUET` – cleaned.parquet not found
- `DOWNLOAD_FAILED` – failed to download dataset artifacts
- `CODEGEN_FAILED` – LLM code generation failure
- `CODE_VALIDATION_FAILED` – AST/allowlist validation failure
- `EXEC_FAILED` – sandboxed execution failed (stderr included)
- `BAD_RESULT` – sandbox output not valid JSON/shape
- `PERSIST_FAILED` – failed to write results to GCS

---

## 3) POST `/api/session/:id/close`

Immediately purge all artifacts for a session and delete Firestore docs.

Response 202 JSON:
```json
{"status":"accepted"}
```

Notes:
- GCS prefix-scope TTL (1 day on `users/`) remains as safety net.

---

## 4) Eventarc → Cloud Run `/eventarc` (Preprocess)

HTTP target for GCS-object-finalize events. Expects CloudEvent-like body with at least:
```json
{
  "data": {
    "bucket": "ai-data-analyser-files",
    "name": "users/<uid>/sessions/<sid>/datasets/<datasetId>/raw/input.csv"
  },
  "id": "...",
  "source": "...",
  "type": "google.cloud.storage.object.v1.finalized",
  "time": "..."
}
```

Response:
- `204 No Content` on success/ignored events.

Processing:
- Download raw → run pipeline → write `cleaned.parquet`, `payload.json`, `cleaning_report.json` → update Firestore dataset doc.
</file>

<file path="backend/docs/payload_schema.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "payload",
  "$comment": "This schema validates the v1 payload (dataset, columns, sample_rows, cleaning_report, mode, version). Additional v2 fields are additive and may be present but are not validated here: schema_version, header_info, analysis_hints, dataset_summary.",
  "type": "object",
  "required": ["dataset", "columns", "mode", "version"],
  "properties": {
    "dataset": {
      "type": "object",
      "required": ["rows", "columns", "column_names", "dtypes"],
      "properties": {
        "rows": { "type": "integer", "minimum": 0 },
        "columns": { "type": "integer", "minimum": 0 },
        "column_names": {
          "type": "array",
          "items": { "type": "string" }
        },
        "dtypes": {
          "type": "object",
          "additionalProperties": { "type": "string" }
        }
      }
    },
    "columns": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "required": ["type", "null_pct", "unique_pct"],
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "integer",
              "float",
              "percentage",
              "currency",
              "date",
              "categorical",
              "text",
              "id"
            ]
          },
          "null_pct": { "type": "number", "minimum": 0, "maximum": 100 },
          "unique_pct": { "type": "number", "minimum": 0, "maximum": 100 },
          "top_values": {
            "type": "array",
            "items": {
              "type": "object",
              "required": ["value", "count"],
              "properties": {
                "value": { },
                "count": { "type": "integer", "minimum": 0 }
              }
            },
            "maxItems": 5
          },
          "datetime_format": { "type": ["string", "null"] },
          "is_potential_dimension": { "type": "boolean" }
        }
      }
    },
    "sample_rows": {
      "type": "array",
      "items": { "type": "object" },
      "maxItems": 50
    },
    "excelInfo": {
      "type": "object",
      "properties": {
        "sheet_used": { "type": "integer", "minimum": 0 },
        "sheet_name": { "type": ["string", "null"] },
        "sheets_total": { "type": ["integer", "null"], "minimum": 1 }
      }
    },
    "cleaning_report": { "type": "object" },
    "mode": { "type": "string", "enum": ["full", "schema_only"] },
    "version": { "type": "string", "enum": ["1"] }
  }
}
</file>

<file path="backend/functions/orchestrator/sandbox_runner.py">
"""
AST-based validation for LLM-generated analysis code.

This module statically validates dynamically generated Python analysis code to
enforce security and sandbox constraints before execution.

Checks performed:
- The presence of a required function signature: `def run(df, ctx):`
- Imports restricted to an allowlist (configurable via SANDBOX_MODE).
- Disallowed function calls (e.g., exec, open, eval).
- Disallowed attribute or name access (dunder methods, globals).
- Optional loop-depth and import-count limits (prevent runaway code).

Environment variable:
    SANDBOX_MODE = "restricted" | "rich"   (default: restricted)
"""

from __future__ import annotations
import ast
import os
from typing import Iterable, Tuple, Dict, List

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

_SANDBOX_MODE = os.getenv("SANDBOX_MODE", "restricted").lower()

# Minimal baseline imports for restricted environments
ALLOWED_IMPORTS_BASE = {"pandas", "numpy", "math", "json"}

# Rich mode adds analysis and visualization support
ALLOWED_IMPORTS_RICH = {
    "matplotlib", "seaborn", "statistics", "io", "itertools", "functools",
    "collections", "re", "datetime", "base64",
}

ALLOWED_IMPORTS = set(ALLOWED_IMPORTS_BASE)
if _SANDBOX_MODE in ("rich", "extended"):
    ALLOWED_IMPORTS.update(ALLOWED_IMPORTS_RICH)

# Dangerous constructs and module prefixes
FORBIDDEN_NAMES = {"exec", "eval", "compile", "open", "__import__", "globals", "locals", "input"}

FORBIDDEN_MODULE_PREFIXES = {
    "os", "sys", "subprocess", "socket", "asyncio", "multiprocessing",
    "threading", "ctypes", "pathlib", "importlib", "pdb", "pickle",
    "dill", "requests", "urllib",
}

# Safety thresholds
MAX_IMPORTS = 12
MAX_LOOP_DEPTH = 4


# ---------------------------------------------------------------------------
# AST Validator
# ---------------------------------------------------------------------------

class _Validator(ast.NodeVisitor):
    """AST walker enforcing import, call, and naming safety rules."""

    def __init__(self, allowlist: Iterable[str]):
        super().__init__()
        self.allowlist = set(allowlist)
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.import_count = 0
        self.loop_depth = 0

    def _err(self, msg: str) -> None:
        self.errors.append(msg)

    def _warn(self, msg: str) -> None:
        self.warnings.append(msg)

    # -------------------------------
    # Import Validation
    # -------------------------------
    def visit_Import(self, node: ast.Import) -> None:
        self.import_count += 1
        for alias in node.names:
            root = (alias.name or "").split(".")[0]
            if root not in self.allowlist:
                self._err(f"Import not allowed: {alias.name}")
            if root in FORBIDDEN_MODULE_PREFIXES:
                self._err(f"Forbidden import: {alias.name}")
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        self.import_count += 1
        mod = node.module or ""
        root = mod.split(".")[0]
        if root not in self.allowlist:
            self._err(f"Import from not allowed: {mod}")
        if any(root == p or root.startswith(p + ".") for p in FORBIDDEN_MODULE_PREFIXES):
            self._err(f"Forbidden import from: {mod}")
        self.generic_visit(node)

    # -------------------------------
    # Function & Call Validation
    # -------------------------------
    def visit_Call(self, node: ast.Call) -> None:
        if isinstance(node.func, ast.Name) and node.func.id in FORBIDDEN_NAMES:
            self._err(f"Forbidden call: {node.func.id}")
        self.generic_visit(node)

    def visit_Attribute(self, node: ast.Attribute) -> None:
        if node.attr.startswith("__") and node.attr.endswith("__"):
            self._err("Use of dunder attributes is not allowed")
        self.generic_visit(node)

    def visit_Name(self, node: ast.Name) -> None:
        if node.id.startswith("__") and node.id.endswith("__"):
            self._err("Use of dunder names is not allowed")
        self.generic_visit(node)

    # -------------------------------
    # Structural Safety Checks
    # -------------------------------
    def visit_For(self, node: ast.For) -> None:
        self.loop_depth += 1
        if self.loop_depth > MAX_LOOP_DEPTH:
            self._warn(f"Deeply nested loop detected (depth {self.loop_depth})")
        self.generic_visit(node)
        self.loop_depth -= 1

    def visit_While(self, node: ast.While) -> None:
        self.loop_depth += 1
        if self.loop_depth > MAX_LOOP_DEPTH:
            self._warn(f"Deeply nested loop detected (depth {self.loop_depth})")
        self.generic_visit(node)
        self.loop_depth -= 1


# ---------------------------------------------------------------------------
# Public Validation API
# ---------------------------------------------------------------------------

def validate_code(
    code: str,
    allowlist: Iterable[str] | None = None
) -> Tuple[bool, List[str], List[str]]:
    """
    Validate Python code against structural and security rules.

    Returns:
        (is_valid, errors, warnings)
    """
    if not code or not isinstance(code, str):
        return False, ["Empty or invalid code string."], []

    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return False, [f"SyntaxError: {e}"], []

    # Ensure required entrypoint exists
    has_run_func = False
    for node in tree.body:
        if isinstance(node, ast.FunctionDef) and node.name == "run":
            args = node.args.args
            if len(args) >= 2 and args[0].arg == "df" and args[1].arg == "ctx":
                has_run_func = True
                break
    if not has_run_func:
        return False, ["Missing required function: def run(df, ctx):"], []

    # Determine which allowlist to apply
    allowlist_to_use = set(allowlist) if allowlist else set(ALLOWED_IMPORTS)
    validator = _Validator(allowlist_to_use)
    validator.visit(tree)

    # Apply import/loop sanity warnings
    if validator.import_count > MAX_IMPORTS:
        validator._warn(f"Too many imports ({validator.import_count} > {MAX_IMPORTS})")

    ok = len(validator.errors) == 0
    return ok, validator.errors, validator.warnings


def structured_validate(code: str) -> Dict[str, any]:
    """
    Return a structured dict for downstream use (e.g., LLM repair loop).
    """
    ok, errors, warnings = validate_code(code)
    return {
        "ok": ok,
        "errors": errors,
        "warnings": warnings,
        "mode": _SANDBOX_MODE,
        "allowed_imports": sorted(list(ALLOWED_IMPORTS)),
    }


# ---------------------------------------------------------------------------
# Stub Execution (compatibility placeholder)
# ---------------------------------------------------------------------------

def run_user_code_stub() -> dict:
    """Simple placeholder result for systems not yet executing code."""
    return {
        "table": [{"category": "A", "value": 1}, {"category": "B", "value": 2}],
        "metrics": {},
        "chartData": {
            "kind": "bar",
            "labels": ["A", "B"],
            "series": [{"label": "Value", "data": [1, 2]}],
        },
        "message": f"Sandbox validation ready (mode={_SANDBOX_MODE}). Execution not yet implemented.",
    }
</file>

<file path="backend/functions/sign_upload_url/requirements.txt">
functions-framework==3.5.0
google-cloud-storage==2.16.0
google-cloud-firestore==2.16.0
firebase-admin==6.5.0
</file>

<file path="backend/run-preprocess/tests/test_polars_helpers.py">
import polars as pl
import math

# Import functions under test
try:
    from ..pipeline_adapter_polars import _drop_fully_blank_rows_pl, _detect_header_row_pl, _numeric_expr_for, NULL_TOKENS
except Exception:  # fallback for direct invocation
    from pipeline_adapter_polars import _drop_fully_blank_rows_pl, _detect_header_row_pl, _numeric_expr_for, NULL_TOKENS  # type: ignore


def test_drop_fully_blank_rows_pl():
    df = pl.DataFrame({
        "a": [None, " ", "value", None],
        "b": [None, "\t", "123", "-"],
    })
    # '-' is a NULL_TOKEN per shared constants
    cleaned = _drop_fully_blank_rows_pl(df)
    # Expect rows with any non-null/non-null-token value to remain
    # Row 0: all null -> drop
    # Row 1: whitespace only -> drop
    # Row 2: has values -> keep
    # Row 3: a=None, b='-' (null token) -> drop
    assert cleaned.height == 1
    assert cleaned.select(pl.first()).to_series()[0] == "value"


def test_detect_header_row_pl():
    # First row looks like header (alpha), second more numeric
    df = pl.DataFrame({
        "c0": ["Product", "A", "B"],
        "c1": ["Revenue", "100", "200"],
        "c2": ["Qty", "1", "3"],
    })
    # The adapter reads CSVs without header; emulate that by providing rows as values
    # Here, row 0 is header-like
    row_like = pl.DataFrame(
        [df.select("c0").to_series(), df.select("c1").to_series(), df.select("c2").to_series()],
    ).transpose(include_header=False)
    hdr_row = _detect_header_row_pl(row_like)
    assert hdr_row == 0


def test_numeric_expr_for_parsing():
    df = pl.DataFrame({
        "x": [
            "1,234",        # separators
            "(12.5)",       # parens negative
            "50%",          # percent
            "$3,000",       # currency
            "1.2k",         # K suffix
            "-",            # null token -> None
            None,
        ]
    })
    expr = _numeric_expr_for("x").alias("y")
    out = df.select(expr)
    got = out["y"].to_list()

    assert got[0] == 1234.0
    assert math.isclose(got[1], -12.5)
    assert math.isclose(got[2], 0.5)
    assert got[3] == 3000.0
    assert math.isclose(got[4], 1200.0)
    assert got[5] is None
    assert got[6] is None
</file>

<file path="frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
firebase-debug.log*
firebase-debug.*.log*

# Firebase cache
.firebase/

# Firebase config

# Uncomment this if you'd like others to create their own Firebase project.
# For a team working on the same Firebase project(s), it is recommended to leave
# it commented so all members can deploy to the same project(s) in .firebaserc.
# .firebaserc

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage

# nyc test coverage
.nyc_output

# Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (http://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variables file
.env
.env.local

# dataconnect generated files
.dataconnect
</file>

<file path="frontend/src/components/ChatHeader.tsx">
import React from "react";
import { DarkModeToggle } from "./DarkModeToggle";

interface ChatHeaderProps {
  sidebarOpen: boolean;
}

export function ChatHeader({ sidebarOpen }: ChatHeaderProps) {
  // Match the sidebar widths: 256px (expanded) or 64px (collapsed)
  const leftOffset = sidebarOpen ? 256 : 64;
  return (
    <header
      className="fixed top-0 right-0 h-14 border-b border-border bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60 z-50 flex items-center justify-between px-4 transition-all duration-300"
      style={{ left: `${leftOffset}px` }}
    >
      <div className="font-semibold tracking-tight">AI Data Analyst</div>
      <DarkModeToggle />
    </header>
  );
}
</file>

<file path="frontend/src/components/ui/button.tsx">
import * as React from "react";
import { Slot } from "@radix-ui/react-slot";
import { cva, type VariantProps } from "class-variance-authority";

import { cn } from "./utils";

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-white hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background text-foreground hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9 rounded-md",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  },
);

function Button({
  className,
  variant,
  size,
  asChild = false,
  ...props
}: React.ComponentProps<"button"> &
  VariantProps<typeof buttonVariants> & {
    asChild?: boolean;
  }) {
  const Comp = asChild ? Slot : "button";

  return (
    <Comp
      data-slot="button"
      className={cn(buttonVariants({ variant, size, className }))}
      {...props}
    />
  );
}

export { Button, buttonVariants };
</file>

<file path="frontend/src/components/ui/textarea.tsx">
import * as React from "react";

import { cn } from "./utils";

const Textarea = React.forwardRef<
  HTMLTextAreaElement,
  React.ComponentProps<"textarea">
>(({ className, ...props }, ref) => {
  return (
    <textarea
      ref={ref}
      data-slot="textarea"
      className={cn(
        "resize-none border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-input-background px-3 py-2 text-base transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        className,
      )}
      {...props}
    />
  );
});
Textarea.displayName = "Textarea";

export { Textarea };
</file>

<file path="frontend/src/index.css">
/*! tailwindcss v4.1.3 | MIT License | https://tailwindcss.com */
@layer properties {
  @supports (((-webkit-hyphens: none)) and (not (margin-trim: inline))) or ((-moz-orient: inline) and (not (color: rgb(from red r g b)))) {
    *, :before, :after, ::backdrop {
      --tw-space-y-reverse: 0;
      --tw-border-style: solid;
      --tw-gradient-position: initial;
      --tw-gradient-from: #0000;
      --tw-gradient-via: #0000;
      --tw-gradient-to: #0000;
      --tw-gradient-stops: initial;
      --tw-gradient-via-stops: initial;
      --tw-gradient-from-position: 0%;
      --tw-gradient-via-position: 50%;
      --tw-gradient-to-position: 100%;
      --tw-font-weight: initial;
      --tw-shadow: 0 0 #0000;
      --tw-shadow-color: initial;
      --tw-shadow-alpha: 100%;
      --tw-inset-shadow: 0 0 #0000;
      --tw-inset-shadow-color: initial;
      --tw-inset-shadow-alpha: 100%;
      --tw-ring-color: initial;
      --tw-ring-shadow: 0 0 #0000;
      --tw-inset-ring-color: initial;
      --tw-inset-ring-shadow: 0 0 #0000;
      --tw-ring-inset: initial;
      --tw-ring-offset-width: 0px;
      --tw-ring-offset-color: #fff;
      --tw-ring-offset-shadow: 0 0 #0000;
      --tw-outline-style: solid;
      --tw-backdrop-blur: initial;
      --tw-backdrop-brightness: initial;
      --tw-backdrop-contrast: initial;
      --tw-backdrop-grayscale: initial;
      --tw-backdrop-hue-rotate: initial;
      --tw-backdrop-invert: initial;
      --tw-backdrop-opacity: initial;
      --tw-backdrop-saturate: initial;
      --tw-backdrop-sepia: initial;
      --tw-duration: initial;
    }
  }
}

@layer theme {
  :root, :host {
    --font-sans: ui-sans-serif, system-ui, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
    --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    --color-white: #fff;
    --spacing: .25rem;
    --container-md: 28rem;
    --container-3xl: 48rem;
    --text-xs: .75rem;
    --text-xs--line-height: calc(1 / .75);
    --text-sm: .875rem;
    --text-sm--line-height: calc(1.25 / .875);
    --text-base: 1rem;
    --text-base--line-height: calc(1.5 / 1);
    --text-lg: 1.125rem;
    --text-xl: 1.25rem;
    --text-2xl: 1.5rem;
    --font-weight-normal: 400;
    --font-weight-medium: 500;
    --radius-3xl: 1.5rem;
    --animate-bounce: bounce 1s infinite;
    --default-transition-duration: .15s;
    --default-transition-timing-function: cubic-bezier(.4, 0, .2, 1);
    --default-font-family: var(--font-sans);
    --default-font-feature-settings: var(--font-sans--font-feature-settings);
    --default-font-variation-settings: var(--font-sans--font-variation-settings);
    --default-mono-font-family: var(--font-mono);
    --default-mono-font-feature-settings: var(--font-mono--font-feature-settings);
    --default-mono-font-variation-settings: var(--font-mono--font-variation-settings);
  }
}

@layer base {
  *, :after, :before, ::backdrop {
    box-sizing: border-box;
    border: 0 solid;
    margin: 0;
    padding: 0;
  }

  ::file-selector-button {
    box-sizing: border-box;
    border: 0 solid;
    margin: 0;
    padding: 0;
  }

  html, :host {
    -webkit-text-size-adjust: 100%;
    tab-size: 4;
    line-height: 1.5;
    font-family: var(--default-font-family, ui-sans-serif, system-ui, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji");
    font-feature-settings: var(--default-font-feature-settings, normal);
    font-variation-settings: var(--default-font-variation-settings, normal);
    -webkit-tap-highlight-color: transparent;
  }

  body {
    line-height: inherit;
  }

  hr {
    height: 0;
    color: inherit;
    border-top-width: 1px;
  }

  abbr:where([title]) {
    -webkit-text-decoration: underline dotted;
    text-decoration: underline dotted;
  }

  h1, h2, h3, h4, h5, h6 {
    font-size: inherit;
    font-weight: inherit;
  }

  a {
    color: inherit;
    -webkit-text-decoration: inherit;
    -webkit-text-decoration: inherit;
    text-decoration: inherit;
  }

  b, strong {
    font-weight: bolder;
  }

  code, kbd, samp, pre {
    font-family: var(--default-mono-font-family, ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace);
    font-feature-settings: var(--default-mono-font-feature-settings, normal);
    font-variation-settings: var(--default-mono-font-variation-settings, normal);
    font-size: 1em;
  }

  small {
    font-size: 80%;
  }

  sub, sup {
    vertical-align: baseline;
    font-size: 75%;
    line-height: 0;
    position: relative;
  }

  sub {
    bottom: -.25em;
  }

  sup {
    top: -.5em;
  }

  table {
    text-indent: 0;
    border-color: inherit;
    border-collapse: collapse;
  }

  :-moz-focusring {
    outline: auto;
  }

  progress {
    vertical-align: baseline;
  }

  summary {
    display: list-item;
  }

  ol, ul, menu {
    list-style: none;
  }

  img, svg, video, canvas, audio, iframe, embed, object {
    vertical-align: middle;
    display: block;
  }

  img, video {
    max-width: 100%;
    height: auto;
  }

  button, input, select, optgroup, textarea {
    font: inherit;
    font-feature-settings: inherit;
    font-variation-settings: inherit;
    letter-spacing: inherit;
    color: inherit;
    opacity: 1;
    background-color: #0000;
    border-radius: 0;
  }

  ::file-selector-button {
    font: inherit;
    font-feature-settings: inherit;
    font-variation-settings: inherit;
    letter-spacing: inherit;
    color: inherit;
    opacity: 1;
    background-color: #0000;
    border-radius: 0;
  }

  :where(select:is([multiple], [size])) optgroup {
    font-weight: bolder;
  }

  :where(select:is([multiple], [size])) optgroup option {
    padding-inline-start: 20px;
  }

  ::file-selector-button {
    margin-inline-end: 4px;
  }

  ::placeholder {
    opacity: 1;
    color: currentColor;
  }

  @supports (color: color-mix(in lab, red, red)) {
    ::placeholder {
      color: color-mix(in oklab, currentColor 50%, transparent);
    }
  }

  textarea {
    resize: vertical;
  }

  ::-webkit-search-decoration {
    -webkit-appearance: none;
  }

  ::-webkit-date-and-time-value {
    min-height: 1lh;
    text-align: inherit;
  }

  ::-webkit-datetime-edit {
    display: inline-flex;
  }

  ::-webkit-datetime-edit-fields-wrapper {
    padding: 0;
  }

  ::-webkit-datetime-edit {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-year-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-month-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-day-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-hour-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-minute-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-second-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-millisecond-field {
    padding-block: 0;
  }

  ::-webkit-datetime-edit-meridiem-field {
    padding-block: 0;
  }

  :-moz-ui-invalid {
    box-shadow: none;
  }

  button, input:where([type="button"], [type="reset"], [type="submit"]) {
    appearance: button;
  }

  ::file-selector-button {
    appearance: button;
  }

  ::-webkit-inner-spin-button {
    height: auto;
  }

  ::-webkit-outer-spin-button {
    height: auto;
  }

  [hidden]:where(:not([hidden="until-found"])) {
    display: none !important;
  }

  * {
    border-color: var(--border);
    outline-color: var(--ring);
  }

  @supports (color: color-mix(in lab, red, red)) {
    * {
      outline-color: color-mix(in oklab, var(--ring) 50%, transparent);
    }
  }

  * {
    border-color: var(--border);
    outline-color: var(--ring);
  }

  @supports (color: color-mix(in lab, red, red)) {
    * {
      outline-color: color-mix(in oklab, var(--ring) 50%, transparent);
    }
  }

  body {
    background-color: var(--background);
    color: var(--foreground);
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) h1 {
    font-size: var(--text-2xl);
    font-weight: var(--font-weight-medium);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) h2 {
    font-size: var(--text-xl);
    font-weight: var(--font-weight-medium);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) h3 {
    font-size: var(--text-lg);
    font-weight: var(--font-weight-medium);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) h4 {
    font-size: var(--text-base);
    font-weight: var(--font-weight-medium);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) p {
    font-size: var(--text-base);
    font-weight: var(--font-weight-normal);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) label, :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) button {
    font-size: var(--text-base);
    font-weight: var(--font-weight-medium);
    line-height: 1.5;
  }

  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) input {
    font-size: var(--text-base);
    font-weight: var(--font-weight-normal);
    line-height: 1.5;
  }
}

@layer utilities {
  .fixed {
    position: fixed;
  }

  .relative {
    position: relative;
  }

  .top-0 {
    top: calc(var(--spacing) * 0);
  }

  .right-0 {
    right: calc(var(--spacing) * 0);
  }

  .bottom-0 {
    bottom: calc(var(--spacing) * 0);
  }

  .left-0 {
    left: calc(var(--spacing) * 0);
  }

  .z-30 {
    z-index: 30;
  }

  .z-40 {
    z-index: 40;
  }

  .z-50 {
    z-index: 50;
  }

  .mx-auto {
    margin-inline: auto;
  }

  .mt-2 {
    margin-top: calc(var(--spacing) * 2);
  }

  .mt-3 {
    margin-top: calc(var(--spacing) * 3);
  }

  .mr-0 {
    margin-right: calc(var(--spacing) * 0);
  }

  .mr-auto {
    margin-right: auto;
  }

  .mb-1 {
    margin-bottom: calc(var(--spacing) * 1);
  }

  .mb-4 {
    margin-bottom: calc(var(--spacing) * 4);
  }

  .ml-0 {
    margin-left: calc(var(--spacing) * 0);
  }

  .ml-2 {
    margin-left: calc(var(--spacing) * 2);
  }

  .ml-auto {
    margin-left: auto;
  }

  .flex {
    display: flex;
  }

  .grid {
    display: grid;
  }

  .inline-flex {
    display: inline-flex;
  }

  .field-sizing-content {
    field-sizing: content;
  }

  .size-4 {
    width: calc(var(--spacing) * 4);
    height: calc(var(--spacing) * 4);
  }

  .size-9 {
    width: calc(var(--spacing) * 9);
    height: calc(var(--spacing) * 9);
  }

  .size-full {
    width: 100%;
    height: 100%;
  }

  .h-2 {
    height: calc(var(--spacing) * 2);
  }

  .h-2\.5 {
    height: calc(var(--spacing) * 2.5);
  }

  .h-4 {
    height: calc(var(--spacing) * 4);
  }

  .h-5 {
    height: calc(var(--spacing) * 5);
  }

  .h-6 {
    height: calc(var(--spacing) * 6);
  }

  .h-8 {
    height: calc(var(--spacing) * 8);
  }

  .h-9 {
    height: calc(var(--spacing) * 9);
  }

  .h-10 {
    height: calc(var(--spacing) * 10);
  }

  .h-14 {
    height: calc(var(--spacing) * 14);
  }

  .h-full {
    height: 100%;
  }

  .max-h-\[200px\] {
    max-height: 200px;
  }

  .min-h-16 {
    min-height: calc(var(--spacing) * 16);
  }

  .min-h-\[24px\] {
    min-height: 24px;
  }

  .w-2 {
    width: calc(var(--spacing) * 2);
  }

  .w-2\.5 {
    width: calc(var(--spacing) * 2.5);
  }

  .w-4 {
    width: calc(var(--spacing) * 4);
  }

  .w-5 {
    width: calc(var(--spacing) * 5);
  }

  .w-6 {
    width: calc(var(--spacing) * 6);
  }

  .w-8 {
    width: calc(var(--spacing) * 8);
  }

  .w-16 {
    width: calc(var(--spacing) * 16);
  }

  .w-64 {
    width: calc(var(--spacing) * 64);
  }

  .w-full {
    width: 100%;
  }

  .max-w-3xl {
    max-width: var(--container-3xl);
  }

  .max-w-md {
    max-width: var(--container-md);
  }

  .min-w-0 {
    min-width: calc(var(--spacing) * 0);
  }

  .flex-1 {
    flex: 1;
  }

  .flex-shrink-0, .shrink-0 {
    flex-shrink: 0;
  }

  .animate-bounce {
    animation: var(--animate-bounce);
  }

  .cursor-pointer {
    cursor: pointer;
  }

  .touch-none {
    touch-action: none;
  }

  .resize-none {
    resize: none;
  }

  .flex-col {
    flex-direction: column;
  }

  .items-center {
    align-items: center;
  }

  .items-end {
    align-items: flex-end;
  }

  .justify-between {
    justify-content: space-between;
  }

  .justify-center {
    justify-content: center;
  }

  .justify-end {
    justify-content: flex-end;
  }

  .justify-start {
    justify-content: flex-start;
  }

  .gap-1 {
    gap: calc(var(--spacing) * 1);
  }

  .gap-1\.5 {
    gap: calc(var(--spacing) * 1.5);
  }

  .gap-2 {
    gap: calc(var(--spacing) * 2);
  }

  .gap-3 {
    gap: calc(var(--spacing) * 3);
  }

  .gap-6 {
    gap: calc(var(--spacing) * 6);
  }

  :where(.space-y-1 > :not(:last-child)) {
    --tw-space-y-reverse: 0;
    margin-block-start: calc(calc(var(--spacing) * 1) * var(--tw-space-y-reverse));
    margin-block-end: calc(calc(var(--spacing) * 1) * calc(1 - var(--tw-space-y-reverse)));
  }

  :where(.space-y-2 > :not(:last-child)) {
    --tw-space-y-reverse: 0;
    margin-block-start: calc(calc(var(--spacing) * 2) * var(--tw-space-y-reverse));
    margin-block-end: calc(calc(var(--spacing) * 2) * calc(1 - var(--tw-space-y-reverse)));
  }

  .truncate {
    text-overflow: ellipsis;
    white-space: nowrap;
    overflow: hidden;
  }

  .rounded-3xl {
    border-radius: var(--radius-3xl);
  }

  .rounded-\[inherit\] {
    border-radius: inherit;
  }

  .rounded-full {
    border-radius: 3.40282e38px;
  }

  .rounded-lg {
    border-radius: var(--radius);
  }

  .rounded-md {
    border-radius: calc(var(--radius)  - 2px);
  }

  .border {
    border-style: var(--tw-border-style);
    border-width: 1px;
  }

  .border-0 {
    border-style: var(--tw-border-style);
    border-width: 0;
  }

  .border-t {
    border-top-style: var(--tw-border-style);
    border-top-width: 1px;
  }

  .border-r {
    border-right-style: var(--tw-border-style);
    border-right-width: 1px;
  }

  .border-b {
    border-bottom-style: var(--tw-border-style);
    border-bottom-width: 1px;
  }

  .border-l {
    border-left-style: var(--tw-border-style);
    border-left-width: 1px;
  }

  .border-border {
    border-color: var(--border);
  }

  .border-input {
    border-color: var(--input);
  }

  .border-sidebar-border {
    border-color: var(--sidebar-border);
  }

  .border-t-transparent {
    border-top-color: #0000;
  }

  .border-l-transparent {
    border-left-color: #0000;
  }

  .bg-background {
    background-color: var(--background);
  }

  .bg-background\/95 {
    background-color: var(--background);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .bg-background\/95 {
      background-color: color-mix(in oklab, var(--background) 95%, transparent);
    }
  }

  .bg-border {
    background-color: var(--border);
  }

  .bg-destructive {
    background-color: var(--destructive);
  }

  .bg-input-background {
    background-color: var(--input-background);
  }

  .bg-muted-foreground {
    background-color: var(--muted-foreground);
  }

  .bg-primary {
    background-color: var(--primary);
  }

  .bg-secondary {
    background-color: var(--secondary);
  }

  .bg-sidebar {
    background-color: var(--sidebar);
  }

  .bg-sidebar-accent {
    background-color: var(--sidebar-accent);
  }

  .bg-sidebar-accent\/30 {
    background-color: var(--sidebar-accent);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .bg-sidebar-accent\/30 {
      background-color: color-mix(in oklab, var(--sidebar-accent) 30%, transparent);
    }
  }

  .bg-sidebar-primary {
    background-color: var(--sidebar-primary);
  }

  .bg-transparent {
    background-color: #0000;
  }

  .bg-gradient-to-t {
    --tw-gradient-position: to top in oklab;
    background-image: linear-gradient(var(--tw-gradient-stops));
  }

  .from-background {
    --tw-gradient-from: var(--background);
    --tw-gradient-stops: var(--tw-gradient-via-stops, var(--tw-gradient-position), var(--tw-gradient-from) var(--tw-gradient-from-position), var(--tw-gradient-to) var(--tw-gradient-to-position));
  }

  .via-background {
    --tw-gradient-via: var(--background);
    --tw-gradient-via-stops: var(--tw-gradient-position), var(--tw-gradient-from) var(--tw-gradient-from-position), var(--tw-gradient-via) var(--tw-gradient-via-position), var(--tw-gradient-to) var(--tw-gradient-to-position);
    --tw-gradient-stops: var(--tw-gradient-via-stops);
  }

  .to-transparent {
    --tw-gradient-to: transparent;
    --tw-gradient-stops: var(--tw-gradient-via-stops, var(--tw-gradient-position), var(--tw-gradient-from) var(--tw-gradient-from-position), var(--tw-gradient-to) var(--tw-gradient-to-position));
  }

  .p-0 {
    padding: calc(var(--spacing) * 0);
  }

  .p-2 {
    padding: calc(var(--spacing) * 2);
  }

  .p-3 {
    padding: calc(var(--spacing) * 3);
  }

  .p-8 {
    padding: calc(var(--spacing) * 8);
  }

  .p-px {
    padding: 1px;
  }

  .px-2 {
    padding-inline: calc(var(--spacing) * 2);
  }

  .px-3 {
    padding-inline: calc(var(--spacing) * 3);
  }

  .px-4 {
    padding-inline: calc(var(--spacing) * 4);
  }

  .px-6 {
    padding-inline: calc(var(--spacing) * 6);
  }

  .py-2 {
    padding-block: calc(var(--spacing) * 2);
  }

  .py-2\.5 {
    padding-block: calc(var(--spacing) * 2.5);
  }

  .py-3 {
    padding-block: calc(var(--spacing) * 3);
  }

  .py-8 {
    padding-block: calc(var(--spacing) * 8);
  }

  .pt-1 {
    padding-top: calc(var(--spacing) * 1);
  }

  .pt-8 {
    padding-top: calc(var(--spacing) * 8);
  }

  .pt-14 {
    padding-top: calc(var(--spacing) * 14);
  }

  .pb-4 {
    padding-bottom: calc(var(--spacing) * 4);
  }

  .pb-32 {
    padding-bottom: calc(var(--spacing) * 32);
  }

  .text-center {
    text-align: center;
  }

  .text-left {
    text-align: left;
  }

  .text-base {
    font-size: var(--text-base);
    line-height: var(--tw-leading, var(--text-base--line-height));
  }

  .text-sm {
    font-size: var(--text-sm);
    line-height: var(--tw-leading, var(--text-sm--line-height));
  }

  .text-xs {
    font-size: var(--text-xs);
    line-height: var(--tw-leading, var(--text-xs--line-height));
  }

  .font-medium {
    --tw-font-weight: var(--font-weight-medium);
    font-weight: var(--font-weight-medium);
  }

  .break-words {
    overflow-wrap: break-word;
  }

  .whitespace-nowrap {
    white-space: nowrap;
  }

  .whitespace-pre-wrap {
    white-space: pre-wrap;
  }

  .text-foreground {
    color: var(--foreground);
  }

  .text-muted-foreground {
    color: var(--muted-foreground);
  }

  .text-primary {
    color: var(--primary);
  }

  .text-primary-foreground {
    color: var(--primary-foreground);
  }

  .text-secondary-foreground {
    color: var(--secondary-foreground);
  }

  .text-sidebar-accent-foreground {
    color: var(--sidebar-accent-foreground);
  }

  .text-sidebar-foreground {
    color: var(--sidebar-foreground);
  }

  .text-sidebar-primary-foreground {
    color: var(--sidebar-primary-foreground);
  }

  .text-white {
    color: var(--color-white);
  }

  .underline-offset-4 {
    text-underline-offset: 4px;
  }

  .shadow-lg {
    --tw-shadow: 0 10px 15px -3px var(--tw-shadow-color, #0000001a), 0 4px 6px -4px var(--tw-shadow-color, #0000001a);
    box-shadow: var(--tw-inset-shadow), var(--tw-inset-ring-shadow), var(--tw-ring-offset-shadow), var(--tw-ring-shadow), var(--tw-shadow);
  }

  .outline {
    outline-style: var(--tw-outline-style);
    outline-width: 1px;
  }

  .backdrop-blur {
    --tw-backdrop-blur: blur(8px);
    -webkit-backdrop-filter: var(--tw-backdrop-blur, ) var(--tw-backdrop-brightness, ) var(--tw-backdrop-contrast, ) var(--tw-backdrop-grayscale, ) var(--tw-backdrop-hue-rotate, ) var(--tw-backdrop-invert, ) var(--tw-backdrop-opacity, ) var(--tw-backdrop-saturate, ) var(--tw-backdrop-sepia, );
    backdrop-filter: var(--tw-backdrop-blur, ) var(--tw-backdrop-brightness, ) var(--tw-backdrop-contrast, ) var(--tw-backdrop-grayscale, ) var(--tw-backdrop-hue-rotate, ) var(--tw-backdrop-invert, ) var(--tw-backdrop-opacity, ) var(--tw-backdrop-saturate, ) var(--tw-backdrop-sepia, );
  }

  .backdrop-filter {
    -webkit-backdrop-filter: var(--tw-backdrop-blur, ) var(--tw-backdrop-brightness, ) var(--tw-backdrop-contrast, ) var(--tw-backdrop-grayscale, ) var(--tw-backdrop-hue-rotate, ) var(--tw-backdrop-invert, ) var(--tw-backdrop-opacity, ) var(--tw-backdrop-saturate, ) var(--tw-backdrop-sepia, );
    backdrop-filter: var(--tw-backdrop-blur, ) var(--tw-backdrop-brightness, ) var(--tw-backdrop-contrast, ) var(--tw-backdrop-grayscale, ) var(--tw-backdrop-hue-rotate, ) var(--tw-backdrop-invert, ) var(--tw-backdrop-opacity, ) var(--tw-backdrop-saturate, ) var(--tw-backdrop-sepia, );
  }

  .transition-\[color\,box-shadow\] {
    transition-property: color, box-shadow;
    transition-timing-function: var(--tw-ease, var(--default-transition-timing-function));
    transition-duration: var(--tw-duration, var(--default-transition-duration));
  }

  .transition-all {
    transition-property: all;
    transition-timing-function: var(--tw-ease, var(--default-transition-timing-function));
    transition-duration: var(--tw-duration, var(--default-transition-duration));
  }

  .transition-colors {
    transition-property: color, background-color, border-color, outline-color, text-decoration-color, fill, stroke, --tw-gradient-from, --tw-gradient-via, --tw-gradient-to;
    transition-timing-function: var(--tw-ease, var(--default-transition-timing-function));
    transition-duration: var(--tw-duration, var(--default-transition-duration));
  }

  .duration-300 {
    --tw-duration: .3s;
    transition-duration: .3s;
  }

  .outline-none {
    --tw-outline-style: none;
    outline-style: none;
  }

  .select-none {
    -webkit-user-select: none;
    user-select: none;
  }

  .placeholder\:text-muted-foreground::placeholder {
    color: var(--muted-foreground);
  }

  @media (hover: hover) {
    .hover\:bg-accent:hover {
      background-color: var(--accent);
    }
  }

  @media (hover: hover) {
    .hover\:bg-destructive\/90:hover {
      background-color: var(--destructive);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .hover\:bg-destructive\/90:hover {
        background-color: color-mix(in oklab, var(--destructive) 90%, transparent);
      }
    }
  }

  @media (hover: hover) {
    .hover\:bg-primary\/90:hover {
      background-color: var(--primary);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .hover\:bg-primary\/90:hover {
        background-color: color-mix(in oklab, var(--primary) 90%, transparent);
      }
    }
  }

  @media (hover: hover) {
    .hover\:bg-secondary\/80:hover {
      background-color: var(--secondary);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .hover\:bg-secondary\/80:hover {
        background-color: color-mix(in oklab, var(--secondary) 80%, transparent);
      }
    }
  }

  @media (hover: hover) {
    .hover\:bg-sidebar-accent\/50:hover {
      background-color: var(--sidebar-accent);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .hover\:bg-sidebar-accent\/50:hover {
        background-color: color-mix(in oklab, var(--sidebar-accent) 50%, transparent);
      }
    }
  }

  @media (hover: hover) {
    .hover\:text-accent-foreground:hover {
      color: var(--accent-foreground);
    }
  }

  @media (hover: hover) {
    .hover\:underline:hover {
      text-decoration-line: underline;
    }
  }

  .focus-visible\:border-ring:focus-visible {
    border-color: var(--ring);
  }

  .focus-visible\:ring-0:focus-visible {
    --tw-ring-shadow: var(--tw-ring-inset, ) 0 0 0 calc(0px + var(--tw-ring-offset-width)) var(--tw-ring-color, currentcolor);
    box-shadow: var(--tw-inset-shadow), var(--tw-inset-ring-shadow), var(--tw-ring-offset-shadow), var(--tw-ring-shadow), var(--tw-shadow);
  }

  .focus-visible\:ring-\[3px\]:focus-visible {
    --tw-ring-shadow: var(--tw-ring-inset, ) 0 0 0 calc(3px + var(--tw-ring-offset-width)) var(--tw-ring-color, currentcolor);
    box-shadow: var(--tw-inset-shadow), var(--tw-inset-ring-shadow), var(--tw-ring-offset-shadow), var(--tw-ring-shadow), var(--tw-shadow);
  }

  .focus-visible\:ring-destructive\/20:focus-visible {
    --tw-ring-color: var(--destructive);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .focus-visible\:ring-destructive\/20:focus-visible {
      --tw-ring-color: color-mix(in oklab, var(--destructive) 20%, transparent);
    }
  }

  .focus-visible\:ring-ring\/50:focus-visible {
    --tw-ring-color: var(--ring);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .focus-visible\:ring-ring\/50:focus-visible {
      --tw-ring-color: color-mix(in oklab, var(--ring) 50%, transparent);
    }
  }

  .focus-visible\:ring-offset-0:focus-visible {
    --tw-ring-offset-width: 0px;
    --tw-ring-offset-shadow: var(--tw-ring-inset, ) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);
  }

  .focus-visible\:outline-1:focus-visible {
    outline-style: var(--tw-outline-style);
    outline-width: 1px;
  }

  .disabled\:pointer-events-none:disabled {
    pointer-events: none;
  }

  .disabled\:cursor-not-allowed:disabled {
    cursor: not-allowed;
  }

  .disabled\:opacity-50:disabled {
    opacity: .5;
  }

  .has-\[\>svg\]\:px-2\.5:has( > svg) {
    padding-inline: calc(var(--spacing) * 2.5);
  }

  .has-\[\>svg\]\:px-3:has( > svg) {
    padding-inline: calc(var(--spacing) * 3);
  }

  .has-\[\>svg\]\:px-4:has( > svg) {
    padding-inline: calc(var(--spacing) * 4);
  }

  .aria-invalid\:border-destructive[aria-invalid="true"] {
    border-color: var(--destructive);
  }

  .aria-invalid\:ring-destructive\/20[aria-invalid="true"] {
    --tw-ring-color: var(--destructive);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .aria-invalid\:ring-destructive\/20[aria-invalid="true"] {
      --tw-ring-color: color-mix(in oklab, var(--destructive) 20%, transparent);
    }
  }

  @supports ((-webkit-backdrop-filter: var(--tw)) or (backdrop-filter: var(--tw))) {
    .supports-\[backdrop-filter\]\:bg-background\/60 {
      background-color: var(--background);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .supports-\[backdrop-filter\]\:bg-background\/60 {
        background-color: color-mix(in oklab, var(--background) 60%, transparent);
      }
    }
  }

  @media (width >= 48rem) {
    .md\:text-sm {
      font-size: var(--text-sm);
      line-height: var(--tw-leading, var(--text-sm--line-height));
    }
  }

  .dark\:border-input:is(.dark *) {
    border-color: var(--input);
  }

  .dark\:bg-destructive\/60:is(.dark *) {
    background-color: var(--destructive);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .dark\:bg-destructive\/60:is(.dark *) {
      background-color: color-mix(in oklab, var(--destructive) 60%, transparent);
    }
  }

  .dark\:bg-input\/30:is(.dark *) {
    background-color: var(--input);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .dark\:bg-input\/30:is(.dark *) {
      background-color: color-mix(in oklab, var(--input) 30%, transparent);
    }
  }

  @media (hover: hover) {
    .dark\:hover\:bg-accent\/50:is(.dark *):hover {
      background-color: var(--accent);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .dark\:hover\:bg-accent\/50:is(.dark *):hover {
        background-color: color-mix(in oklab, var(--accent) 50%, transparent);
      }
    }
  }

  @media (hover: hover) {
    .dark\:hover\:bg-input\/50:is(.dark *):hover {
      background-color: var(--input);
    }

    @supports (color: color-mix(in lab, red, red)) {
      .dark\:hover\:bg-input\/50:is(.dark *):hover {
        background-color: color-mix(in oklab, var(--input) 50%, transparent);
      }
    }
  }

  .dark\:focus-visible\:ring-destructive\/40:is(.dark *):focus-visible {
    --tw-ring-color: var(--destructive);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .dark\:focus-visible\:ring-destructive\/40:is(.dark *):focus-visible {
      --tw-ring-color: color-mix(in oklab, var(--destructive) 40%, transparent);
    }
  }

  .dark\:aria-invalid\:ring-destructive\/40:is(.dark *)[aria-invalid="true"] {
    --tw-ring-color: var(--destructive);
  }

  @supports (color: color-mix(in lab, red, red)) {
    .dark\:aria-invalid\:ring-destructive\/40:is(.dark *)[aria-invalid="true"] {
      --tw-ring-color: color-mix(in oklab, var(--destructive) 40%, transparent);
    }
  }

  .\[\&_svg\]\:pointer-events-none svg {
    pointer-events: none;
  }

  .\[\&_svg\]\:shrink-0 svg {
    flex-shrink: 0;
  }

  .\[\&_svg\:not\(\[class\*\=\'size-\'\]\)\]\:size-4 svg:not([class*="size-"]) {
    width: calc(var(--spacing) * 4);
    height: calc(var(--spacing) * 4);
  }
}

:root {
  --font-size: 16px;
  --background: #fff;
  --foreground: oklch(.145 0 0);
  --card: #fff;
  --card-foreground: oklch(.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(.145 0 0);
  --primary: #030213;
  --primary-foreground: oklch(1 0 0);
  --secondary: oklch(.95 .0058 264.53);
  --secondary-foreground: #030213;
  --muted: #ececf0;
  --muted-foreground: #717182;
  --accent: #e9ebef;
  --accent-foreground: #030213;
  --destructive: #d4183d;
  --destructive-foreground: #fff;
  --border: #0000001a;
  --input: transparent;
  --input-background: #f3f3f5;
  --switch-background: #cbced4;
  --font-weight-medium: 500;
  --font-weight-normal: 400;
  --ring: oklch(.708 0 0);
  --chart-1: oklch(.646 .222 41.116);
  --chart-2: oklch(.6 .118 184.704);
  --chart-3: oklch(.398 .07 227.392);
  --chart-4: oklch(.828 .189 84.429);
  --chart-5: oklch(.769 .188 70.08);
  --radius: .625rem;
  --sidebar: oklch(.985 0 0);
  --sidebar-foreground: oklch(.145 0 0);
  --sidebar-primary: #030213;
  --sidebar-primary-foreground: oklch(.985 0 0);
  --sidebar-accent: oklch(.97 0 0);
  --sidebar-accent-foreground: oklch(.205 0 0);
  --sidebar-border: oklch(.922 0 0);
  --sidebar-ring: oklch(.708 0 0);
}

.dark {
  --background: oklch(.145 0 0);
  --foreground: oklch(.985 0 0);
  --card: oklch(.145 0 0);
  --card-foreground: oklch(.985 0 0);
  --popover: oklch(.145 0 0);
  --popover-foreground: oklch(.985 0 0);
  --primary: oklch(.985 0 0);
  --primary-foreground: oklch(.205 0 0);
  --secondary: oklch(.269 0 0);
  --secondary-foreground: oklch(.985 0 0);
  --muted: oklch(.269 0 0);
  --muted-foreground: oklch(.708 0 0);
  --accent: oklch(.269 0 0);
  --accent-foreground: oklch(.985 0 0);
  --destructive: oklch(.396 .141 25.723);
  --destructive-foreground: oklch(.637 .237 25.331);
  --border: oklch(.269 0 0);
  --input: oklch(.269 0 0);
  --ring: oklch(.439 0 0);
  --font-weight-medium: 500;
  --font-weight-normal: 400;
  --chart-1: oklch(.488 .243 264.376);
  --chart-2: oklch(.696 .17 162.48);
  --chart-3: oklch(.769 .188 70.08);
  --chart-4: oklch(.627 .265 303.9);
  --chart-5: oklch(.645 .246 16.439);
  --sidebar: oklch(.205 0 0);
  --sidebar-foreground: oklch(.985 0 0);
  --sidebar-primary: oklch(.488 .243 264.376);
  --sidebar-primary-foreground: oklch(.985 0 0);
  --sidebar-accent: oklch(.269 0 0);
  --sidebar-accent-foreground: oklch(.985 0 0);
  --sidebar-border: oklch(.269 0 0);
  --sidebar-ring: oklch(.439 0 0);
}

html {
  font-size: var(--font-size);
}

@property --tw-space-y-reverse {
  syntax: "*";
  inherits: false;
  initial-value: 0;
}

@property --tw-border-style {
  syntax: "*";
  inherits: false;
  initial-value: solid;
}

@property --tw-gradient-position {
  syntax: "*";
  inherits: false
}

@property --tw-gradient-from {
  syntax: "<color>";
  inherits: false;
  initial-value: #0000;
}

@property --tw-gradient-via {
  syntax: "<color>";
  inherits: false;
  initial-value: #0000;
}

@property --tw-gradient-to {
  syntax: "<color>";
  inherits: false;
  initial-value: #0000;
}

@property --tw-gradient-stops {
  syntax: "*";
  inherits: false
}

@property --tw-gradient-via-stops {
  syntax: "*";
  inherits: false
}

@property --tw-gradient-from-position {
  syntax: "<length-percentage>";
  inherits: false;
  initial-value: 0%;
}

@property --tw-gradient-via-position {
  syntax: "<length-percentage>";
  inherits: false;
  initial-value: 50%;
}

@property --tw-gradient-to-position {
  syntax: "<length-percentage>";
  inherits: false;
  initial-value: 100%;
}

@property --tw-font-weight {
  syntax: "*";
  inherits: false
}

@property --tw-shadow {
  syntax: "*";
  inherits: false;
  initial-value: 0 0 #0000;
}

@property --tw-shadow-color {
  syntax: "*";
  inherits: false
}

@property --tw-shadow-alpha {
  syntax: "<percentage>";
  inherits: false;
  initial-value: 100%;
}

@property --tw-inset-shadow {
  syntax: "*";
  inherits: false;
  initial-value: 0 0 #0000;
}

@property --tw-inset-shadow-color {
  syntax: "*";
  inherits: false
}

@property --tw-inset-shadow-alpha {
  syntax: "<percentage>";
  inherits: false;
  initial-value: 100%;
}

@property --tw-ring-color {
  syntax: "*";
  inherits: false
}

@property --tw-ring-shadow {
  syntax: "*";
  inherits: false;
  initial-value: 0 0 #0000;
}

@property --tw-inset-ring-color {
  syntax: "*";
  inherits: false
}

@property --tw-inset-ring-shadow {
  syntax: "*";
  inherits: false;
  initial-value: 0 0 #0000;
}

@property --tw-ring-inset {
  syntax: "*";
  inherits: false
}

@property --tw-ring-offset-width {
  syntax: "<length>";
  inherits: false;
  initial-value: 0;
}

@property --tw-ring-offset-color {
  syntax: "*";
  inherits: false;
  initial-value: #fff;
}

@property --tw-ring-offset-shadow {
  syntax: "*";
  inherits: false;
  initial-value: 0 0 #0000;
}

@property --tw-outline-style {
  syntax: "*";
  inherits: false;
  initial-value: solid;
}

@property --tw-backdrop-blur {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-brightness {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-contrast {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-grayscale {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-hue-rotate {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-invert {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-opacity {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-saturate {
  syntax: "*";
  inherits: false
}

@property --tw-backdrop-sepia {
  syntax: "*";
  inherits: false
}

@property --tw-duration {
  syntax: "*";
  inherits: false
}

@keyframes bounce {
  0%, 100% {
    animation-timing-function: cubic-bezier(.8, 0, 1, 1);
    transform: translateY(-25%);
  }

  50% {
    animation-timing-function: cubic-bezier(0, 0, .2, 1);
    transform: none;
  }
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite';
  import react from '@vitejs/plugin-react-swc';
  import path from 'path';

  export default defineConfig({
    plugins: [react()],
    resolve: {
      extensions: ['.js', '.jsx', '.ts', '.tsx', '.json'],
      alias: {
        'vaul@1.1.2': 'vaul',
        'sonner@2.0.3': 'sonner',
        'recharts@2.15.2': 'recharts',
        'react-resizable-panels@2.1.7': 'react-resizable-panels',
        'react-hook-form@7.55.0': 'react-hook-form',
        'react-day-picker@8.10.1': 'react-day-picker',
        'next-themes@0.4.6': 'next-themes',
        'lucide-react@0.487.0': 'lucide-react',
        'input-otp@1.4.2': 'input-otp',
        'embla-carousel-react@8.6.0': 'embla-carousel-react',
        'cmdk@1.1.1': 'cmdk',
        'class-variance-authority@0.7.1': 'class-variance-authority',
        '@radix-ui/react-tooltip@1.1.8': '@radix-ui/react-tooltip',
        '@radix-ui/react-toggle@1.1.2': '@radix-ui/react-toggle',
        '@radix-ui/react-toggle-group@1.1.2': '@radix-ui/react-toggle-group',
        '@radix-ui/react-tabs@1.1.3': '@radix-ui/react-tabs',
        '@radix-ui/react-switch@1.1.3': '@radix-ui/react-switch',
        '@radix-ui/react-slot@1.1.2': '@radix-ui/react-slot',
        '@radix-ui/react-slider@1.2.3': '@radix-ui/react-slider',
        '@radix-ui/react-separator@1.1.2': '@radix-ui/react-separator',
        '@radix-ui/react-select@2.1.6': '@radix-ui/react-select',
        '@radix-ui/react-scroll-area@1.2.3': '@radix-ui/react-scroll-area',
        '@radix-ui/react-radio-group@1.2.3': '@radix-ui/react-radio-group',
        '@radix-ui/react-progress@1.1.2': '@radix-ui/react-progress',
        '@radix-ui/react-popover@1.1.6': '@radix-ui/react-popover',
        '@radix-ui/react-navigation-menu@1.2.5': '@radix-ui/react-navigation-menu',
        '@radix-ui/react-menubar@1.1.6': '@radix-ui/react-menubar',
        '@radix-ui/react-label@2.1.2': '@radix-ui/react-label',
        '@radix-ui/react-hover-card@1.1.6': '@radix-ui/react-hover-card',
        '@radix-ui/react-dropdown-menu@2.1.6': '@radix-ui/react-dropdown-menu',
        '@radix-ui/react-dialog@1.1.6': '@radix-ui/react-dialog',
        '@radix-ui/react-context-menu@2.2.6': '@radix-ui/react-context-menu',
        '@radix-ui/react-collapsible@1.1.3': '@radix-ui/react-collapsible',
        '@radix-ui/react-checkbox@1.1.4': '@radix-ui/react-checkbox',
        '@radix-ui/react-avatar@1.1.3': '@radix-ui/react-avatar',
        '@radix-ui/react-aspect-ratio@1.1.2': '@radix-ui/react-aspect-ratio',
        '@radix-ui/react-alert-dialog@1.1.6': '@radix-ui/react-alert-dialog',
        '@radix-ui/react-accordion@1.2.3': '@radix-ui/react-accordion',
        '@': path.resolve(__dirname, './src'),
      },
    },
    build: {
      target: 'esnext',
      outDir: 'build',
    },
    server: {
      port: 5173,
      open: true,
    },
  });
</file>

<file path="backend/deploy-preprocess.ps1">
# =========
# Settings (Preprocess Stage Only)
# =========
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

gcloud config set project $PROJECT_ID | Out-Null

# Script-relative paths
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$ROOT_DIR   = Split-Path -Parent $SCRIPT_DIR
$SRC_RUN     = Join-Path $SCRIPT_DIR "run-preprocess"

# =====================
# Enable required APIs
# =====================
gcloud services enable `
  run.googleapis.com `
  eventarc.googleapis.com `
  cloudbuild.googleapis.com `
  artifactregistry.googleapis.com `
  firestore.googleapis.com `
  storage.googleapis.com `
  pubsub.googleapis.com

# ===========================
# Project number and SA vars
# ===========================
$PROJECT_NUMBER = gcloud projects describe $PROJECT_ID --format="value(projectNumber)"
$SERVICE_ACCOUNT = "$PROJECT_NUMBER-compute@developer.gserviceaccount.com"

# ======================================================
# Deploy Cloud Run: preprocess-svc (from run-preprocess/)
# ======================================================
gcloud run deploy preprocess-svc `
  --region=$REGION `
  --source="$SRC_RUN" `
  --service-account="$SERVICE_ACCOUNT" `
  --set-build-env-vars="GOOGLE_PYTHON_VERSION=3.12" `
  --cpu=2 `
  --memory=2Gi `
  --concurrency=10 `
  --set-env-vars="FILES_BUCKET=$BUCKET,GCP_PROJECT=$PROJECT_ID,TTL_DAYS=1,PREPROCESS_ENGINE=polars" `
  --no-allow-unauthenticated

# ==================================================
# IAM: baseline access for preprocess + artifacts
# ==================================================
# Firestore (read/write)
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/datastore.user"

# Storage (read raw + write artifacts) - scoped to the bucket
gcloud storage buckets add-iam-policy-binding gs://$BUCKET `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/storage.objectAdmin"

# =============================
# GCS lifecycle: users/ → 1 day
# =============================
$lifecycleFile = Join-Path $env:TEMP "gcs_lifecycle_users.json"
@'
{
  "rule": [
    {
      "action": {"type": "Delete"},
      "condition": {"age": 1, "matchesPrefix": ["users/"]}
    }
  ]
}
'@ | Out-File -FilePath $lifecycleFile -Encoding ascii -NoNewline
try {
  gsutil lifecycle set $lifecycleFile gs://$BUCKET
  Write-Host "Applied GCS lifecycle rule: delete users/ after 1 day."
} catch {
  Write-Host "Warning: failed to set lifecycle:" $_.Exception.Message
} finally {
  if (Test-Path $lifecycleFile) { Remove-Item $lifecycleFile -Force }
}

# ==================================================
# Eventarc: permissions + trigger for GCS Object Finalize
# ==================================================
# Allow SA to receive events and invoke the service
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/eventarc.eventReceiver"

# Grant Pub/Sub Publisher to the Cloud Storage service account so GCS can
# publish CloudEvents to Pub/Sub in this project (required by Eventarc).
$GCS_SA = "service-$PROJECT_NUMBER@gs-project-accounts.iam.gserviceaccount.com"
gcloud projects add-iam-policy-binding $PROJECT_ID `
  --member="serviceAccount:$GCS_SA" `
  --role="roles/pubsub.publisher"

# Verify service exists before binding/creating trigger
$RUN_URL_CHECK = gcloud run services describe preprocess-svc --region=$REGION --format="value(status.url)"
if ($RUN_URL_CHECK) {
  gcloud run services add-iam-policy-binding preprocess-svc `
    --region=$REGION `
    --member="serviceAccount:$SERVICE_ACCOUNT" `
    --role="roles/run.invoker"

  # Ensure trigger (GCS -> Cloud Run) exists; update if present
  $TRIGGER_NAME = "preprocess-trigger"
  $TRIGGER_EXISTS = gcloud eventarc triggers describe $TRIGGER_NAME --location=$REGION --format="value(name)" 2>$null
  if ($TRIGGER_EXISTS) {
    gcloud eventarc triggers update $TRIGGER_NAME `
      --location=$REGION `
      --event-filters="type=google.cloud.storage.object.v1.finalized" `
      --event-filters="bucket=$BUCKET" `
      --destination-run-service="preprocess-svc" `
      --destination-run-path="/eventarc" `
      --destination-run-region=$REGION `
      --service-account="$SERVICE_ACCOUNT"
  } else {
    gcloud eventarc triggers create $TRIGGER_NAME `
      --location=$REGION `
      --event-filters="type=google.cloud.storage.object.v1.finalized" `
      --event-filters="bucket=$BUCKET" `
      --destination-run-service="preprocess-svc" `
      --destination-run-path="/eventarc" `
      --destination-run-region=$REGION `
      --service-account="$SERVICE_ACCOUNT"
  }

  # Verify trigger
  gcloud eventarc triggers describe $TRIGGER_NAME --location=$REGION
} else {
  Write-Host "Cloud Run service preprocess-svc not found; skipping Eventarc IAM/trigger setup."
}

# ====================
# Output service URL
# ====================
$RUN_URL  = gcloud run services describe preprocess-svc --region=$REGION --format="value(status.url)"
$RUN_URL  = ($RUN_URL  | Out-String).Trim()
Write-Host "preprocess-svc: $RUN_URL"
</file>

<file path="backend/functions/orchestrator/requirements.txt">
functions-framework==3.5.0
google-cloud-firestore==2.16.0
google-cloud-storage==2.16.0
google-cloud-secret-manager==2.20.2
flask==3.0.3
pandas==2.2.2
numpy==2.0.2
pyarrow==16.1.0
google-generativeai==0.7.2
firebase-admin==6.5.0
</file>

<file path="backend/run-preprocess/main.py">
import os
import json
import logging
import base64
import io
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta, timezone
from typing import Optional, Tuple

from fastapi import FastAPI, Request, Response

# Optional: import google clients (not used until wired)
try:
    from google.cloud import storage  # type: ignore
    from google.cloud import firestore  # type: ignore
    import google.cloud.logging as cloud_logging  # type: ignore
except Exception:  # pragma: no cover - allow local runs without GCP libs installed
    storage = None
    firestore = None
    cloud_logging = None  # type: ignore

app = FastAPI(title="Preprocess Service", version="0.1.0")

FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PROJECT_ID = os.getenv("GCP_PROJECT", os.getenv("GOOGLE_CLOUD_PROJECT", "ai-data-analyser"))
TTL_DAYS = int(os.getenv("TTL_DAYS", "1"))

def _setup_cloud_logging() -> None:
    """Configure Google Cloud Logging handler if available.

    In Cloud Run, this attaches a handler to the root logger, enabling structured
    logs in Cloud Logging. Fallback to std logging if the client is unavailable.
    """
    if cloud_logging is None:
        return
    try:
        client = cloud_logging.Client(project=PROJECT_ID)
        client.setup_logging()  # attaches handler to root logger
        logging.getLogger().setLevel(logging.INFO)
        logging.info("cloud-logging: configured")
    except Exception as e:
        logging.warning("cloud-logging: setup failed: %s", e)


# Configure logging at startup time
@app.on_event("startup")
async def _on_startup() -> None:  # pragma: no cover
    _setup_cloud_logging()

"""Lazy import loader for pipeline_adapter.

We avoid importing heavy data libs (pandas/pyarrow) at module import time so
the Cloud Run container can start and bind to PORT quickly. The actual import
occurs on first use inside the request handler.
"""
_process_file_to_artifacts = None  # type: ignore
_process_bytes_to_artifacts = None  # type: ignore

def _get_engine() -> str:
    eng = os.getenv("PREPROCESS_ENGINE", "polars").strip().lower()
    return "polars" if eng == "polars" else "pandas"

def get_process_file_to_artifacts():
    """Import and cache process_file_to_artifacts lazily.

    Tries package-relative import first, then absolute import for script mode.
    Raises the underlying exception if imports fail, and logs for visibility.
    """
    global _process_file_to_artifacts
    if _process_file_to_artifacts is not None:
        return _process_file_to_artifacts
    engine = _get_engine()
    # Prefer polars adapter when requested, else default to pandas adapter
    try:
        if engine == "polars":
            try:
                from .pipeline_adapter_polars import process_file_to_artifacts as _fn  # type: ignore
            except Exception:
                from pipeline_adapter_polars import process_file_to_artifacts as _fn  # type: ignore
            logging.info("preprocess engine: polars")
        else:
            raise ImportError("force_pandas")
    except Exception:
        # Fallback to pandas adapter
        try:
            from .pipeline_adapter import process_file_to_artifacts as _fn  # type: ignore
        except Exception:
            from pipeline_adapter import process_file_to_artifacts as _fn  # type: ignore
        logging.info("preprocess engine: pandas (fallback)")
    _process_file_to_artifacts = _fn
    return _fn


def get_process_bytes_to_artifacts():
    """Import and cache process_bytes_to_artifacts lazily."""
    global _process_bytes_to_artifacts
    if _process_bytes_to_artifacts is not None:
        return _process_bytes_to_artifacts
    engine = _get_engine()
    try:
        if engine == "polars":
            try:
                from .pipeline_adapter_polars import process_bytes_to_artifacts as _fn  # type: ignore
            except Exception:
                from pipeline_adapter_polars import process_bytes_to_artifacts as _fn  # type: ignore
            logging.info("preprocess engine: polars")
        else:
            raise ImportError("force_pandas")
    except Exception:
        try:
            from .pipeline_adapter import process_bytes_to_artifacts as _fn  # type: ignore
        except Exception:
            from pipeline_adapter import process_bytes_to_artifacts as _fn  # type: ignore
        logging.info("preprocess engine: pandas (fallback)")
    _process_bytes_to_artifacts = _fn
    return _fn


@app.get("/healthz")
def healthz() -> dict:
    return {"status": "ok", "bucket": FILES_BUCKET, "project": PROJECT_ID}


def _parse_object_path(name: str) -> Optional[Tuple[str, str, str]]:
    """Parse gs object path to (uid, sid, datasetId).
    Expected structure:
    users/{uid}/sessions/{sid}/datasets/{datasetId}/raw/input.csv|xlsx
    """
    try:
        parts = name.split("/")
        i = parts.index("users")
        uid = parts[i + 1]
        assert parts[i + 2] == "sessions"
        sid = parts[i + 3]
        assert parts[i + 4] == "datasets"
        dataset_id = parts[i + 5]
        assert parts[i + 6] == "raw"
        return uid, sid, dataset_id
    except Exception:
        return None


@app.post("/eventarc")
async def handle_eventarc(request: Request) -> Response:
    """Handle Eventarc GCS finalize events.

    Accepts CloudEvent-like body with `data.bucket` and `data.name`.
    Ignores events that are not under `/raw/` or are not CSV/XLSX.
    """
    # Read raw body and attempt to support multiple delivery shapes:
    # 1) CloudEvents structured: { "data": { "bucket": ..., "name": ... }, ... }
    # 2) CloudEvents binary: body is the data object itself
    # 3) Pub/Sub push: { "message": { "data": base64(json) }, ... }
    # 4) GCS notification compatibility: top-level { "bucket": ..., "name": ... }
    envelope: dict = {}
    try:
        body_bytes = await request.body()
        body_text = body_bytes.decode("utf-8") if body_bytes else "{}"
        envelope = json.loads(body_text or "{}") if body_text else {}
    except Exception as e:
        logging.warning("Invalid JSON for Eventarc request: %s", e)
        envelope = {}

    data = None
    t0 = time.monotonic()

    # Structured CloudEvent
    if isinstance(envelope, dict) and "data" in envelope and isinstance(envelope["data"], dict):
        data = envelope["data"]

    # Pub/Sub push format
    if data is None and isinstance(envelope, dict) and "message" in envelope:
        try:
            msg = envelope.get("message", {})
            b64 = msg.get("data")
            if isinstance(b64, str):
                decoded = base64.b64decode(b64 + "==")
                inner = json.loads(decoded.decode("utf-8"))
                data = inner.get("data") if isinstance(inner, dict) and "data" in inner else inner
        except Exception as e:
            logging.warning("Failed to decode Pub/Sub envelope: %s", e)

    # GCS notification compatibility or binary CE body
    if data is None and isinstance(envelope, dict):
        if "bucket" in envelope or "name" in envelope or "objectId" in envelope:
            data = envelope

    if not isinstance(data, dict):
        logging.warning("Event parsing failed; envelope keys=%s headers.ce-type=%s", list(envelope.keys()) if isinstance(envelope, dict) else type(envelope), request.headers.get("ce-type"))
        return Response(status_code=204)

    bucket = data.get("bucket")
    name = data.get("name") or data.get("objectId") or data.get("object")

    if not bucket or not name:
        logging.warning("Missing bucket/name after normalization: %s", data)
        return Response(status_code=204)

    # Only react to raw uploads (csv/xlsx)
    is_raw = "/raw/" in name
    is_csv = name.lower().endswith(".csv")
    is_xlsx = name.lower().endswith(".xlsx") or name.lower().endswith(".xls")
    if not (is_raw and (is_csv or is_xlsx)):
        return Response(status_code=204)

    ids = _parse_object_path(name)
    if not ids:
        logging.warning("Object path does not match expected pattern: %s", name)
        return Response(status_code=204)

    uid, sid, dataset_id = ids
    logging.info("Preprocess trigger: bucket=%s name=%s uid=%s sid=%s dataset=%s", bucket, name, uid, sid, dataset_id)
    # Initialize clients lazily
    if storage is None or firestore is None:
        logging.error("GCP clients not available in runtime environment")
        return Response(status_code=500)

    storage_client = storage.Client(project=PROJECT_ID)
    firestore_client = firestore.Client(project=PROJECT_ID)

    try:
        # 1) Download raw into memory
        raw_blob = storage_client.bucket(bucket).blob(name)
        raw_bytes = raw_blob.download_as_bytes()
        kind = "excel" if is_xlsx else "csv"
        t_download = time.monotonic()

        # 2) Optional manual header-row override from GCS metadata (preferred) or env fallback
        header_row_override = None
        try:
            prefix = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}"
            override_blob = storage_client.bucket(bucket).blob(f"{prefix}/metadata/preprocess_overrides.json")
            if override_blob.exists(storage_client):
                override_obj = json.loads(override_blob.download_as_text())
                val = override_obj.get("header_row_index")
                if isinstance(val, int):
                    header_row_override = val
        except Exception as e:
            logging.warning("override_read_failed: %s", e)

        if header_row_override is None:
            try:
                env_val = os.getenv("PREPROCESS_HEADER_ROW_OVERRIDE")
                if env_val is not None and str(env_val).strip() != "":
                    header_row_override = int(str(env_val).strip())
            except Exception:
                pass

        # 3) Run pipeline adapter (bytes variant; lazy import on first use)
        result = get_process_bytes_to_artifacts()( 
            raw_bytes,
            kind,
            sample_rows_for_llm=50,
            metric_rename_heuristic=False,
            header_row_override=header_row_override,
        )
        t_process = time.monotonic()

        # 3) Write artifacts to GCS under dataset prefix (in parallel)
        prefix = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}"
        cleaned_path = f"{prefix}/cleaned/cleaned.parquet"
        payload_path = f"{prefix}/metadata/payload.json"
        report_path = f"{prefix}/reports/cleaning_report.json"

        # Build in-memory parquet
        parquet_buf = io.BytesIO()
        result.cleaned_df.to_parquet(parquet_buf, engine="pyarrow", index=False)
        parquet_size = parquet_buf.getbuffer().nbytes
        parquet_buf.seek(0)
        t_build = time.monotonic()

        payload_data = json.dumps(result.payload, ensure_ascii=False).encode("utf-8")
        report_data = json.dumps(result.cleaning_report, ensure_ascii=False).encode("utf-8")

        bkt = storage_client.bucket(bucket)
        cleaned_blob = bkt.blob(cleaned_path)
        payload_blob = bkt.blob(payload_path)
        report_blob = bkt.blob(report_path)

        # Upload three artifacts in parallel
        with ThreadPoolExecutor(max_workers=3) as ex:
            futs = []
            futs.append(ex.submit(cleaned_blob.upload_from_file, parquet_buf, size=parquet_size, content_type="application/octet-stream"))
            futs.append(ex.submit(payload_blob.upload_from_string, payload_data, content_type="application/json; charset=utf-8"))
            futs.append(ex.submit(report_blob.upload_from_string, report_data, content_type="application/json; charset=utf-8"))
            for f in futs:
                f.result()
        t_uploads = time.monotonic()

        cleaned_uri = f"gs://{bucket}/{cleaned_path}"
        payload_uri = f"gs://{bucket}/{payload_path}"
        report_uri = f"gs://{bucket}/{report_path}"

        # 4) Update Firestore dataset doc
        ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
        doc_ref = firestore_client.document(
            "users", uid, "sessions", sid, "datasets", dataset_id
        )
        doc_ref.set(
            {
                "rawUri": f"gs://{bucket}/{name}",
                "cleanedUri": cleaned_uri,
                "payloadUri": payload_uri,
                "reportUri": report_uri,
                "rows": result.rows,
                "columns": result.columns,
                "status": "ready",
                "updatedAt": datetime.now(timezone.utc),
                "ttlAt": ttl_at,
            },
            merge=True,
        )
        t_firestore = time.monotonic()

        logging.info(json.dumps({
            "event": "preprocess_complete",
            "uid": uid,
            "sid": sid,
            "datasetId": dataset_id,
            "rows": result.rows,
            "columns": result.columns,
            "cleanedUri": cleaned_uri,
            "payloadUri": payload_uri,
            "reportUri": report_uri,
        }))

        # header detection telemetry (v2 fields are additive; guard if missing)
        try:
            header_info = (result.payload or {}).get("header_info", {})
            conf = float(header_info.get("confidence") or 0.0)
            method = header_info.get("method") or "unknown"
            hrow = int(header_info.get("header_row_index") or -1)
            is_transposed = bool(header_info.get("is_transposed") or False)
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            low_thr = float(os.getenv("HEADER_LOW_CONFIDENCE_THRESHOLD", "0.4"))
            low_conf = conf < low_thr
            logging.info(json.dumps({
                "event": "header_detection",
                "uid": uid,
                "sid": sid,
                "datasetId": dataset_id,
                "engine": _get_engine(),
                "header_row_index": hrow,
                "confidence": round(conf, 3),
                "method": method,
                "is_transposed": is_transposed,
                "lookahead": lookahead,
                "low_confidence": low_conf,
            }))
        except Exception as e:
            logging.warning("header_detection_log_failed: %s", e)
        # timings
        timings = {
            "event": "preprocess_timings",
            "uid": uid,
            "sid": sid,
            "datasetId": dataset_id,
            "download_s": round(t_download - t0, 3),
            "process_s": round(t_process - t_download, 3),
            "build_parquet_s": round(t_build - t_process, 3),
            "uploads_s": round(t_uploads - t_build, 3),
            "firestore_s": round(t_firestore - t_uploads, 3),
            "total_s": round(t_firestore - t0, 3),
        }
        logging.info(json.dumps(timings))
        return Response(status_code=204)

    except Exception as e:  # noqa: BLE001 (broad for last-resort error path)
        logging.exception("Preprocess failed: %s", e)
        try:
            ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
            firestore_client.document(
                "users", uid, "sessions", sid, "datasets", dataset_id
            ).set(
                {
                    "status": "error",
                    "errorMessage": str(e)[:2000],
                    "updatedAt": datetime.now(timezone.utc),
                    "ttlAt": ttl_at,
                },
                merge=True,
            )
        except Exception:
            logging.warning("Failed to write error status to Firestore")
        return Response(status_code=500)

if __name__ == "__main__":  # pragma: no cover
    # This block is for local development or when run directly.
    # Cloud Run will provide the PORT environment variable.
    import uvicorn

    port = int(os.getenv("PORT", "8080"))
    uvicorn.run(app, host="0.0.0.0", port=port)
</file>

<file path="backend/run-preprocess/pipeline_adapter_polars.py">
"""
Polars-backed adapter for the preprocess service.

CSV paths use Polars for faster load and normalization, then convert to pandas
for payload construction that reuses pandas-based helpers (type inference).
Excel remains handled by pandas for compatibility.
"""
from __future__ import annotations

import io
import re
import math
from typing import Any, Dict, List, Optional

import polars as pl  # type: ignore
import pandas as pd
import os

# Reuse shared types and helpers with robust import (package/script modes)
try:
    from .pipeline_adapter import (  # type: ignore
        ProcessResult,
        NULL_TOKENS,
        KMB_PATTERN,
        CURRENCY_PATTERN,
        PARENS_PATTERN,
        PERCENT_PATTERN,
        infer_column_type,
    )
except Exception:  # pragma: no cover
    from pipeline_adapter import (  # type: ignore
        ProcessResult,
        NULL_TOKENS,
        KMB_PATTERN,
        CURRENCY_PATTERN,
        PARENS_PATTERN,
        PERCENT_PATTERN,
        infer_column_type,
    )


def _pl_read_csv(source: Any) -> pl.DataFrame:
    """Read CSV via Polars with no header (robust settings)."""
    return pl.read_csv(
        source,
        has_header=False,
        infer_schema_length=2048,
        ignore_errors=True,
        try_parse_dates=False,
    )


def _drop_fully_blank_rows_pl(df: pl.DataFrame) -> pl.DataFrame:
    cols = df.columns
    blank_exprs = []
    for c in cols:
        s = pl.col(c).cast(pl.Utf8).str.strip_chars().str.to_lowercase()
        blank_exprs.append(pl.col(c).is_null() | s.is_in(list(NULL_TOKENS)))
    all_blank = pl.all_horizontal(blank_exprs)
    return df.filter(~all_blank)


def _detect_header_row_pl(df: pl.DataFrame, lookahead: int = 5) -> int:
    max_r = min(df.height, lookahead)
    best_row = 0
    best_score = float("-inf")
    for r in range(max_r):
        row = df.row(r)
        vals = [str(v).strip() for v in row]
        non_empty_ratio = sum(1 for v in vals if v != "") / max(1, len(vals))
        alpha_ratio = sum(1 for v in vals if re.search(r"[A-Za-z]", v or "")) / max(1, len(vals))
        score = alpha_ratio - (0.5 if non_empty_ratio < 0.7 else 0)
        if score > best_score:
            best_score = score
            best_row = r
    return best_row


def _build_headers(vals: list[str]) -> list[str]:
    headers: list[str] = []
    for v in vals:
        s = re.sub(r"\s+", " ", str(v).strip())
        headers.append(s if s else "Column")
    seen: Dict[str, int] = {}
    dedup: list[str] = []
    for h in headers:
        n = h
        if n in seen:
            seen[n] += 1
            n = f"{n}_{seen[n]}"
        else:
            seen[n] = 1
        dedup.append(n)
    return dedup


def _numeric_expr_for(col: str) -> pl.Expr:
    s = pl.col(col).cast(pl.Utf8)
    s = s.str.replace("\u2212", "-")
    s = s.str.replace("\u00a0", " ")
    s = s.str.replace(r"[\u2000-\u200B]", " ", literal=False)
    s = s.str.strip_chars()

    lower = s.str.to_lowercase()
    s = pl.when(lower.is_in(list(NULL_TOKENS))).then(None).otherwise(s)

    # Detect negatives via parentheses, then drop parentheses for casting
    mask_paren = s.str.contains(PARENS_PATTERN.pattern, literal=False)
    s = s.str.replace_all(r"[()]", "", literal=False)
    mask_trail = s.str.ends_with("-")
    s = s.str.replace(r"-$", "", literal=False)

    # K/M/B
    suffix = s.str.extract(KMB_PATTERN.pattern, group_index=1)
    mult = (
        pl.when(suffix.str.to_lowercase() == "k").then(1e3)
        .when(suffix.str.to_lowercase() == "m").then(1e6)
        .when(suffix.str.to_lowercase() == "b").then(1e9)
        .otherwise(1.0)
    )
    s = s.str.replace(KMB_PATTERN.pattern, "", literal=False)

    # Currency and separators
    s = s.str.replace(CURRENCY_PATTERN.pattern, "", literal=False)
    s = s.str.replace(",", "")
    s = s.str.replace(" ", "")

    # Percent
    percent_mask = s.str.contains(PERCENT_PATTERN.pattern, literal=False)
    s = s.str.replace(PERCENT_PATTERN.pattern, "", literal=False)

    nums = s.cast(pl.Float64, strict=False) * mult
    nums = pl.when(mask_paren | mask_trail).then(-nums).otherwise(nums)
    nums = pl.when(percent_mask).then(nums / 100.0).otherwise(nums)
    return nums


def _apply_numeric_normalization(df: pl.DataFrame) -> tuple[pl.DataFrame, list[str]]:
    numeric_cols: list[str] = []
    out = df
    for c in df.columns:
        expr = _numeric_expr_for(c)
        ratio = out.select(expr.is_not_null().mean()).item()
        if ratio is not None and float(ratio) >= 0.6:
            out = out.with_columns(expr.alias(c))
            numeric_cols.append(c)
    return out, numeric_cols


def process_file_to_artifacts(
    local_path: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    # Decide file kind by extension
    ext = local_path.lower().rsplit(".", 1)[-1]
    if ext in ("xlsx", "xls"):
        # Excel via pandas
        raw_df = pd.read_excel(local_path, sheet_name=0, header=None, dtype=object)
        file_kind = "excel"
    else:
        # CSV via Polars → native cleaning → convert to pandas for payload
        df_pl = _pl_read_csv(local_path)
        rows_before = df_pl.height
        work = _drop_fully_blank_rows_pl(df_pl)
        # Lazy import header helpers (package/script modes)
        try:
            from .header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore
        except Exception:  # pragma: no cover
            from header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore

        if header_row_override is not None:
            hdr_row = int(header_row_override)
            header_confidence = 1.0
            method = "override"
        else:
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            pdf = work.head(lookahead).to_pandas()
            hdr_row, header_confidence = detect_header_row_simple(pdf, lookahead=lookahead)
            method = "auto_detected"

        raw_headers = [str(x) for x in work.row(hdr_row)] if work.height > hdr_row else [None] * work.width
        headers, header_issues = finalize_headers(raw_headers)
        body = work.slice(hdr_row + 1)
        # Rename first N columns to headers
        rename_map = {old: headers[i] for i, old in enumerate(body.columns[: len(headers)])}
        body = body.rename(rename_map)
        # Remove repeated header rows
        eq_exprs = [(pl.col(h).cast(pl.Utf8).str.strip_chars() == headers[i]) for i, h in enumerate(headers)]
        is_hdr_row = pl.all_horizontal(eq_exprs)
        body = body.filter(~is_hdr_row)
        # Numeric normalization
        body, numeric_cols = _apply_numeric_normalization(body)
        # Convert to pandas for payload/type inference
        df = body.to_pandas()
        # Build payload pieces
        columns_meta: Dict[str, Any] = {}
        for col in df.columns:
            col_type, dt_fmt = infer_column_type(df[col], str(col))
            col_series = df[col]
            nn = col_series.dropna()
            null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
            unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
            vc = nn.value_counts().head(5)
            top_values = []
            for v, c in vc.items():
                val: Any = v
                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                    continue
                if isinstance(v, (float, int)):
                    val = float(v)
                if isinstance(v, str) and len(v) > 300:
                    val = v[:300]
                top_values.append({"value": val, "count": int(c)})
            entry: Dict[str, Any] = {
                "type": col_type,
                "null_pct": round(float(null_pct), 2),
                "unique_pct": round(float(unique_pct), 2),
                "top_values": top_values,
            }
            if col_type == "date":
                entry["datetime_format"] = dt_fmt
            entry["is_potential_dimension"] = False
            columns_meta[str(col)] = entry

        # Heuristic for potential dimension
        if len(df.columns) > 0:
            first = str(df.columns[0])
            first_type = columns_meta[first]["type"]
            numeric_others = [c for c in list(df.columns)[1:] if columns_meta[str(c)]["type"] in ("integer", "float", "percentage", "currency")]
            if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
                columns_meta[first]["is_potential_dimension"] = True

        dataset_meta = {
            "rows": int(df.shape[0]),
            "columns": int(df.shape[1]),
            "column_names": [str(x) for x in df.columns.tolist()],
            "dtypes": {str(c): str(df[c].dtype) for c in df.columns},
        }

        n = min(sample_rows_for_llm, len(df))
        sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)
        sample_rows: list[dict] = []
        for _, r in sample.iterrows():
            obj: Dict[str, Any] = {}
            for k, v in r.to_dict().items():
                if isinstance(v, str) and len(v) > 300:
                    obj[str(k)] = v[:300]
                elif isinstance(v, (float, int)):
                    obj[str(k)] = float(v)
                else:
                    obj[str(k)] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
            sample_rows.append(obj)

        # Build v2 analysis hints and dataset summary
        hints, dataset_summary = build_analysis_hints(df, headers, hdr_row, float(header_confidence))
        # Minimal header_info with transposed signal
        is_transposed = False
        non_empty_final = [h for h in headers if str(h).strip() != ""]
        if non_empty_final:
            is_transposed = all(is_numeric_string(h) for h in non_empty_final)

        header_info = {
            "method": method,
            "header_row_index": int(hdr_row),
            "confidence": float(header_confidence),
            "original_headers": [str(x) if x is not None else None for x in raw_headers],
            "final_headers": headers,
            "is_transposed": bool(is_transposed),
        }
        if header_issues:
            header_info["issues"] = header_issues

        payload: Dict[str, Any] = {
            "dataset": dataset_meta,
            "columns": columns_meta,
            "sample_rows": sample_rows,
            "cleaning_report": {
                "header_row": int(hdr_row),
                "renamed_columns": {},
                "numeric_columns": numeric_cols,
                "rows_before": int(rows_before),
                "rows_after": int(df.shape[0]),
                "file_kind": "csv",
            },
            "mode": "full",
            "version": "1",
            # v2 additive fields
            "schema_version": "2.0",
            "header_info": header_info,
            "analysis_hints": hints,
            "dataset_summary": dataset_summary,
        }

        return ProcessResult(
            cleaned_df=df,
            payload=payload,
            cleaning_report=payload["cleaning_report"],
            rows=int(df.shape[0]),
            columns=int(df.shape[1]),
        )


def process_bytes_to_artifacts(
    data: bytes,
    kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    if kind not in ("csv", "excel"):
        raise ValueError("kind must be 'csv' or 'excel'")

    if kind == "excel":
        # Excel via pandas
        raw_df = pd.read_excel(io.BytesIO(data), sheet_name=0, header=None, dtype=object)
        # Defer to shared pandas path by converting directly
        # Import locally to avoid circulars
        try:
            from .pipeline_adapter import process_df_to_artifacts as _p  # type: ignore
        except Exception:
            from pipeline_adapter import process_df_to_artifacts as _p  # type: ignore
        return _p(raw_df, "excel", sample_rows_for_llm=sample_rows_for_llm, metric_rename_heuristic=metric_rename_heuristic, header_row_override=header_row_override)
    else:
        # CSV via Polars (same as file path case)
        tmp_path = io.BytesIO(data)
        df_pl = _pl_read_csv(tmp_path)
        rows_before = df_pl.height
        work = _drop_fully_blank_rows_pl(df_pl)
        # Lazy import header helpers
        try:
            from .header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore
        except Exception:  # pragma: no cover
            from header_utils import (
                detect_header_row_simple,
                finalize_headers,
                build_analysis_hints,
                is_numeric_string,
            )  # type: ignore

        if header_row_override is not None:
            hdr_row = int(header_row_override)
            header_confidence = 1.0
            method = "override"
        else:
            lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
            pdf = work.head(lookahead).to_pandas()
            hdr_row, header_confidence = detect_header_row_simple(pdf, lookahead=lookahead)
            method = "auto_detected"

        raw_headers = [str(x) for x in work.row(hdr_row)] if work.height > hdr_row else [None] * work.width
        headers, header_issues = finalize_headers(raw_headers)
        body = work.slice(hdr_row + 1)
        rename_map = {old: headers[i] for i, old in enumerate(body.columns[: len(headers)])}
        body = body.rename(rename_map)
        eq_exprs = [(pl.col(h).cast(pl.Utf8).str.strip_chars() == headers[i]) for i, h in enumerate(headers)]
        is_hdr_row = pl.all_horizontal(eq_exprs)
        body = body.filter(~is_hdr_row)
        body, numeric_cols = _apply_numeric_normalization(body)
        df = body.to_pandas()

        columns_meta: Dict[str, Any] = {}
        for col in df.columns:
            col_type, dt_fmt = infer_column_type(df[col], str(col))
            col_series = df[col]
            nn = col_series.dropna()
            null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
            unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
            vc = nn.value_counts().head(5)
            top_values = []
            for v, c in vc.items():
                val: Any = v
                if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                    continue
                if isinstance(v, (float, int)):
                    val = float(v)
                if isinstance(v, str) and len(v) > 300:
                    val = v[:300]
                top_values.append({"value": val, "count": int(c)})
            entry: Dict[str, Any] = {
                "type": col_type,
                "null_pct": round(float(null_pct), 2),
                "unique_pct": round(float(unique_pct), 2),
                "top_values": top_values,
            }
            if col_type == "date":
                entry["datetime_format"] = dt_fmt
            entry["is_potential_dimension"] = False
            columns_meta[str(col)] = entry

        if len(df.columns) > 0:
            first = str(df.columns[0])
            first_type = columns_meta[first]["type"]
            numeric_others = [c for c in list(df.columns)[1:] if columns_meta[str(c)]["type"] in ("integer", "float", "percentage", "currency")]
            if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
                columns_meta[first]["is_potential_dimension"] = True

        dataset_meta = {
            "rows": int(df.shape[0]),
            "columns": int(df.shape[1]),
            "column_names": [str(x) for x in df.columns.tolist()],
            "dtypes": {str(c): str(df[c].dtype) for c in df.columns},
        }
        n = min(sample_rows_for_llm, len(df))
        sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)
        sample_rows: list[dict] = []
        for _, r in sample.iterrows():
            obj: Dict[str, Any] = {}
            for k, v in r.to_dict().items():
                if isinstance(v, str) and len(v) > 300:
                    obj[str(k)] = v[:300]
                elif isinstance(v, (float, int)):
                    obj[str(k)] = float(v)
                else:
                    obj[str(k)] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
            sample_rows.append(obj)

        # Build v2 analysis hints and dataset summary
        hints, dataset_summary = build_analysis_hints(df, headers, hdr_row, float(header_confidence))
        # Minimal header_info with transposed signal
        is_transposed = False
        non_empty_final = [h for h in headers if str(h).strip() != ""]
        if non_empty_final:
            is_transposed = all(is_numeric_string(h) for h in non_empty_final)

        header_info = {
            "method": method,
            "header_row_index": int(hdr_row),
            "confidence": float(header_confidence),
            "original_headers": [str(x) if x is not None else None for x in raw_headers],
            "final_headers": headers,
            "is_transposed": bool(is_transposed),
        }
        if header_issues:
            header_info["issues"] = header_issues

        payload: Dict[str, Any] = {
            "dataset": dataset_meta,
            "columns": columns_meta,
            "sample_rows": sample_rows,
            "cleaning_report": {
                "header_row": int(hdr_row),
                "renamed_columns": {},
                "numeric_columns": numeric_cols,
                "rows_before": int(rows_before),
                "rows_after": int(df.shape[0]),
                "file_kind": "csv",
            },
            "mode": "full",
            "version": "1",
            # v2 additive fields
            "schema_version": "2.0",
            "header_info": header_info,
            "analysis_hints": hints,
            "dataset_summary": dataset_summary,
        }

        return ProcessResult(
            cleaned_df=df,
            payload=payload,
            cleaning_report=payload["cleaning_report"],
            rows=int(df.shape[0]),
            columns=int(df.shape[1]),
        )
</file>

<file path="backend/run-preprocess/pipeline_adapter.py">
"""
Pipeline adapter for the preprocess service.

This module loads a raw CSV/XLSX file from a local path, performs light but
robust cleaning, infers lightweight column metadata for LLM prompting
(payload), and returns both the cleaned DataFrame and JSON-serializable
artifacts.

Design goals:
- Preserve original column names (no destructive renames).
- Excel: first sheet only.
- Deterministic sampling for payload (seed=42, up to 50 rows).
- Numeric normalization for %, K/M/B, currency symbols, and common separators.
- Add `datetime_format` where inferred with high confidence.
- Add `is_potential_dimension` instead of renaming first column to Metric.

Note: This is a focused adapter tailored for serverless preprocessing. It is
not a full port of the original repository in data_processing_profiling.md, but
it follows the same principles and payload structure agreed for this project.
"""
from __future__ import annotations

import io
import math
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import os
try:
    from .header_utils import (
        detect_header_row_simple,
        finalize_headers,
        build_analysis_hints,
        is_numeric_string,
    )  # type: ignore
except Exception:  # pragma: no cover
    from header_utils import (
        detect_header_row_simple,
        finalize_headers,
        build_analysis_hints,
        is_numeric_string,
    )  # type: ignore

# -----------------------------
# Constants & helpers
# -----------------------------

NULL_TOKENS = {
    "", "na", "n/a", "none", "null", "nil", "nan", "-", "--", "n\u00a0a",
}
KMB_PATTERN = re.compile(r"\s*([kKmMbB])\s*$")
CURRENCY_PATTERN = re.compile(r"[$€£¥]|USD|EUR|GBP|JPY|TRY|TL|₺")
TRAILING_MINUS_PATTERN = re.compile(r"-$")
PARENS_PATTERN = re.compile(r"^\(.*\)$")
PERCENT_PATTERN = re.compile(r"%")
DATE_REGEXES = [
    (re.compile(r"^\d{4}-\d{2}-\d{2}$"), "%Y-%m-%d"),
    (re.compile(r"^\d{2}/\d{2}/\d{4}$"), "%m/%d/%Y"),
    (re.compile(r"^\d{2}-\d{2}-\d{4}$"), "%m-%d-%Y"),
    (re.compile(r"^\d{2}\.\d{2}\.\d{4}$"), "%m.%d.%Y"),
    (re.compile(r"^\d{1,2}\s+[A-Za-z]{3}\s+\d{4}$"), "%d %b %Y"),
    (re.compile(r"^[A-Za-z]{3}\s+\d{1,2},\s+\d{4}$"), "%b %d, %Y"),
]


@dataclass
class ProcessResult:
    cleaned_df: pd.DataFrame
    payload: Dict[str, Any]
    cleaning_report: Dict[str, Any]
    rows: int
    columns: int


# -----------------------------
# Loading & cleaning
# -----------------------------

def _load_raw(local_path: str) -> Tuple[pd.DataFrame, str]:
    ext = local_path.lower().rsplit(".", 1)[-1]
    if ext in ("xlsx", "xls"):
        df = pd.read_excel(local_path, sheet_name=0, header=None, dtype=object)
        kind = "excel"
    else:
        # CSV fast path: C engine + Arrow-backed dtypes for speed/memory
        df = pd.read_csv(
            local_path,
            header=None,
            engine="c",
            dtype_backend="pyarrow",
        )
        kind = "csv"
    return df, kind


def _drop_fully_blank_rows(df: pd.DataFrame) -> pd.DataFrame:
    is_na = df.isna()
    lowered = df.astype(str).apply(lambda s: s.str.strip().str.lower(), axis=0)
    is_token_blank = lowered.isin(NULL_TOKENS)
    is_blank = is_na | is_token_blank
    keep_mask = ~is_blank.all(axis=1)
    return df.loc[keep_mask]


def _detect_header_row(df: pd.DataFrame, lookahead: int = 5) -> int:
    # Simple heuristic: choose the earliest row where more than half of columns
    # are non-numeric-ish strings, followed by a row with more numeric candidates.
    max_r = min(len(df), lookahead)
    best_row = 0
    best_score = float("-inf")
    for r in range(max_r):
        row = df.iloc[r].astype(str).str.strip()
        non_empty = row.replace({"": np.nan}).notna().mean()
        alpha_ratio = row.str.contains(r"[A-Za-z]", regex=True, na=False).mean()
        # Penalize rows with very low non-empty ratio
        score = alpha_ratio - (0.5 if non_empty < 0.7 else 0)
        if score > best_score:
            best_score = score
            best_row = r
    return best_row


def _build_headers(df: pd.DataFrame, header_row: int) -> Tuple[List[str], int]:
    row = df.iloc[header_row].tolist()
    headers: List[str] = []
    for val in row:
        s = re.sub(r"\s+", " ", str(val).strip())
        headers.append(s if s else "Column")
    # Deduplicate while preserving order
    seen = {}
    dedup: List[str] = []
    for h in headers:
        n = h
        if n in seen:
            seen[n] += 1
            n = f"{n}_{seen[n]}"
        else:
            seen[n] = 1
        dedup.append(n)
    start_row = header_row + 1
    return dedup, start_row


# -----------------------------
# Numeric normalization
# -----------------------------

def _normalize_whitespace_and_minus(series: pd.Series) -> pd.Series:
    s = series.astype(str)
    s = s.str.replace("\u2212", "-", regex=False)  # unicode minus → ASCII
    s = s.str.replace("\u00a0", " ", regex=False)  # NBSP → space
    s = s.str.replace(r"[\u2000-\u200B]", " ", regex=True)  # thin spaces
    return s


def normalize_numeric_series(series: pd.Series) -> pd.Series:
    s = series.astype(object)
    s = s.where(~s.isna(), None)
    s = _normalize_whitespace_and_minus(s).str.strip()

    lower = s.str.lower()
    s = s.mask(lower.isin(NULL_TOKENS), np.nan)
    s = s.fillna("")

    # Detect negative forms and strip wrappers
    mask_paren = s.str.match(PARENS_PATTERN, na=False)
    s = s.mask(mask_paren, s.str.replace(r"^[\(](.*)[\)]$", r"\1", regex=True))
    mask_trail = s.str.endswith("-", na=False)
    s = s.mask(mask_trail, s.str[:-1])
    negative_mask = mask_paren | mask_trail

    # Extract K/M/B multiplier and strip suffix
    m = s.str.extract(KMB_PATTERN)
    suffix = m[0].fillna("") if not m.empty else ""
    mult = pd.Series(1.0, index=s.index, dtype=float)
    mult = mult.mask(suffix.str.lower() == "k", 1e3)
    mult = mult.mask(suffix.str.lower() == "m", 1e6)
    mult = mult.mask(suffix.str.lower() == "b", 1e9)
    s = s.str.replace(KMB_PATTERN.pattern, "", regex=True)

    # Strip currency and common separators
    s = s.str.replace(CURRENCY_PATTERN, "", regex=True)
    s = s.str.replace(",", "", regex=False)
    s = s.str.replace(" ", "", regex=False)

    # Percent
    percent_mask = s.str.contains(PERCENT_PATTERN, na=False)
    s = s.str.replace(PERCENT_PATTERN, "", regex=True)

    nums = pd.to_numeric(s, errors="coerce")
    nums = nums * mult
    nums = nums.mask(negative_mask, -nums)
    nums = nums.mask(percent_mask, nums / 100.0)
    nums = nums.replace([np.inf, -np.inf], np.nan)
    return nums


def is_numeric_candidate(series: pd.Series) -> bool:
    sample = series.dropna()
    if sample.empty:
        return False
    sample = sample.sample(min(100, len(sample)), random_state=42)
    parsed = normalize_numeric_series(sample)
    ratio = parsed.notna().mean()
    return ratio >= 0.6


# -----------------------------
# Type inference & utilities
# -----------------------------

def _detect_datetime_format_from_value(val: str) -> Optional[str]:
    for regex, fmt in DATE_REGEXES:
        if regex.match(val):
            return fmt
    return None


def infer_column_type(series: pd.Series, name: str) -> Tuple[str, Optional[str]]:
    """Return (type, datetime_format)."""
    s = series.dropna().astype(str).str.strip()
    if s.empty:
        return "text", None

    # Try date detection (regex + pandas parse success rate)
    sample_vals = s.sample(min(50, len(s)), random_state=42)
    regex_hits = sum(1 for v in sample_vals if _detect_datetime_format_from_value(v))
    if regex_hits / max(1, len(sample_vals)) >= 0.6:
        parsed = pd.to_datetime(s, errors="coerce")
        if parsed.notna().mean() >= 0.8:
            for v in sample_vals:
                fmt = _detect_datetime_format_from_value(v)
                if fmt:
                    return "date", fmt
            return "date", None

    # Numeric / percentage / currency
    if is_numeric_candidate(series):
        lower_name = name.lower()
        contains_pct = any("%" in str(x) for x in series.dropna().head(50))
        contains_curr = any(CURRENCY_PATTERN.search(str(x) or "") for x in series.dropna().head(50))
        if contains_pct or any(k in lower_name for k in ("percent", "pct", "rate")):
            return "percentage", None
        if contains_curr or any(k in lower_name for k in ("price", "revenue", "amount", "cost", "currency")):
            return "currency", None
        nums = normalize_numeric_series(series)
        if nums.notna().any() and (nums.dropna() % 1 == 0).mean() > 0.95:
            return "integer", None
        return "float", None

    # ID-like
    unique_ratio = series.nunique(dropna=True) / max(1, len(series.dropna()))
    lower = name.lower()
    if unique_ratio > 0.9 and any(k in lower for k in ("id", "uuid", "code", "identifier")):
        return "id", None

    # Categorical vs text
    if series.dropna().nunique() / max(1, len(series.dropna())) <= 0.2:
        return "categorical", None

    return "text", None


# -----------------------------
# Main adapter
# -----------------------------

def process_file_to_artifacts(
    local_path: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,  # kept for compatibility; not used (non-destructive)
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    raw_df, file_kind = _load_raw(local_path)
    return process_df_to_artifacts(
        raw_df,
        file_kind,
        sample_rows_for_llm=sample_rows_for_llm,
        metric_rename_heuristic=metric_rename_heuristic,
        header_row_override=header_row_override,
    )


def process_bytes_to_artifacts(
    data: bytes,
    kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    """In-memory variant of process_file_to_artifacts.

    Parameters
    - data: raw file bytes
    - kind: "csv" or "excel"
    - sample_rows_for_llm: number of sampled rows to include in payload.sample_rows
    - metric_rename_heuristic: kept for API compatibility
    """
    if kind not in ("csv", "excel"):
        raise ValueError("kind must be 'csv' or 'excel'")
    if kind == "excel":
        raw_df = pd.read_excel(io.BytesIO(data), sheet_name=0, header=None, dtype=object)
        file_kind = "excel"
    else:
        # CSV fast path from bytes
        raw_df = pd.read_csv(
            io.BytesIO(data),
            header=None,
            engine="c",
            dtype_backend="pyarrow",
        )
        file_kind = "csv"
    return process_df_to_artifacts(
        raw_df,
        file_kind,
        sample_rows_for_llm=sample_rows_for_llm,
        metric_rename_heuristic=metric_rename_heuristic,
        header_row_override=header_row_override,
    )


def process_df_to_artifacts(
    raw_df: pd.DataFrame,
    file_kind: str,
    *,
    sample_rows_for_llm: int = 50,
    metric_rename_heuristic: bool = False,
    header_row_override: Optional[int] = None,
) -> ProcessResult:
    rows_before = raw_df.shape[0]

    work = _drop_fully_blank_rows(raw_df).reset_index(drop=True)
    # Determine header row (override or auto-detect)
    if header_row_override is not None:
        header_row = int(header_row_override)
        header_confidence = 1.0
        method = "override"
    else:
        lookahead = int(os.getenv("PREPROCESS_HEADER_LOOKAHEAD", "12"))
        header_row, header_confidence = detect_header_row_simple(work, lookahead=lookahead)
        method = "auto_detected"

    raw_headers = work.iloc[header_row].tolist() if len(work) > header_row else [None] * work.shape[1]
    headers, header_issues = finalize_headers(raw_headers)
    start_row = header_row + 1

    body = work.iloc[start_row:].reset_index(drop=True)
    body.columns = headers

    # Remove rows that fully match headers (repeated header lines)
    hdr_norm = [str(h).strip() for h in headers]
    keep_mask = []
    for _, r in body.iterrows():
        vals = [str(x).strip() for x in r.tolist()]
        keep_mask.append(vals != hdr_norm)
    df = body.loc[keep_mask].reset_index(drop=True)

    # Normalize numeric columns (best-effort)
    numeric_cols: List[str] = []
    for col in df.columns:
        if is_numeric_candidate(df[col]):
            df[col] = normalize_numeric_series(df[col])
            numeric_cols.append(col)

    # Type inference & column summaries
    columns_meta: Dict[str, Any] = {}
    for col in df.columns:
        col_type, dt_fmt = infer_column_type(df[col], col)
        col_series = df[col]
        nn = col_series.dropna()
        null_pct = 0.0 if len(col_series) == 0 else (1 - len(nn) / len(col_series)) * 100.0
        unique_pct = 0.0 if len(nn) == 0 else (nn.nunique() / len(nn)) * 100.0
        vc = nn.value_counts().head(5)
        top_values = []
        for v, c in vc.items():
            val: Any = v
            if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                continue
            if isinstance(v, (np.floating, np.integer)):
                val = float(v)
            if isinstance(v, str) and len(v) > 300:
                val = v[:300]
            top_values.append({"value": val, "count": int(c)})

        entry: Dict[str, Any] = {
            "type": col_type,
            "null_pct": round(float(null_pct), 2),
            "unique_pct": round(float(unique_pct), 2),
            "top_values": top_values,
        }
        if col_type == "date":
            entry["datetime_format"] = dt_fmt
        entry["is_potential_dimension"] = False
        columns_meta[col] = entry

    # Heuristic for potential dimension in the first column
    if len(df.columns) > 0:
        first = df.columns[0]
        first_type = columns_meta[first]["type"]
        numeric_others = [c for c in df.columns[1:] if columns_meta[c]["type"] in ("integer", "float", "percentage", "currency")]
        if first_type in ("categorical", "text") and len(numeric_others) >= max(1, int(0.5 * (len(df.columns) - 1))):
            columns_meta[first]["is_potential_dimension"] = True

    dataset_meta = {
        "rows": int(df.shape[0]),
        "columns": int(df.shape[1]),
        "column_names": list(map(str, df.columns.tolist())),
        "dtypes": {c: str(df[c].dtype) for c in df.columns},
    }

    # Deterministic sample rows
    n = min(sample_rows_for_llm, len(df))
    sample = df.sample(n=n, random_state=42) if n > 0 else df.head(0)

    sample_rows: List[Dict[str, Any]] = []
    for _, r in sample.iterrows():
        obj = {}
        for k, v in r.to_dict().items():
            if isinstance(v, str) and len(v) > 300:
                obj[k] = v[:300]
            elif isinstance(v, (np.floating, np.integer)):
                obj[k] = float(v)
            else:
                obj[k] = v if (v is None or isinstance(v, (str, int, float, bool))) else str(v)
        sample_rows.append(obj)

    # Build v2 analysis hints and dataset summary
    hints, dataset_summary = build_analysis_hints(df, headers, header_row, float(header_confidence))
    # Minimal header_info with transposed signal
    is_transposed = False
    non_empty_final = [h for h in headers if str(h).strip() != ""]
    if non_empty_final:
        is_transposed = all(is_numeric_string(h) for h in non_empty_final)

    header_info = {
        "method": method,
        "header_row_index": int(header_row),
        "confidence": float(header_confidence),
        "original_headers": [str(x).strip() if x is not None else None for x in raw_headers],
        "final_headers": headers,
        "is_transposed": bool(is_transposed),
    }
    if header_issues:
        header_info["issues"] = header_issues

    payload: Dict[str, Any] = {
        "dataset": dataset_meta,
        "columns": columns_meta,
        "sample_rows": sample_rows,
        "cleaning_report": {
            "header_row": int(header_row),
            "renamed_columns": {},
            "numeric_columns": numeric_cols,
            "rows_before": int(rows_before),
            "rows_after": int(df.shape[0]),
            "file_kind": "xlsx" if file_kind == "excel" else "csv",
        },
        "mode": "full",
        "version": "1",
        # v2 additive fields (compact)
        "schema_version": "2.0",
        "header_info": header_info,
        "analysis_hints": hints,
        "dataset_summary": dataset_summary,
    }

    if file_kind == "excel":
        payload["excelInfo"] = {"sheet_used": 0, "sheet_name": None, "sheets_total": None}

    return ProcessResult(
        cleaned_df=df,
        payload=payload,
        cleaning_report=payload["cleaning_report"],
        rows=int(df.shape[0]),
        columns=int(df.shape[1]),
    )
</file>

<file path="backend/run-preprocess/requirements.txt">
fastapi==0.111.0
uvicorn[standard]==0.30.0
gunicorn==22.0.0

# Pandas/Numpy/Arrow/Polars: keep pinned versions for Python <3.13 (deployment parity)
# and allow newer versions for Python >=3.13 (local dev on latest Python)
pandas==2.2.2; python_version < "3.13"
pandas>=2.2.3,<3; python_version >= "3.13"

numpy==2.0.2; python_version < "3.13"
numpy>=2.1.0,<3; python_version >= "3.13"

pyarrow==16.1.0; python_version < "3.13"
pyarrow>=16.1.0,<19; python_version >= "3.13"

openpyxl==3.1.5

polars[xlsx]==1.7.1; python_version < "3.13"
polars[xlsx]>=1.7.1,<2; python_version >= "3.13"

google-cloud-storage==2.16.0
google-cloud-firestore==2.16.0
google-cloud-logging==3.10.0
</file>

<file path="frontend/firebase.json">
{
  "hosting": {
    "public": "build",
    "ignore": [
      "firebase.json",
      "**/.*",
      "**/node_modules/**"
    ],
    "rewrites": [
      {
        "source": "/api/sign-upload-url",
        "function": {
          "functionId": "sign-upload-url",
          "region": "europe-west4"
        }
      },
      {
        "source": "/api/chat",
        "function": {
          "functionId": "chat",
          "region": "europe-west4"
        }
      },
      {
        "source": "**",
        "destination": "/index.html"
      }
    ]
  }
}
</file>

<file path="frontend/src/components/ui/scroll-area.tsx">
"use client";

import * as React from "react";
import * as ScrollAreaPrimitive from "@radix-ui/react-scroll-area";

import { cn } from "./utils";

const ScrollArea = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<typeof ScrollAreaPrimitive.Root>
>(({ className, children, ...props }, ref) => {
  return (
    <ScrollAreaPrimitive.Root
      ref={ref}
      data-slot="scroll-area"
      className={cn("relative", className)}
      {...props}
    >
      <ScrollAreaPrimitive.Viewport
        data-slot="scroll-area-viewport"
        className="focus-visible:ring-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] outline-none focus-visible:ring-[3px] focus-visible:outline-1"
      >
        {children}
      </ScrollAreaPrimitive.Viewport>
      <ScrollBar />
      <ScrollAreaPrimitive.Corner />
    </ScrollAreaPrimitive.Root>
  );
});
ScrollArea.displayName = "ScrollArea";

function ScrollBar({
  className,
  orientation = "vertical",
  ...props
}: React.ComponentProps<typeof ScrollAreaPrimitive.ScrollAreaScrollbar>) {
  return (
    <ScrollAreaPrimitive.ScrollAreaScrollbar
      data-slot="scroll-area-scrollbar"
      orientation={orientation}
      className={cn(
        "flex touch-none p-px transition-colors select-none",
        orientation === "vertical" &&
          "h-full w-2.5 border-l border-l-transparent",
        orientation === "horizontal" &&
          "h-2.5 flex-col border-t border-t-transparent",
        className,
      )}
      {...props}
    >
      <ScrollAreaPrimitive.ScrollAreaThumb
        data-slot="scroll-area-thumb"
        className="bg-border relative flex-1 rounded-full"
      />
    </ScrollAreaPrimitive.ScrollAreaScrollbar>
  );
}

export { ScrollArea, ScrollBar };
</file>

<file path="frontend/src/context/AuthContext.tsx">
import React, { createContext, useContext, useEffect, useState } from "react";
import { auth } from "../lib/firebase";
import { onIdTokenChanged, signInAnonymously, User } from "firebase/auth";

interface AuthState {
  user: User | null;
  idToken: string | null;
  loading: boolean;
}

const AuthContext = createContext<AuthState>({ user: null, idToken: null, loading: true });

export const AuthProvider: React.FC<{ children?: React.ReactNode }> = ({ children }) => {
  const [user, setUser] = useState<User | null>(null);
  const [idToken, setIdToken] = useState<string | null>(null);
  const [loading, setLoading] = useState<boolean>(true);

  useEffect(() => {
    let unsub = () => {};

    async function init() {
      try {
        // Ensure we have a user
        if (!auth.currentUser) {
          await signInAnonymously(auth);
        }
      } catch (_) {
        // ignore
      }

      unsub = onIdTokenChanged(auth, async (u) => {
        setUser(u);
        if (u) {
          const tok = await u.getIdToken();
          setIdToken(tok);
        } else {
          setIdToken(null);
        }
        setLoading(false);
      });
    }

    init();
    return () => unsub();
  }, []);

  return (
    <AuthContext.Provider value={{ user, idToken, loading }}>
      {children}
    </AuthContext.Provider>
  );
};

export function useAuth() {
  return useContext(AuthContext);
}
</file>

<file path="backend/deploy-analysis.ps1">
# =========
# Settings (Analysis Stage Only)
# =========
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

# Configure gcloud
gcloud config set project $PROJECT_ID | Out-Null

# Script-relative paths
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$SRC_FN_SIGN = Join-Path $SCRIPT_DIR "functions\sign_upload_url"
$SRC_FN_ORCH = Join-Path $SCRIPT_DIR "functions\orchestrator"

# =====================
# Enable required APIs
# =====================
gcloud services enable `
  cloudfunctions.googleapis.com `
  run.googleapis.com `
  artifactregistry.googleapis.com `
  storage.googleapis.com `
  firestore.googleapis.com `
  secretmanager.googleapis.com `
  iamcredentials.googleapis.com

# ===========================
# Project number and SA vars
# ===========================
$PROJECT_NUMBER = gcloud projects describe $PROJECT_ID --format="value(projectNumber)"
$SERVICE_ACCOUNT = "$PROJECT_NUMBER-compute@developer.gserviceaccount.com"

# ==============================
# Compose ALLOWED_ORIGINS string
# ==============================
$ALLOWED_ORIGINS = "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com"

# Build YAML env files for Gen2
$SIGN_ENV_FILE = Join-Path $SCRIPT_DIR "env.sign-upload-url.yaml"
$CHAT_ENV_FILE = Join-Path $SCRIPT_DIR "env.chat.yaml"

# Sign-upload-url env (YAML)
@"
FILES_BUCKET: "$BUCKET"
GCP_PROJECT: "$PROJECT_ID"
TTL_DAYS: "1"
RUNTIME_SERVICE_ACCOUNT: "$SERVICE_ACCOUNT"
ALLOWED_ORIGINS: "$ALLOWED_ORIGINS"
"@ | Out-File -Encoding ascii -FilePath $SIGN_ENV_FILE

# Chat env (YAML)
@"
FILES_BUCKET: "$BUCKET"
GCP_PROJECT: "$PROJECT_ID"
ORCH_IPC_MODE: "base64"
GEMINI_FUSED: "1"
RUNTIME_SERVICE_ACCOUNT: "$SERVICE_ACCOUNT"
ALLOWED_ORIGINS: "$ALLOWED_ORIGINS"
"@ | Out-File -Encoding ascii -FilePath $CHAT_ENV_FILE

# ======================================
# Secret Manager access for chat (Gemini)
# ======================================
gcloud secrets add-iam-policy-binding GEMINI_API_KEY `
  --member="serviceAccount:$SERVICE_ACCOUNT" `
  --role="roles/secretmanager.secretAccessor" `
  --project=$PROJECT_ID

# ============================================
# Deploy Functions Gen2: sign-upload-url (HTTP)
# ============================================
# Auth flag (set ALLOW_UNAUTHENTICATED=1 in env for dev convenience)
$AUTH_FLAG = if ($env:ALLOW_UNAUTHENTICATED -eq "1") { "--allow-unauthenticated" } else { "" }

gcloud functions deploy sign-upload-url `
  --gen2 `
  --runtime=python312 `
  --region=$REGION `
  --source="$SRC_FN_SIGN" `
  --entry-point="sign_upload_url" `
  --trigger-http `
  $AUTH_FLAG `
  --service-account="$SERVICE_ACCOUNT" `
  --env-vars-file="$SIGN_ENV_FILE"

# ======================================
# Deploy Functions Gen2: chat (SSE HTTP)
# ======================================
gcloud functions deploy chat `
  --gen2 `
  --runtime=python312 `
  --region=$REGION `
  --source="$SRC_FN_ORCH" `
  --entry-point="chat" `
  --trigger-http `
  $AUTH_FLAG `
  --service-account="$SERVICE_ACCOUNT" `
  --memory=512Mi `
  --env-vars-file="$CHAT_ENV_FILE" `
  --set-secrets="GEMINI_API_KEY=GEMINI_API_KEY:latest"

# ====================
# Grab service URLs
# ====================
$SIGN_URL = gcloud functions describe sign-upload-url --gen2 --region=$REGION --format="value(url)"
$CHAT_URL = gcloud functions describe chat --gen2 --region=$REGION --format="value(url)"

# Trim URLs
$SIGN_URL = ($SIGN_URL | Out-String).Trim()
$CHAT_URL = ($CHAT_URL | Out-String).Trim()

Write-Host "sign-upload-url: $SIGN_URL"
Write-Host "chat (SSE):     $CHAT_URL"
</file>

<file path="backend/functions/sign_upload_url/main.py">
import os
import json
import uuid
from datetime import datetime, timedelta, timezone
from typing import Tuple
import time
import re

from google.cloud import storage
from google.cloud import firestore
import google.auth
from google.auth import impersonated_credentials
import google.auth.transport.requests
import firebase_admin
from firebase_admin import auth as fb_auth

MAX_FILE_BYTES = 20 * 1024 * 1024  # 20 MB
FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PROJECT_ID = os.getenv("GCP_PROJECT", os.getenv("GOOGLE_CLOUD_PROJECT", "ai-data-analyser"))
TTL_DAYS = int(os.getenv("TTL_DAYS", "1"))
RUNTIME_SERVICE_ACCOUNT = os.getenv("RUNTIME_SERVICE_ACCOUNT")
SIGNING_CREDS_TTL_SECONDS = int(os.getenv("SIGNING_CREDS_TTL_SECONDS", "3300"))  # ~55m

ALLOWED_ORIGINS = {
    o.strip()
    for o in re.split(r'[,;]', os.getenv(
        "ALLOWED_ORIGINS",
        "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com",
    ) or "")
    if o.strip()
}

ALLOWED_MIME = {
    "text/csv": ".csv",
    "application/vnd.ms-excel": ".xls",
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",
}
ALLOWED_EXT = {".csv", ".xls", ".xlsx"}


def _ext_from_filename_or_type(filename: str, mime: str) -> str:
    ext = os.path.splitext(filename)[1].lower() if filename else ""
    if ext in ALLOWED_EXT:
        return ext
    return ALLOWED_MIME.get(mime, "")

def _origin_allowed(origin: str | None) -> bool:
    if not origin:
        return False
    # Normalize the origin by removing any trailing slashes before checking
    normalized_origin = origin.rstrip('/')
    return normalized_origin in ALLOWED_ORIGINS


def _require_session_id(request) -> str:
    sid = request.headers.get("X-Session-Id") or request.args.get("sessionId")
    if not sid:
        raise ValueError("Missing X-Session-Id header")
    return sid


_CACHED_SIGNING_CREDS = None
_CACHED_EXPIRES_AT = 0.0


def _impersonated_signing_credentials(sa_email: str):
    global _CACHED_SIGNING_CREDS, _CACHED_EXPIRES_AT
    now = time.time()
    if _CACHED_SIGNING_CREDS is not None and now < _CACHED_EXPIRES_AT:
        return _CACHED_SIGNING_CREDS
    if not sa_email:
        creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
        _CACHED_SIGNING_CREDS = creds
        _CACHED_EXPIRES_AT = now + SIGNING_CREDS_TTL_SECONDS
        return _CACHED_SIGNING_CREDS
    source_creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    if getattr(source_creds, "token", None) is None:
        source_creds.refresh(google.auth.transport.requests.Request())
    _CACHED_SIGNING_CREDS = impersonated_credentials.Credentials(
        source_credentials=source_creds,
        target_principal=sa_email,
        target_scopes=["https://www.googleapis.com/auth/cloud-platform"],
        lifetime=3600,
    )
    _CACHED_EXPIRES_AT = now + SIGNING_CREDS_TTL_SECONDS
    return _CACHED_SIGNING_CREDS


try:
    firebase_admin.get_app()
except ValueError:
    firebase_admin.initialize_app()


def sign_upload_url(request):
    # --- DIAGNOSTIC LOGS START ---
    origin = request.headers.get("Origin") or ""
    print("--- DIAGNOSTIC LOGS ---")
    print(f"Received Origin Header: '{origin}'")
    print(f"Parsed Allowed Origins Set: {ALLOWED_ORIGINS}")
    print(f"Origin Check Result: {_origin_allowed(origin)}")
    print("--- DIAGNOSTIC LOGS END ---")
    # --- DIAGNOSTIC LOGS END ---
    
    if request.method == "OPTIONS":
        # CORS preflight
        if not _origin_allowed(origin):
            return ("Origin not allowed", 403, {"Content-Type": "text/plain"})
        headers = {
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, content-type, Authorization, authorization, X-Session-Id, x-session-id",
            "Access-Control-Max-Age": "3600",
        }
        return ("", 204, headers)

    try:
        # Origin allowlist
        if not _origin_allowed(origin):
            return (json.dumps({"error": "origin not allowed"}), 403, {"Content-Type": "application/json"})

        # ... (rest of the function is the same)
        auth_header = request.headers.get("Authorization", "")
        token = auth_header.split(" ", 1)[1] if auth_header.lower().startswith("bearer ") else None
        if not token:
            return (json.dumps({"error": "missing Authorization Bearer token"}), 401, {"Content-Type": "application/json"})
        try:
            decoded = fb_auth.verify_id_token(token)
            uid = decoded.get("uid")
        except Exception as e:
            return (json.dumps({"error": "invalid token", "detail": str(e)[:200]}), 401, {"Content-Type": "application/json"})

        sid = _require_session_id(request)
        filename = request.args.get("filename", "")
        size = int(request.args.get("size", "0"))
        mime = request.args.get("type", "")

        if not filename or not mime:
            return (json.dumps({"error": "filename and type are required"}), 400, {"Content-Type": "application/json"})
        if size <= 0 or size > MAX_FILE_BYTES:
            return (json.dumps({"error": "file too large (max 20MB)"}), 400, {"Content-Type": "application/json"})

        ext = _ext_from_filename_or_type(filename, mime)
        if ext not in ALLOWED_EXT:
            return (json.dumps({"error": "unsupported file type"}), 400, {"Content-Type": "application/json"})

        dataset_id = str(uuid.uuid4())
        object_path = f"users/{uid}/sessions/{sid}/datasets/{dataset_id}/raw/input{ext}"

        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(FILES_BUCKET)
        blob = bucket.blob(object_path)

        signing_creds = _impersonated_signing_credentials(RUNTIME_SERVICE_ACCOUNT)

        url = blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=15),
            method="PUT",
            content_type=mime,
            credentials=signing_creds,
        )

        fs = firestore.Client(project=PROJECT_ID)
        ttl_at = datetime.now(timezone.utc) + timedelta(days=TTL_DAYS)
        fs.document("users", uid, "sessions", sid, "datasets", dataset_id).set(
            {
                "status": "awaiting_upload",
                "rawUri": f"gs://{FILES_BUCKET}/{object_path}",
                "createdAt": datetime.now(timezone.utc),
                "updatedAt": datetime.now(timezone.utc),
                "ttlAt": ttl_at,
            },
            merge=True,
        )

        resp = {
            "url": url,
            "datasetId": dataset_id,
            "storagePath": object_path,
        }
        headers = {
            "Content-Type": "application/json",
            "Access-Control-Allow-Origin": origin,
        }
        return (json.dumps(resp), 200, headers)

    except ValueError as ve:
        return (json.dumps({"error": str(ve)}), 400, {"Content-Type": "application/json"})
    except Exception as e:
        return (json.dumps({"error": "internal error", "detail": str(e)[:500]}), 500, {"Content-Type": "application/json"})
</file>

<file path="backend/run-preprocess/README.md">
# Preprocess Service (Cloud Run)

Region: `europe-west4`  •  Project: `ai-data-analyser`  •  Bucket: `ai-data-analyser-files`

This service is the HTTP target of an Eventarc trigger for `google.cloud.storage.object.v1.finalized` events. When a raw file is uploaded under the `users/{uid}/sessions/{sid}/datasets/{datasetId}/raw/` prefix, it:

1. Downloads the raw CSV/XLSX bytes into memory (no `/tmp` in the happy path).
2. Runs the pipeline adapter (Polars CSV by default; Excel via pandas) to clean + profile.
3. Writes:
   - `cleaned/cleaned.parquet`
   - `metadata/payload.json`
   - `reports/cleaning_report.json`
4. Updates Firestore dataset doc with URIs, rows, columns, `status=ready`, and `ttlAt`.

## Run locally

```bash
pip install -r requirements.txt
export FILES_BUCKET=ai-data-analyser-files
export GCP_PROJECT=ai-data-analyser
uvicorn main:app --reload --port 8080
```

Health check:
```
curl http://localhost:8080/healthz
```

## Deploy (sketch)

From the repository root, you can deploy just this service via:

```
./backend/deploy-preprocess.ps1
```

The script enables required APIs (once), deploys Cloud Run `preprocess-svc`, and ensures the Eventarc trigger. After deployment, run `./backend/test.ps1` for a quick smoke test.

If you prefer a unified flow for all backend components, use `./backend/deploy.ps1`.

## Notes

- Excel policy: first sheet only; payload includes `excelInfo`.
- No column renames; instead flag potential dimensions in the payload.
- Deterministic sample of 50 rows for payload; truncate long strings.
- Parquet written with `pyarrow` and compression.
- Polars string API: we use `str.strip_chars()` (not `str.strip()`) for whitespace trimming in expressions to match the deployed Polars version. If you upgrade Polars, re-verify string namespace compatibility in `pipeline_adapter_polars.py`.

### Engine selection

- Default engine for CSV is Polars. Set via env var `PREPROCESS_ENGINE=polars|pandas` (default: `polars`).
- Excel files are handled by pandas (first sheet) regardless of engine.
- Core cleaning/payload logic is centralized in `pipeline_adapter.process_df_to_artifacts()` and shared by both adapters.

---

## Operational Runbook

### Verify Cloud Run health

1) Get service URL

```
gcloud run services describe preprocess-svc \
  --region=europe-west4 --format="value(status.url)"
```

2) Health check endpoint

```
curl -s "$(gcloud run services describe preprocess-svc \
  --region=europe-west4 --format='value(status.url)')/healthz"
```

### Verify Eventarc trigger

```
gcloud eventarc triggers describe preprocess-trigger \
  --location=europe-west4
```

Confirm:

- `eventFilters`: `type=google.cloud.storage.object.v1.finalized`, `bucket=ai-data-analyser-files`
- `destination`: run service `preprocess-svc`, path `/eventarc`, region `europe-west4`

### End-to-end smoke test

Prereqs: `functions/sign-upload-url` deployed and publicly reachable.

1) Request signed URL (replace values as needed)

```
SIGN_URL=$(gcloud functions describe sign-upload-url \
  --gen2 --region=europe-west4 --format='value(url)')

UID=demo-uid
SID=demo-sid
FILE=test_files/basic.csv
MIME=text/csv

FILENAME=$(basename "$FILE")
curl -s -H "X-User-Id: $UID" -H "X-Session-Id: $SID" \
  "$SIGN_URL?filename=$(python -c "import urllib.parse,sys;print(urllib.parse.quote(sys.argv[1]))" "$FILENAME")&size=$(stat -c%s "$FILE")&type=$(python -c "import urllib.parse;print(urllib.parse.quote('$MIME'))")" \
  | tee /tmp/sign_resp.json

DATASET=$(jq -r .datasetId </tmp/sign_resp.json)
PUT_URL=$(jq -r .url </tmp/sign_resp.json)
```

2) Upload file to signed URL

```
curl -X PUT -H "Content-Type: $MIME" --data-binary @"$FILE" "$PUT_URL"
```

3) Wait and list artifacts

```
sleep 8
gcloud storage ls \
  "gs://ai-data-analyser-files/users/$UID/sessions/$SID/datasets/$DATASET/**"
```

4) Inspect Firestore doc (from Console or gcloud) at:

`users/{uid}/sessions/{sid}/datasets/{datasetId}`

### Troubleshooting

- Container failed to start (PORT): ensure the service is launched with a PORT-aware entrypoint.
  - We use `GOOGLE_ENTRYPOINT=python main.py` and `main.py` starts uvicorn honoring `$PORT`.
- Build fails compiling `pyarrow`: ensure Python 3.12 wheels are used.
  - `project.toml` sets `GOOGLE_PYTHON_VERSION=3.12` and `deploy.ps1` passes the same build env var.
</file>

<file path="frontend/index.html">
<!DOCTYPE html>
  <html lang="en">
    <head>
      <meta charset="UTF-8" />
      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <title>AI Data Analyst</title>
    </head>

    <body>
      <div id="root"></div>
      <script type="module" src="/src/main.tsx"></script>
    </body>
  </html>
</file>

<file path="frontend/src/components/ChatSidebar.tsx">
import React from "react";
import { Button } from "./ui/button";
import { ScrollArea } from "./ui/scroll-area";
import { PanelLeftClose, PanelLeft, SquarePen, Trash2, History } from "lucide-react";
import { cn } from "./ui/utils";

interface Conversation {
  id: string;
  title: string;
  timestamp: Date;
}

interface ChatSidebarProps {
  isOpen: boolean;
  onToggle: () => void;
  conversations: Conversation[];
  activeConversationId: string | null;
  onSelectConversation: (id: string) => void;
  onNewChat: () => void;
  userName: string;
  userPlan: string;
  dailyLimit: number;
  dailyUsed: number;
  onDeleteConversation: (id: string) => void;
}

export function ChatSidebar({
  isOpen,
  onToggle,
  conversations,
  activeConversationId,
  onSelectConversation,
  onNewChat,
  userName,
  userPlan,
  dailyLimit,
  dailyUsed,
  onDeleteConversation,
}: ChatSidebarProps) {
  return (
    <>
      {/* Sidebar */}
      <aside
        className={cn(
          "fixed left-0 top-0 h-full bg-sidebar border-r border-sidebar-border transition-all duration-300 flex flex-col z-40",
          isOpen ? "w-64" : "w-16"
        )}
      >
        {isOpen ? (
          <>
            {/* Header - Expanded */}
            <div className="h-14 p-3 flex items-center justify-between">
              <Button
                onClick={onNewChat}
                variant="ghost"
                className="flex-1 flex items-center justify-start gap-2"
              >
                <SquarePen className="h-4 w-4" />
                New chat
              </Button>
              <Button
                variant="ghost"
                size="icon"
                onClick={onToggle}
                className="ml-2"
              >
                <PanelLeftClose className="h-5 w-5" />
              </Button>
            </div>

            {/* Recent Chats Header */}
            <div className="px-3 py-2">
              <h2 className="flex items-center gap-2 text-sm font-semibold text-sidebar-foreground pl-4">
                <History className="h-4 w-4" />
                <span>Recent Chats</span>
              </h2>
            </div>

            {/* Chat History */}
            <ScrollArea className="flex-1 px-2">
              <div className="space-y-1">
                {conversations.map((conversation) => (
                  <div
                    key={conversation.id}
                    className={cn(
                      "group w-full flex items-center justify-between px-3 py-2.5 rounded-lg transition-colors text-left",
                      activeConversationId === conversation.id
                        ? "bg-sidebar-accent text-sidebar-accent-foreground"
                        : "text-sidebar-foreground hover:bg-sidebar-accent/50"
                    )}
                  >
                    <button
                      onClick={() => onSelectConversation(conversation.id)}
                      className="truncate flex-1 text-sm text-left pl-4"
                    >
                      {conversation.title}
                    </button>
                    <Button
                      variant="ghost"
                      size="icon"
                      aria-label="Delete conversation"
                      className={cn(
                        "h-8 w-8 shrink-0 opacity-0 transition-opacity group-hover:opacity-100 group-focus-within:opacity-100 focus-visible:opacity-100",
                        activeConversationId === conversation.id
                          ? "text-sidebar-accent-foreground"
                          : "text-sidebar-foreground"
                      )}
                      onClick={() => onDeleteConversation(conversation.id)}
                    >
                      <Trash2 className="h-4 w-4" />
                    </Button>
                  </div>
                ))}
              </div>
            </ScrollArea>

            {/* User Profile */}
            <div className="border-t border-sidebar-border p-3">
              <div className="flex items-center gap-3 px-3 py-2 rounded-lg hover:bg-sidebar-accent/50 cursor-pointer transition-colors">
                <div className="w-8 h-8 rounded-full bg-sidebar-primary flex items-center justify-center text-sidebar-primary-foreground">
                  {userName.charAt(0).toUpperCase()}
                </div>
                <div className="flex-1 min-w-0">
                  <div className="text-sm truncate">{userName}</div>
                  <div className="text-xs text-muted-foreground">{userPlan}</div>
                </div>
              </div>
              
              {/* Daily Limit */}
              <div className="mt-3 px-3 py-2 rounded-lg bg-sidebar-accent/30">
                <div className="text-xs text-muted-foreground mb-1">Daily Limit</div>
                <div className="text-sm">
                  {dailyUsed} / {dailyLimit} messages
                </div>
              </div>
            </div>
          </>
        ) : (
          <>
            {/* Header - Collapsed */}
            <div className="p-2 flex flex-col items-center gap-2">
              <Button
                variant="ghost"
                size="icon"
                onClick={onToggle}
              >
                <PanelLeft className="h-5 w-5" />
              </Button>
              <Button
                variant="ghost"
                size="icon"
                onClick={onNewChat}
                title="New chat"
              >
                <SquarePen className="h-5 w-5" />
              </Button>
            </div>

            {/* Recent Chats Icon - Collapsed */}
            <div className="p-2 flex flex-col items-center gap-2">
              <Button
                variant="ghost"
                size="icon"
                title="Recent Chats"
              >
                <History className="h-5 w-5" />
              </Button>
            </div>

            <div className="flex-1" />

            {/* User Profile - Collapsed */}
            <div className="border-t border-sidebar-border p-2 flex flex-col items-center gap-2">
              <Button
                variant="ghost"
                size="icon"
                title={`${userName} (${userPlan})`}
              >
                <div className="w-6 h-6 rounded-full bg-sidebar-primary flex items-center justify-center text-sidebar-primary-foreground text-xs">
                  {userName.charAt(0).toUpperCase()}
                </div>
              </Button>
            </div>
          </>
        )}
      </aside>
    </>
  );
}
</file>

<file path="frontend/src/main.tsx">
import React from "react";
import { createRoot } from "react-dom/client";
import App from "./App.tsx";
import "./styles/tailwind.css";
import { AuthProvider } from "./context/AuthContext";

createRoot(document.getElementById("root")!).render(
  <AuthProvider>
    <App />
  </AuthProvider>
);
</file>

<file path="frontend/src/services/api.ts">
import { fetchEventSource } from "@microsoft/fetch-event-source";

export interface SignedUrlResponse {
  url: string;
  datasetId: string;
  storagePath: string;
}

export type ChatEvent =
  | { type: "ping"; ts: string }
  | { type: "received"; data: { sessionId: string; datasetId: string } }
  | { type: "validating" }
  | { type: "generating_code" }
  | { type: "running_fast" }
  | { type: "summarizing" }
  | { type: "persisting" }
  | { type: "error"; data: { code: string; message: string } }
  | {
      type: "done";
      data: {
        messageId: string;
        chartData?: any;
        tableSample?: any[];
        uris?: Record<string, string>;
        urisGs?: Record<string, string>;
        summary?: string;
      };
    };

export async function getSignedUploadUrl(params: {
  signUrl: string;
  idToken: string;
  sessionId: string;
  filename: string;
  size: number;
  type: string;
}): Promise<SignedUrlResponse> {
  // Use URLSearchParams to safely build the query string.
  const query = new URLSearchParams({
    sessionId: params.sessionId,
    filename: params.filename,
    size: params.size.toString(),
    type: params.type,
  });

  // Append the query string to the base URL. This works for both
  // absolute URLs (like http://localhost...) and relative ones (/api/...).
  const fullUrl = `${params.signUrl}?${query.toString()}`;

  const res = await fetch(fullUrl, {
    method: "GET",
    headers: {
      Authorization: `Bearer ${params.idToken}`,
    },
  });

  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`sign-upload-url failed ${res.status}: ${txt}`);
  }
  return (await res.json()) as SignedUrlResponse;
}

export async function putToSignedUrl(url: string, file: File) {
  const res = await fetch(url, {
    method: "PUT",
    headers: {
      "Content-Type": file.type || "application/octet-stream",
    },
    body: file,
  });
  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`PUT upload failed ${res.status}: ${txt}`);
  }
}

export async function streamChat(params: {
  chatUrl: string;
  idToken: string;
  sessionId: string;
  datasetId: string;
  question: string;
  signal?: AbortSignal;
  onEvent: (ev: ChatEvent) => void;
}) {
  const { chatUrl, idToken, sessionId, datasetId, question, signal, onEvent } =
    params;

  await fetchEventSource(chatUrl, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${idToken}`,
    },
    body: JSON.stringify({ sessionId, datasetId, question }),
    signal,
    onmessage(msg) {
      if (!msg.data) return;
      try {
        const obj = JSON.parse(msg.data) as ChatEvent;
        onEvent(obj);
      } catch (_) {
        // ignore parse errors
      }
    },
    onerror(err) {
      throw err;
    },
    openWhenHidden: true,
  });
}
</file>

<file path="frontend/src/services/firestore.ts">
import { db } from "../lib/firebase";
import {
  collection,
  doc,
  setDoc,
  updateDoc,
  getDocs,
  orderBy,
  limit as qlimit,
  query,
  onSnapshot,
} from "firebase/firestore";
import type { Message } from "../components/ChatMessage";

export async function ensureSession(uid: string, sid: string, title: string) {
  const ref = doc(collection(db, "users", uid, "sessions"), sid);
  const now = new Date();
  await setDoc(
    ref,
    { title, createdAt: now, updatedAt: now },
    { merge: true }
  );
}

export function subscribeDatasetMeta(
  uid: string,
  sid: string,
  datasetId: string,
  cb: (meta: { rows?: number; columns?: number }) => void
): () => void {
  const ref = doc(collection(db, "users", uid, "sessions", sid, "datasets"), datasetId);
  const unsub = onSnapshot(ref, (snap) => {
    const d = (snap.data() as any) || {};
    cb({ rows: d?.rows, columns: d?.columns });
  });
  return unsub;
}

export async function updateSessionDataset(uid: string, sid: string, datasetId: string) {
  const ref = doc(collection(db, "users", uid, "sessions"), sid);
  const now = new Date();
  await updateDoc(ref, { datasetId, updatedAt: now });
}

export async function saveUserMessage(uid: string, sid: string, messageId: string, content: string) {
  const msgRef = doc(collection(db, "users", uid, "sessions", sid, "messages"), messageId);
  const now = new Date();
  await setDoc(msgRef, { role: "user", content, createdAt: now }, { merge: true });
}

function toDate(x: any): Date {
  // Firestore Timestamp compatibility
  if (x && typeof x.toDate === "function") return x.toDate();
  if (typeof x === "string" || typeof x === "number") return new Date(x);
  return new Date();
}

export interface ConversationLoaded {
  id: string;
  title: string;
  timestamp: Date;
  messages: Message[];
  datasetId?: string;
}

export async function getRecentSessionsWithMessages(uid: string, take: number): Promise<ConversationLoaded[]> {
  const sessCol = collection(db, "users", uid, "sessions");
  const q = query(sessCol, orderBy("updatedAt", "desc"), qlimit(take));
  const snap = await getDocs(q);

  const convs: ConversationLoaded[] = [];
  for (const s of snap.docs) {
    const sd = s.data() as any;
    const sessId = s.id;
    const title = sd?.title || "Untitled";
    const datasetId = sd?.datasetId as string | undefined;
    const timestamp = toDate(sd?.updatedAt || sd?.createdAt);

    // Load messages (ascending)
    const msgCol = collection(db, "users", uid, "sessions", sessId, "messages");
    const mQ = query(msgCol, orderBy("createdAt", "asc"), qlimit(200));
    const mSnap = await getDocs(mQ);
    const messages: Message[] = [];
    for (const m of mSnap.docs) {
      const md = m.data() as any;
      const role = (md?.role === "assistant" ? "assistant" : "user") as "assistant" | "user";
      const content = typeof md?.content === "string" ? md.content : "";
      messages.push({ id: m.id, role, kind: "text", content, timestamp: toDate(md?.createdAt) });
    }

    convs.push({ id: sessId, title, timestamp, messages, datasetId });
  }
  return convs;
}
</file>

<file path="frontend/src/styles/globals.css">
@custom-variant dark (&:is(.dark *));

:root {
  --font-size: 16px;
  --background: #ffffff;
  --foreground: oklch(0.145 0 0);
  --card: #ffffff;
  --card-foreground: oklch(0.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.145 0 0);
  --primary: #030213;
  --primary-foreground: oklch(1 0 0);
  --secondary: oklch(0.95 0.0058 264.53);
  --secondary-foreground: #030213;
  --muted: #ececf0;
  --muted-foreground: #717182;
  --accent: #e9ebef;
  --accent-foreground: #030213;
  --destructive: #d4183d;
  --destructive-foreground: #ffffff;
  --border: rgba(0, 0, 0, 0.1);
  --input: transparent;
  --input-background: #f3f3f5;
  --switch-background: #cbced4;
  --font-weight-medium: 500;
  --font-weight-normal: 400;
  --ring: oklch(0.708 0 0);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --radius: 0.625rem;
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.145 0 0);
  --sidebar-primary: #030213;
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.97 0 0);
  --sidebar-accent-foreground: oklch(0.205 0 0);
  --sidebar-border: oklch(0.922 0 0);
  --sidebar-ring: oklch(0.708 0 0);
}

.dark {
  --background: oklch(0.145 0 0);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.145 0 0);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.145 0 0);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.985 0 0);
  --primary-foreground: oklch(0.205 0 0);
  --secondary: oklch(0.269 0 0);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.269 0 0);
  --muted-foreground: oklch(0.708 0 0);
  --accent: oklch(0.269 0 0);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.396 0.141 25.723);
  --destructive-foreground: oklch(0.637 0.237 25.331);
  --border: oklch(0.269 0 0);
  --input: oklch(0.269 0 0);
  --ring: oklch(0.439 0 0);
  --font-weight-medium: 500;
  --font-weight-normal: 400;
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.205 0 0);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.488 0.243 264.376);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.269 0 0);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(0.269 0 0);
  --sidebar-ring: oklch(0.439 0 0);
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-destructive-foreground: var(--destructive-foreground);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-input-background: var(--input-background);
  --color-switch-background: var(--switch-background);
  --color-ring: var(--ring);
  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
  --radius-sm: calc(var(--radius) - 4px);
  --radius-md: calc(var(--radius) - 2px);
  --radius-lg: var(--radius);
  --radius-xl: calc(var(--radius) + 4px);
  --color-sidebar: var(--sidebar);
  --color-sidebar-foreground: var(--sidebar-foreground);
  --color-sidebar-primary: var(--sidebar-primary);
  --color-sidebar-primary-foreground: var(--sidebar-primary-foreground);
  --color-sidebar-accent: var(--sidebar-accent);
  --color-sidebar-accent-foreground: var(--sidebar-accent-foreground);
  --color-sidebar-border: var(--sidebar-border);
  --color-sidebar-ring: var(--sidebar-ring);
}

@layer base {
  * {
    @apply border-border outline-ring/50;
  }

  body {
    @apply bg-background text-foreground;
  }
}

/**
 * Base typography. This is not applied to elements which have an ancestor with a Tailwind text class.
 */
@layer base {
  :where(:not(:has([class*=" text-"]), :not(:has([class^="text-"])))) {
    h1 {
      font-size: var(--text-2xl);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    h2 {
      font-size: var(--text-xl);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    h3 {
      font-size: var(--text-lg);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    h4 {
      font-size: var(--text-base);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    p {
      font-size: var(--text-base);
      font-weight: var(--font-weight-normal);
      line-height: 1.5;
    }

    label {
      font-size: var(--text-base);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    button {
      font-size: var(--text-base);
      font-weight: var(--font-weight-medium);
      line-height: 1.5;
    }

    input {
      font-size: var(--text-base);
      font-weight: var(--font-weight-normal);
      line-height: 1.5;
    }
  }
}

html {
  font-size: var(--font-size);
}
</file>

<file path=".gitignore">
# ---- OS generated files ----
.DS_Store
Thumbs.db
desktop.ini

# ---- IDE / Editor ----
.vscode/
.idea/
*.swp

# ---- Node / Frontend ----
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# Build outputs
build/
dist/
.cache/
.parcel-cache/
.eslintcache
.turbo/
.next/
.out/
coverage/

# ---- Python / Backend ----
__pycache__/
*.py[cod]
*$py.class

# Virtual environments
.venv/
venv/

# Env & tooling
.env
.env.*
.envrc
.python-version
# Keep sample env files under version control
!.env.example
!.env.sample
!.env.local.example
/frontend/.env
/frontend/.env.local
/frontend/.env.development
/frontend/.env.production
/backend/env.chat.yaml
/backend/env.sign-upload-url.yaml
/backend/env.chat.env
/backend/env.sign-upload-url.env

# Python tooling caches
.mypy_cache/
.pytest_cache/
.tox/
.coverage
.coverage.*
htmlcov/

# ---- Logs ----
logs/
*.log

# ---- Jupyter ----
.ipynb_checkpoints/

# ---- Test files ----
test_files/
test_files
/test_files



final_project_design.md

backend/env.chat.env
backend/env.sign-upload-url.env
</file>

<file path="backend/functions/orchestrator/gemini_client.py">
from __future__ import annotations

import os
import re
from typing import Any, Dict, List

import google.generativeai as genai

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

_MODEL_NAME = "gemini-2.5-flash"  # As requested
_API_KEY = os.getenv("GEMINI_API_KEY", "")

_MAX_TOKENS = 4096
_TEMPERATURE = 0.2

_configured = False
_model = None


def _ensure_model():
    """Ensure the Gemini client is configured and return the model."""
    global _configured, _model
    if not _configured:
        if not _API_KEY:
            raise RuntimeError("GEMINI_API_KEY not set")
        genai.configure(api_key=_API_KEY)
        _model = genai.GenerativeModel(_MODEL_NAME)
        _configured = True
    return _model


# ---------------------------------------------------------------------------
# Core: Generate Analysis Code
# ---------------------------------------------------------------------------

def generate_analysis_code(
    question: str,
    schema_snippet: str,
    sample_rows: list[dict],
    row_limit: int = 200
) -> str:
    """
    Ask Gemini to produce Python code, with a one-time auto-repair attempt.
    This function is primarily used for the repair loop.
    """
    model = _ensure_model()
    sample_preview = sample_rows[: min(len(sample_rows), 10)]

    prompt = (
        "You are an expert Python data analyst. Write a single Python function "
        "`def run(df, ctx):` to answer the user's question about the dataset.\n\n"
        "OUTPUT RULES:\n"
        "- Return ONLY one fenced Python code block starting with ```python.\n"
        "- The function must return a dictionary as specified in the project documentation.\n"
        "- Use matplotlib or seaborn for charts when the user asks for a chart/plot/visualization.\n"
        "- Allowed imports: pandas, numpy, matplotlib, seaborn, math, statistics, json, io, "
        "itertools, functools, collections, re, datetime, base64.\n\n"
        f"SCHEMA:\n{schema_snippet}\n\n"
        f"SAMPLE ROWS:\n{sample_preview}\n\n"
        f"USER QUESTION:\n\"{question}\"\n\n"
        "Return only the fenced Python code block now."
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "max_output_tokens": _MAX_TOKENS,
            "temperature": _TEMPERATURE,
        },
    )

    text = _safe_response_text(resp)
    code = _extract_code_block(text)

    if not code or "def run(" not in code:
        raise RuntimeError("CODEGEN_FAILED: Missing valid 'def run(df, ctx):' implementation.")
    return code


# ---------------------------------------------------------------------------
# Generate Summary
# ---------------------------------------------------------------------------

def generate_summary(
    question: str,
    table_head: list[dict],
    metrics: dict,
    code: str | None = None
) -> str:
    """Generate a concise, data-driven summary from analysis results."""
    model = _ensure_model()
    preview = table_head[: min(len(table_head), 5)]

    prompt = (
        "You are a financial data analyst. Interpret the analysis results below. "
        "Focus on trends, anomalies, or key figures; do NOT describe the code.\n\n"
        f"USER QUESTION: \"{question}\"\n\n"
        f"TABLE PREVIEW:\n{preview}\n\n"
        f"KEY METRICS:\n{metrics}\n\n"
        "Now write a one-paragraph interpretation of the data:"
    )

    try:
        resp = model.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": _MAX_TOKENS,
                "temperature": _TEMPERATURE,
            },
        )
        text = _safe_response_text(resp).strip()
        if text:
            return text
    except Exception:
        # Fallthrough to generate a fallback summary if API fails
        pass

    # Fallback: minimal textual info if API fails or returns empty
    parts = []
    if question: parts.append(f"Question: {question}")
    if metrics: parts.append(f"Metrics: {list(metrics.keys())[:5]}")
    return " ".join(parts) or "No textual summary available."


# ---------------------------------------------------------------------------
# Fused: Generate Code + Summary in a Single Call
# ---------------------------------------------------------------------------

def generate_code_and_summary(
    question: str,
    schema_snippet: str,
    sample_rows: list[dict],
    row_limit: int = 200
) -> tuple[str, str]:
    """
    Return (code, summary) using a single Gemini call, with a one-time repair loop.
    """
    fused = os.getenv("GEMINI_FUSED", "1").lower() not in ("0", "false", "no")
    if not fused:
        code = generate_analysis_code(question, schema_snippet, sample_rows, row_limit=row_limit)
        return code, "Analysis planned. Executed results will follow."

    model = _ensure_model()
    sample_preview = sample_rows[: min(len(sample_rows), 10)]

    prompt = (
        "You are an expert Python data analyst.\n\n"
        "Step 1: Provide a one-sentence summary of the analysis you will perform.\n"
        "Step 2: Provide one fenced Python code block implementing `def run(df, ctx):`.\n\n"
        "CODE REQUIREMENTS:\n"
        "- Must return a dictionary with keys like 'summary', 'tables', 'charts'.\n"
        "- Use matplotlib or seaborn if charts are requested.\n"
        "- Allowed imports: pandas, numpy, matplotlib, seaborn, math, statistics, json, io, "
        "itertools, functools, collections, re, datetime, base64.\n\n"
        f"--- DATA CONTEXT ---\nSchema: {schema_snippet}\nSample rows: {sample_preview}\n\n"
        f"--- QUESTION ---\n\"{question}\"\n\n"
        "Return your one-line summary, then the Python code block."
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "max_output_tokens": _MAX_TOKENS,
            "temperature": _TEMPERATURE,
        },
    )

    text = _safe_response_text(resp)
    code = _extract_code_block(text)
    
    # One-time repair attempt if initial extraction fails
    if not code:
        feedback_prompt = (
            "Your previous response was not formatted correctly. "
            f"Please regenerate the response for the following question: \"{question}\"\n\n"
            "Return a one-sentence summary, then a single, valid, fenced Python code block "
            "defining `def run(df, ctx):`."
        )
        retry_resp = model.generate_content(
            feedback_prompt,
            generation_config={
                "max_output_tokens": _MAX_TOKENS,
                "temperature": 0.0, # Use 0 temp for deterministic repair
            },
        )
        text = _safe_response_text(retry_resp)
        code = _extract_code_block(text)

    # If code extraction still fails, return the raw text for the orchestrator's repair loop.
    if not code:
        return "", text

    summary = text.split("```")[0].strip() or "Analysis planned. Executed results will follow."
    return code, summary


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _safe_response_text(resp) -> str:
    """Safely extract text from Gemini responses, handling potential exceptions."""
    try:
        if getattr(resp, "text", None):
            return resp.text or ""
        if getattr(resp, "candidates", None):
            for c in resp.candidates:
                content = getattr(c, "content", None)
                parts = getattr(content, "parts", None) if content else None
                if parts:
                    return "".join(p.text for p in parts if hasattr(p, "text"))
    except Exception:
        return ""
    return ""


def _extract_code_block(text: str) -> str:
    """Extract a Python code block robustly from a response."""
    # 1. Prefer fenced python block
    m = re.search(r"```python\s*(.*?)```", text, flags=re.DOTALL | re.IGNORECASE)
    if m:
        code = m.group(1).strip()
        if "def run(" in code:
            return code

    # 2. Fallback to any fenced block
    m = re.search(r"```(.*?)```", text, flags=re.DOTALL)
    if m:
        code = m.group(1).strip()
        if "def run(" in code:
            return code

    # 3. Heuristic fallback: find the start of the function definition
    m = re.search(r"(def\s+run\s*\(.*)", text, flags=re.DOTALL)
    if m:
        return m.group(1).strip()

    return ""
</file>

<file path="frontend/package.json">
{
      "name": "ai-data-analyst-frontend",
      "version": "0.1.0",
      "private": true,
      "dependencies": {
            "@radix-ui/react-accordion": "^1.2.3",
            "@radix-ui/react-alert-dialog": "^1.1.6",
            "@radix-ui/react-aspect-ratio": "^1.1.2",
            "@radix-ui/react-avatar": "^1.1.3",
            "@radix-ui/react-checkbox": "^1.1.4",
            "@radix-ui/react-collapsible": "^1.1.3",
            "@radix-ui/react-context-menu": "^2.2.6",
            "@radix-ui/react-dialog": "^1.1.6",
            "@radix-ui/react-dropdown-menu": "^2.1.6",
            "@radix-ui/react-hover-card": "^1.1.6",
            "@radix-ui/react-label": "^2.1.2",
            "@radix-ui/react-menubar": "^1.1.6",
            "@radix-ui/react-navigation-menu": "^1.2.5",
            "@radix-ui/react-popover": "^1.1.6",
            "@radix-ui/react-progress": "^1.1.2",
            "@radix-ui/react-radio-group": "^1.2.3",
            "@radix-ui/react-scroll-area": "^1.2.3",
            "@radix-ui/react-select": "^2.1.6",
            "@radix-ui/react-separator": "^1.1.2",
            "@radix-ui/react-slider": "^1.2.3",
            "@radix-ui/react-slot": "^1.1.2",
            "@radix-ui/react-switch": "^1.1.3",
            "@radix-ui/react-tabs": "^1.1.3",
            "@radix-ui/react-toggle": "^1.1.2",
            "@radix-ui/react-toggle-group": "^1.1.2",
            "@radix-ui/react-tooltip": "^1.1.8",
            "class-variance-authority": "^0.7.1",
            "clsx": "*",
            "cmdk": "^1.1.1",
            "embla-carousel-react": "^8.6.0",
            "input-otp": "^1.4.2",
            "lucide-react": "^0.487.0",
            "next-themes": "^0.4.6",
            "react": "^18.3.1",
            "react-day-picker": "^8.10.1",
            "react-dom": "^18.3.1",
            "react-hook-form": "^7.55.0",
            "react-resizable-panels": "^2.1.7",
            "recharts": "^2.15.2",
            "sonner": "^2.0.3",
            "tailwind-merge": "*",
            "vaul": "^1.1.2",
            "firebase": "^11.0.2",
            "@microsoft/fetch-event-source": "^2.0.1"
      },
      "devDependencies": {
            "@tailwindcss/postcss": "^4.1.13",
            "@types/node": "^20.10.0",
            "@vitejs/plugin-react-swc": "^3.10.2",
            "autoprefixer": "^10.4.21",
            "postcss": "^8.5.6",
            "tailwindcss": "^4.1.13",
            "vite": "6.3.5"
      },
      "scripts": {
            "dev": "vite",
            "build": "vite build"
      }
}
</file>

<file path="frontend/src/components/renderers/TableRenderer.tsx">
import React from "react";
import { ChevronsUpDown, ChevronUp, ChevronDown } from "lucide-react";

export function TableRenderer({ rows }: { rows: any[] }) {
  const cols: string[] = React.useMemo(() => {
    if (!rows || rows.length === 0) return [];
    const keys = new Set<string>();
    for (const r of rows.slice(0, 50)) {
      Object.keys(r || {}).forEach((k) => keys.add(k));
    }
    return Array.from(keys);
  }, [rows]);

  const [sortBy, setSortBy] = React.useState<string | null>(null);
  const [sortDir, setSortDir] = React.useState<"asc" | "desc">("asc");

  const onToggleSort = (col: string) => {
    if (sortBy !== col) {
      setSortBy(col);
      setSortDir("asc");
    } else {
      setSortDir((d) => (d === "asc" ? "desc" : "asc"));
    }
  };

  const sortedRows = React.useMemo(() => {
    const sample = rows ? rows.slice(0, 50) : [];
    if (!sortBy) return sample;
    const copy = sample.slice();
    copy.sort((a: any, b: any) => {
      const av = a?.[sortBy];
      const bv = b?.[sortBy];
      // Try numeric compare if both parseable
      const an = typeof av === "number" ? av : Number.parseFloat(av);
      const bn = typeof bv === "number" ? bv : Number.parseFloat(bv);
      const bothNumeric = Number.isFinite(an) && Number.isFinite(bn);
      let cmp = 0;
      if (bothNumeric) {
        cmp = an === bn ? 0 : an < bn ? -1 : 1;
      } else {
        const as = String(av ?? "").toLowerCase();
        const bs = String(bv ?? "").toLowerCase();
        cmp = as === bs ? 0 : as < bs ? -1 : 1;
      }
      return sortDir === "asc" ? cmp : -cmp;
    });
    return copy;
  }, [rows, sortBy, sortDir]);

  if (!rows || rows.length === 0) {
    return (
      <div className="text-sm text-muted-foreground">No rows to display.</div>
    );
  }

  return (
    <div className="relative inline-block max-w-full max-h-[420px] overflow-auto border rounded-xl">
      <table className="min-w-max text-sm">
        <thead className="bg-muted/40 sticky top-0 z-10">
          <tr>
            {cols.map((c) => {
              const isActive = sortBy === c;
              const icon = !isActive ? (
                <ChevronsUpDown className="h-3.5 w-3.5 opacity-60" />
              ) : sortDir === "asc" ? (
                <ChevronUp className="h-3.5 w-3.5" />
              ) : (
                <ChevronDown className="h-3.5 w-3.5" />
              );
              return (
                <th key={c} className="text-left px-3 py-2 font-medium whitespace-nowrap bg-muted/40">
                  <button
                    type="button"
                    className="inline-flex items-center gap-1 hover:underline decoration-dotted cursor-pointer select-none"
                    aria-sort={isActive ? (sortDir === "asc" ? "ascending" : "descending") : "none"}
                    onClick={() => onToggleSort(c)}
                  >
                    <span>{c}</span>
                    {icon}
                  </button>
                </th>
              );
            })}
          </tr>
        </thead>
        <tbody className="divide-y divide-border">
          {sortedRows.map((r, i) => (
            <tr key={i} className="odd:bg-background hover:bg-muted/30">
              {cols.map((c) => (
                <td key={c} className="px-3 py-2 whitespace-nowrap max-w-[320px] overflow-hidden text-ellipsis">
                  {String(r?.[c] ?? "")}
                </td>
              ))}
            </tr>
          ))}
        </tbody>
      </table>
      {rows.length > 50 && (
        <div className="text-xs text-muted-foreground p-2">Showing first 50 rows of {rows.length}.</div>
      )}
    </div>
  );
}
</file>

<file path="backend/functions/orchestrator/worker.py">
#!/usr/bin/env python3
"""
Worker process to safely execute LLM-generated analysis code in a sandboxed environment.

This script is launched as a subprocess by the orchestrator. It receives JSON via stdin:
{
  "code": "<python source>",
  "parquet_b64": "<base64 bytes>" | optional,
  "arrow_ipc_b64": "<base64 bytes>" | optional,
  "parquet_path": "/tmp/cleaned.parquet" | optional,
  "ctx": { ... }
}

Steps:
1. Validate code with sandbox_runner.
2. Load the dataset into a DataFrame.
3. Execute the validated run(df, ctx) safely.
4. Sanitize and normalize the result for JSON output.

It always prints a JSON payload to stdout and exits with 0 unless the payload is malformed.
"""
from __future__ import annotations

import io
import json
import base64
import sys
import signal
import traceback
import os
from typing import Any

import pandas as pd
import numpy as np
import pyarrow as pa  # type: ignore

import matplotlib
matplotlib.use("Agg")  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

# Try to import sandbox validator (preferred)
try:
    from sandbox_runner import structured_validate, ALLOWED_IMPORTS as SANDBOX_ALLOWED_IMPORTS
except Exception:
    SANDBOX_ALLOWED_IMPORTS = {
        "pandas", "numpy", "matplotlib", "seaborn",
        "math", "statistics", "json", "io", "itertools", "functools",
        "collections", "re", "datetime", "base64"
    }
    structured_validate = None  # fallback


# --------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------
ALLOWED_IMPORTS = set(SANDBOX_ALLOWED_IMPORTS)
CODE_TIMEOUT = int(os.getenv("CODE_TIMEOUT", "60"))
MAX_MEMORY_BYTES = int(os.getenv("CODE_MAX_MEMORY_BYTES", str(512 * 1024 * 1024)))  # 512MB
try:
    import resource
except Exception:
    resource = None


# --------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------
def sanitize_for_json(obj: Any) -> Any:
    """Recursively replaces NaN/Inf with None for Firestore/JSON compatibility."""
    if isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [sanitize_for_json(v) for v in obj]
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    return obj


def _safe_import(name, globals=None, locals=None, fromlist=(), level=0):
    root = (name.split(".") or [name])[0]
    if root not in ALLOWED_IMPORTS:
        raise ImportError(f"Import not allowed: {name}")
    return _orig_import(name, globals, locals, fromlist, level)


def _prepare_globals() -> dict:
    """Prepare restricted globals for execution."""
    import builtins as _builtins
    safe_builtins = {
        b: getattr(_builtins, b)
        for b in [
            "abs", "all", "any", "bool", "dict", "enumerate", "filter",
            "float", "int", "len", "list", "map", "max", "min", "pow",
            "range", "round", "set", "slice", "sorted", "str", "sum",
            "zip", "print", "isinstance", "getattr", "hasattr", "type",
        ]
        if hasattr(_builtins, b)
    }
    safe_builtins["__import__"] = _safe_import
    return {"__builtins__": safe_builtins, "pd": pd, "np": np, "plt": plt, "sns": sns, "RESULT": None}


def _set_resource_limits():
    """Apply memory and CPU limits (POSIX only)."""
    if resource is None:
        return
    try:
        resource.setrlimit(resource.RLIMIT_AS, (MAX_MEMORY_BYTES, MAX_MEMORY_BYTES))
    except Exception:
        pass
    try:
        resource.setrlimit(resource.RLIMIT_CPU, (CODE_TIMEOUT + 5, CODE_TIMEOUT + 5))
    except Exception:
        pass


class _TimeoutException(Exception):
    pass


def _timeout_handler(signum, frame):
    raise _TimeoutException("User code timed out")


def _load_dataframe(payload: dict) -> pd.DataFrame:
    """Load df from base64 Parquet/Arrow or path."""
    if payload.get("arrow_ipc_b64"):
        ipc_bytes = base64.b64decode(payload["arrow_ipc_b64"])
        with pa.ipc.open_stream(io.BytesIO(ipc_bytes)) as reader:
            table = reader.read_all()
        return table.to_pandas()
    if payload.get("parquet_b64"):
        data = base64.b64decode(payload["parquet_b64"])
        return pd.read_parquet(io.BytesIO(data))
    if payload.get("parquet_path"):
        return pd.read_parquet(payload["parquet_path"])
    raise ValueError("Missing data payload: no parquet_b64, arrow_ipc_b64, or parquet_path provided")


def _fallback_result(df: pd.DataFrame, ctx: dict) -> dict:
    """Fallback minimal result when code fails."""
    row_limit = int((ctx or {}).get("row_limit", 200))
    return {
        "table": df.head(row_limit).to_dict(orient="records"),
        "metrics": {"rows": len(df), "columns": len(df.columns)},
        "chartData": {},
        "message": "Fallback result generated due to code execution failure."
    }


# --------------------------------------------------------------------------
# Main Execution
# --------------------------------------------------------------------------
def main() -> int:
    global _orig_import
    import builtins as _builtins
    _orig_import = _builtins.__import__

    # Step 1: Read payload
    try:
        payload = json.load(sys.stdin)
        code = payload.get("code", "")
        ctx = payload.get("ctx", {}) or {}
        if not code:
            raise ValueError("Missing 'code' field in payload")
    except Exception as e:
        sys.stderr.write(f"Invalid input payload: {e}\n")
        return 1

    # Step 2: Validate code via sandbox_runner
    try:
        if structured_validate:
            validation = structured_validate(code)
            if not validation.get("ok", False):
                output = {
                    "table": [],
                    "metrics": {},
                    "chartData": {},
                    "error": "Validation failed",
                    "validation": validation,
                }
                print(json.dumps(output, ensure_ascii=False))
                return 0
    except Exception as e:
        output = {
            "table": [],
            "metrics": {},
            "chartData": {},
            "error": f"Validator error: {e}",
        }
        print(json.dumps(output, ensure_ascii=False))
        return 0

    # Step 3: Load DataFrame
    try:
        df = _load_dataframe(payload)
    except Exception as e:
        output = {"table": [], "metrics": {}, "chartData": {}, "error": f"Failed to load data: {e}"}
        print(json.dumps(output, ensure_ascii=False))
        return 0

    # Step 4: Execute code safely
    globs = _prepare_globals()
    locs: dict = {}

    _set_resource_limits()
    old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
    signal.alarm(CODE_TIMEOUT)

    try:
        compiled = compile(code, filename="<user_code>", mode="exec")
        exec(compiled, globs, locs)

        run_func = locs.get("run") or globs.get("run")
        if not callable(run_func):
            raise RuntimeError("Missing required function: def run(df, ctx):")

        result = run_func(df, ctx)
        if result is None:
            result = globs.get("RESULT")

        # Normalize
        if isinstance(result, pd.DataFrame):
            result = {"table": result.to_dict(orient="records"), "metrics": {}, "chartData": {}}
        elif isinstance(result, list):
            result = {"table": result, "metrics": {}, "chartData": {}}
        elif not isinstance(result, dict):
            result = _fallback_result(df, ctx)

        # Ensure required keys
        result.setdefault("table", df.head(int(ctx.get("row_limit", 200))).to_dict(orient="records"))
        result.setdefault("metrics", {"rows": len(df), "columns": len(df.columns)})
        result.setdefault("chartData", {})

        # Enforce chart rule
        q = str(ctx.get("question", "")).lower()
        if not any(t in q for t in ("chart", "plot", "graph", "visualization")):
            result["chartData"] = {}

        sanitized = sanitize_for_json(result)
        print(json.dumps(sanitized, ensure_ascii=False))
        return 0

    except _TimeoutException:
        output = {"table": [], "metrics": {}, "chartData": {}, "error": f"Execution timed out after {CODE_TIMEOUT}s."}
        print(json.dumps(output, ensure_ascii=False))
        return 0

    except Exception as e:
        tb = traceback.format_exc(limit=8)
        output = {
            "table": [],
            "metrics": {},
            "chartData": {},
            "error": f"Runtime error: {e}",
            "traceback": tb,
        }
        print(json.dumps(output, ensure_ascii=False))
        return 0

    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)


if __name__ == "__main__":
    raise SystemExit(main())
</file>

<file path="backend/test.ps1">
# Config
$PROJECT_ID = "ai-data-analyser"
$REGION = "europe-west4"
$BUCKET = "ai-data-analyser-files"

$UID = "demo-uid"
$SID = "demo-sid"
$SCRIPT_DIR = Split-Path -Parent $MyInvocation.MyCommand.Path
$ROOT_DIR   = Split-Path -Parent $SCRIPT_DIR
$FILE = (Join-Path $ROOT_DIR "test_files\basic.csv")
$MIME = "text/csv"

# 1) Cloud Run health (service is public right now)
$PROJECT_NUMBER = (gcloud projects describe $PROJECT_ID --format "value(projectNumber)" | Out-String).Trim()

# Prefer URL reported by Cloud Run; fallback to canonical project-number URL
$RUN_URL_SVC = (gcloud run services describe preprocess-svc --region=$REGION --format "value(status.url)" | Out-String).Trim()
if ($RUN_URL_SVC -and ($RUN_URL_SVC -match '^https?://')) {
  $RUN_URL = $RUN_URL_SVC
} else {
  $RUN_URL = "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app"
}
Write-Host "RUN_URL: $RUN_URL"

# Try health; if it fails, try alternate once, then continue without throwing
$healthOk = $false
try {
  $health = Invoke-RestMethod -Uri "$RUN_URL/healthz" -Method GET -TimeoutSec 10
  Write-Host "Health: $($health | ConvertTo-Json -Compress)"
  $healthOk = $true
} catch {
  Write-Host "Health check warning:" $_.Exception.Message
  # Pick alternate URL
  if ($RUN_URL -like "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app") {
    $RUN_URL_ALT = $RUN_URL_SVC
  } else {
    $RUN_URL_ALT = "https://preprocess-svc-$PROJECT_NUMBER.$REGION.run.app"
  }
  if ($RUN_URL_ALT -and ($RUN_URL_ALT -match '^https?://') -and ($RUN_URL_ALT -ne $RUN_URL)) {
    Write-Host "Retrying health on alternate URL: $RUN_URL_ALT"
    try {
      $health2 = Invoke-RestMethod -Uri "$RUN_URL_ALT/healthz" -Method GET -TimeoutSec 10
      Write-Host "Health (alt): $($health2 | ConvertTo-Json -Compress)"
      $RUN_URL = $RUN_URL_ALT
      $healthOk = $true
    } catch {
      Write-Host "Health check warning (alt):" $_.Exception.Message
    }
  }
}
if (-not $healthOk) {
  Write-Host "Proceeding despite health probe warnings (likely local DNS/routing)."
}

# 2) Get signed URL (unchanged)
$SIGN_URL = (gcloud functions describe sign-upload-url --gen2 --region=$REGION --format "value(url)" | Out-String).Trim()
Write-Host "SIGN_URL: $SIGN_URL"
if (-not $SIGN_URL -or -not ($SIGN_URL -match '^https?://')) { throw "SIGN_URL invalid or empty" }

if (-not (Test-Path $FILE)) { throw "Test file not found at $FILE" }
$SIZE = (Get-Item $FILE).Length
$FILENAME = [System.IO.Path]::GetFileName($FILE)
$encName = [System.Uri]::EscapeDataString($FILENAME)
$encMime = [System.Uri]::EscapeDataString($MIME)
$reqUri = ("{0}?filename={1}&size={2}&type={3}" -f $SIGN_URL, $encName, $SIZE, $encMime)

$headers = @{
  "X-User-Id"    = $UID
  "X-Session-Id" = $SID
}
Write-Host "Signed URL request URI: $reqUri"
try {
  $resp = Invoke-RestMethod -Uri $reqUri -Headers $headers -Method GET
} catch {
  Write-Host "Signed URL request failed:" $_.Exception.Message
  throw
}

Write-Host "DatasetId: $($resp.datasetId)"
if (-not $resp.url) { throw "Signed URL missing in response" }

# 3) Upload
Invoke-WebRequest -Uri $resp.url -Method PUT -InFile $FILE -ContentType $MIME | Out-Null
Write-Host "Upload complete."

# 4) Wait and list artifacts
Start-Sleep -Seconds 30
gcloud storage ls "gs://$BUCKET/users/$UID/sessions/$SID/datasets/$($resp.datasetId)/**"

# 5) Fetch Firestore dataset status (best-effort)
try {
  $TOKEN = (gcloud auth print-access-token | Out-String).Trim()
  $docUrl = "https://firestore.googleapis.com/v1/projects/$PROJECT_ID/databases/(default)/documents/users/$UID/sessions/$SID/datasets/$($resp.datasetId)"
  $doc = Invoke-RestMethod -Uri $docUrl -Headers @{ Authorization = "Bearer $TOKEN" } -Method GET -TimeoutSec 10
  $status = $doc.fields.status.stringValue
  if ($status) {
    Write-Host "Firestore status: $status"
  } else {
    Write-Host "Firestore document retrieved (status field missing):"
  }
} catch {
  Write-Host "Firestore read warning:" $_.Exception.Message
}
# 6) Chat SSE smoke test (best-effort)
$CHAT_URL = (gcloud functions describe chat --gen2 --region=$REGION --format "value(url)" | Out-String).Trim()
Write-Host "CHAT_URL: $CHAT_URL"
if ($CHAT_URL -and ($CHAT_URL -match '^https?://')) {
  # Use a metadata-only question for a fast "done" event
  $body = @{ uid = $UID; sessionId = $SID; datasetId = $resp.datasetId; question = "How many rows?" } | ConvertTo-Json -Compress
  $payloadPath = Join-Path $env:TEMP "chat_payload.json"
  $sseOut = Join-Path $env:TEMP ("chat_sse_{0}.log" -f ([Guid]::NewGuid().ToString('N')))
  $body | Out-File -FilePath $payloadPath -Encoding utf8 -NoNewline
  try {
    & curl.exe -sN `
      -H "Origin: http://localhost:5173" `
      -H "Content-Type: application/json" `
      -H "X-User-Id: $UID" `
      -H "X-Session-Id: $SID" `
      --data-binary "@$payloadPath" `
      --max-time 60 `
      $CHAT_URL 2>&1 | Tee-Object -FilePath $sseOut | Out-Null

    # Try to parse the final done event and verify results in GCS
    try {
      $doneMatch = Select-String -Path $sseOut -Pattern 'data:\s*\{.*"type"\s*:\s*"done"' | Select-Object -Last 1
      if ($doneMatch) {
        $jsonLine = ($doneMatch.Line -replace '^data:\s*','')
        $evt = $jsonLine | ConvertFrom-Json
        $msgId = $evt.data.messageId
        $uris  = $evt.data.uris
        if ($msgId) {
          $resultsPrefix = "users/$UID/sessions/$SID/results/$msgId"
          Write-Host "Verifying analysis artifacts at gs://$BUCKET/$resultsPrefix/"
          gcloud storage ls "gs://$BUCKET/$resultsPrefix/**"
          # Verify HTTPS signed URLs exist and fetch 200
          if ($uris -and $uris.table -and ($uris.table -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.table -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: table.json signed URL fetchable" } catch { Write-Host "Warning: table.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS table URL in done event" }
          if ($uris -and $uris.metrics -and ($uris.metrics -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.metrics -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: metrics.json signed URL fetchable" } catch { Write-Host "Warning: metrics.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS metrics URL in done event" }
          if ($uris -and $uris.chartData -and ($uris.chartData -match '^https://')) {
            try { Invoke-WebRequest -Uri $uris.chartData -Method GET -TimeoutSec 10 | Out-Null; Write-Host "OK: chart_data.json signed URL fetchable" } catch { Write-Host "Warning: chart_data.json signed URL fetch failed: $($_.Exception.Message)" }
          } else { Write-Host "Warning: missing or non-HTTPS chartData URL in done event" }
        } else {
          Write-Host "SSE parse warning: messageId missing in done event"
        }
      } else {
        Write-Host "SSE parse warning: no done event found"
      }
    } catch {
      Write-Host "SSE parse warning:" $_.Exception.Message
    }
  } catch {
  } finally {
    if (Test-Path $payloadPath) { Remove-Item $payloadPath -Force }
    if (Test-Path $sseOut) { Remove-Item $sseOut -Force }
  }
} else {
  Write-Host "SSE test skipped: CHAT_URL invalid"
}

# =============================
# Optional: XLSX smoke test
# =============================
try {
  $XLSX_FILE = (Join-Path $ROOT_DIR "test_files\test.xlsx")
  if (-not (Test-Path $XLSX_FILE)) {
    $maybe = Get-ChildItem -Path (Join-Path $ROOT_DIR "test_files") -Filter *.xlsx -Recurse -ErrorAction SilentlyContinue | Select-Object -First 1
    if ($maybe) { $XLSX_FILE = $maybe.FullName } else { $XLSX_FILE = $null }
  }

  if ($XLSX_FILE -and (Test-Path $XLSX_FILE)) {
    Write-Host "Running XLSX smoke test with: $XLSX_FILE"
    $MIME2 = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    $SIZE2 = (Get-Item $XLSX_FILE).Length
    $FILENAME2 = [System.IO.Path]::GetFileName($XLSX_FILE)
    $encName2 = [System.Uri]::EscapeDataString($FILENAME2)
    $encMime2 = [System.Uri]::EscapeDataString($MIME2)
    $reqUri2 = ("{0}?filename={1}&size={2}&type={3}" -f $SIGN_URL, $encName2, $SIZE2, $encMime2)

    $headers2 = @{ "X-User-Id" = $UID; "X-Session-Id" = $SID }
    $resp2 = Invoke-RestMethod -Uri $reqUri2 -Headers $headers2 -Method GET
    if (-not $resp2.url) { throw "Signed URL missing for XLSX" }
    Invoke-WebRequest -Uri $resp2.url -Method PUT -InFile $XLSX_FILE -ContentType $MIME2 | Out-Null
    Write-Host "XLSX upload complete."
    Start-Sleep -Seconds 30
    gcloud storage ls "gs://$BUCKET/users/$UID/sessions/$SID/datasets/$($resp2.datasetId)/**"
  } else {
    Write-Host "XLSX smoke test skipped: no .xlsx file found in test_files."
  }
} catch {
  Write-Host "XLSX smoke test warning:" $_.Exception.Message
}
</file>

<file path="frontend/src/components/ChatInput.tsx">
import React, { useState, useRef, useEffect } from "react";
import { Button } from "./ui/button";
import { Textarea } from "./ui/textarea";
import { Paperclip, ArrowUp } from "lucide-react";

interface ChatInputProps {
  onSendMessage: (message: string) => void;
  onUploadFile?: (file: File) => void | Promise<void>;
  disabled?: boolean;
}

export function ChatInput({ onSendMessage, onUploadFile, disabled }: ChatInputProps) {
  const [message, setMessage] = useState("");
  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const handleSubmit = () => {
    if (message.trim() && !disabled) {
      onSendMessage(message.trim());
      setMessage("");
    }
  };

  const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      handleSubmit();
    }
  };

  // Auto-resize textarea
  useEffect(() => {
    const textarea = textareaRef.current;
    if (textarea) {
      textarea.style.height = "auto";
      textarea.style.height = `${Math.min(textarea.scrollHeight, 200)}px`;
    }
  }, [message]);

  return (
    <div className="fixed bottom-0 left-0 right-0 bg-gradient-to-t from-background via-background to-transparent pt-8 pb-4 z-30">
      <div className="max-w-3xl mx-auto px-4">
        <div className="group relative flex items-end gap-2 rounded-3xl px-4 py-3 border border-border bg-input-background/90 dark:bg-sidebar-accent/60 dark:border-sidebar-border shadow-lg text-foreground dark:text-sidebar-accent-foreground transition-all duration-300
        focus-within:-translate-y-1 focus-within:shadow-2xl focus-within:ring-[6px] focus-within:ring-white/50 dark:focus-within:ring-white/25
        focus-within:border-foreground/40 dark:focus-within:border-white/35 has-[textarea:not(:placeholder-shown)]:-translate-y-1 has-[textarea:not(:placeholder-shown)]:ring-[8px] has-[textarea:not(:placeholder-shown)]:ring-white/60 dark:has-[textarea:not(:placeholder-shown)]:ring-white/40">
          {/* File Upload Button */}
          <Button
            variant="ghost"
            size="icon"
            className="h-8 w-8 rounded-full flex-shrink-0 text-foreground dark:text-sidebar-accent-foreground"
            disabled={disabled}
            onClick={() => fileInputRef.current?.click()}
          >
            <Paperclip className="h-4 w-4 text-foreground dark:text-white" />
          </Button>

          {/* Hidden file input */}
          <input
            ref={fileInputRef}
            type="file"
            accept=".csv,.xlsx,.xls"
            className="hidden"
            onChange={async (e) => {
              const inputEl = e.currentTarget as HTMLInputElement;
              const f = inputEl.files?.[0];
              if (f && onUploadFile && !disabled) {
                try {
                  await onUploadFile(f);
                } finally {
                  // Reset input so selecting same file again still fires change
                  if (fileInputRef.current) {
                    fileInputRef.current.value = "";
                  } else {
                    inputEl.value = "";
                  }
                }
              }
            }}
          />

          {/* Textarea */}
          <Textarea
            ref={textareaRef}
            value={message}
            onChange={(e) => setMessage(e.target.value)}
            onKeyDown={handleKeyDown}
            placeholder="Ask anything"
            disabled={disabled}
            className="flex-1 min-h-[24px] max-h-[200px] resize-none border-0 bg-transparent focus-visible:outline-none focus-visible:ring-0 focus-visible:ring-offset-0 p-0 placeholder:text-muted-foreground group-focus-within:text-foreground"
            rows={1}
          />

          {/* Send Button */}
          {message.trim() && (
            <Button
              onClick={handleSubmit}
              disabled={disabled}
              size="icon"
              className="h-8 w-8 rounded-full bg-primary text-primary-foreground hover:bg-primary/90 flex-shrink-0"
            >
              <ArrowUp className="h-4 w-4" />
            </Button>
          )}
        </div>
        
        <p className="text-xs text-muted-foreground text-center mt-2">
          Upload CSV/Excel files, 20 MB file limit. Press Enter to send, Shift+Enter for new line.
        </p>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/lib/firebase.ts">
import { initializeApp, getApps } from "firebase/app";
import { getAuth } from "firebase/auth";
import { getFirestore } from "firebase/firestore";

// ✅ Read Firebase config from environment variables (.env.development / .env.production)
const firebaseConfig = {
  apiKey: import.meta.env.VITE_FIREBASE_API_KEY,
  authDomain: import.meta.env.VITE_FIREBASE_AUTH_DOMAIN,
  projectId: import.meta.env.VITE_FIREBASE_PROJECT_ID,
  storageBucket: import.meta.env.VITE_FIREBASE_STORAGE_BUCKET,
  messagingSenderId: import.meta.env.VITE_FIREBASE_MESSAGING_SENDER_ID,
  appId: import.meta.env.VITE_FIREBASE_APP_ID,
  measurementId: import.meta.env.VITE_FIREBASE_MEASUREMENT_ID,
};

// ✅ Initialize Firebase only once (prevents duplicate app errors during hot reloads)
const app = !getApps().length ? initializeApp(firebaseConfig) : getApps()[0];

export const auth = getAuth(app);
export const db = getFirestore(app);
export default app;
</file>

<file path="README.md">
# AI Data Analyst

AI Data Analyst is a serverless, event-driven pipeline that lets users upload data files directly to Google Cloud Storage and automatically preprocesses them into clean, analysis-ready artifacts. It also provides a chat interface that generates and executes Python analysis against the preprocessed data and streams results to the client.

## What this project does
- Issues secure, short-lived signed URLs for direct uploads from the client to GCS.
- Listens for object-finalize events and triggers a preprocessing service.
- Cleans, profiles, and converts uploaded data into standard artifacts (Parquet + JSON payload/report).
- Stores dataset metadata and status in Firestore for downstream consumption.

## Key components
- `backend/functions/sign_upload_url/` – Cloud Function (Gen2) that validates upload requests (Firebase ID token) and returns a V4 signed URL (PUT).
- `backend/functions/orchestrator/` – Cloud Function (Gen2) that orchestrates analysis and streams SSE events while generating code, executing it in a sandbox, and persisting artifacts.
- `backend/run-preprocess/` – Cloud Run service that receives Eventarc events and runs the preprocessing pipeline.
- Eventarc trigger – Routes GCS finalize events (on `ai-data-analyser-files`) to the Cloud Run `/eventarc` endpoint.
- Firestore – Stores dataset documents (status, URIs, summary) with TTL-based cleanup.
- GCS bucket `ai-data-analyser-files` – Stores raw uploads and generated artifacts.

## High-level workflow
```mermaid
flowchart TD
  A[Client/Frontend] -->|Request| B[Cloud Function: sign-upload-url]
  B -->|Signed URL + datasetId| A
  A -->|PUT file| C[GCS: ai-data-analyser-files]
  C -->|Object Finalized| D[Eventarc]
  D -->|HTTP CloudEvent| E[Cloud Run: preprocess-svc /eventarc]
  E -->|Artifacts| C
  E -->|Status update| F[Firestore]
  A -->|SSE chat| G[Cloud Function: chat]
  G -->|LLM code + sandbox run| C
  G -->|Persist results| C
  G -->|Stream 'done'| A
```

## Design choices
- Direct-to-GCS uploads using signed URLs avoid proxying large payloads through app servers.
- Eventarc decouples ingestion from processing and enables reliable, at-least-once delivery.
- No private keys: V4 URL signing relies on IAM-based impersonated credentials.
- Stateless compute: Cloud Run function loads, processes, and writes artifacts; data/state live in GCS/Firestore.
- TTL cleanup: Firestore documents have a TTL policy; add GCS lifecycle rules as needed.

## Repository layout
- `backend/` – All deployable backend components
  - `functions/` – Serverless functions (Gen2)
    - `sign_upload_url/` – Signed URL issuance
    - `orchestrator/` – SSE chat/orchestrator
  - `run-preprocess/` – Cloud Run preprocess service (FastAPI), pipeline, and requirements
  - `deploy-preprocess.ps1` – One-shot provisioning and deploy script (script-relative paths)
  - `test.ps1` – Local smoke test (upload + artifact/Firestore checks)
- `docs/` – API drafts and operational notes
 - `frontend/` – Vite + React app (Auth + Upload + Chat UI)
## Getting started (quick)
1. Ensure you’re on the correct project and have gcloud configured.
2. Deploy using scripts in `backend/` (preprocess + functions). For functions only, use `deploy-analysis.ps1`.
3. Optionally run `./backend/test.ps1` to upload a sample CSV and verify artifacts and Firestore status.

## Frontend

- Vite + React app under `frontend/` with:
  - Firebase Anonymous Auth (see `.env.example` and create `.env.development` for local dev).
  - File upload (paperclip) → `sign-upload-url` (signed PUT) → GCS → Eventarc → preprocess.
  - Chat UI that streams SSE from `chat` and posts an answer on `done`.

Dev setup:
```bash
cp frontend/.env.example frontend/.env.development
# Fill VITE_CHAT_URL, VITE_SIGN_URL, and Firebase web app config for local dev
npm install
npm run dev  # serves on http://localhost:5173
```

Important:
- CORS allowlist includes: `http://localhost:5173`, Firebase Hosting origins.
- Functions preflight allow lowercase headers (`authorization`, `content-type`, `x-session-id`).
- Frontend passes `sessionId` as a query param to avoid custom header preflight.

Production env guidance:
- The app defaults to relative endpoints `/api/sign-upload-url` and `/api/chat` in production via Firebase Hosting rewrites (see `frontend/firebase.json`).
- Do not set `VITE_CHAT_URL` or `VITE_SIGN_URL` when building for production; leaving them unset enables the `/api/*` fallback.
- For development, use `.env.development` to point to function URLs explicitly.

Chat persistence and UI:
- Frontend persists sessions/messages to Firestore under `users/{uid}/sessions/{sid}/messages/{mid}`.
- On startup, the app loads the last ~5 sessions into the sidebar and hydrates messages for the active one.
- SSE events produce a live assistant placeholder that updates status and finally renders:
  - summary (text),
  - a sample table (`tableSample`),
  - and a chart (`chartData`, bar/line) using Recharts.
  Renderers live under `frontend/src/components/renderers/`.

Sandbox hardening plan (Phase 3):
- Execute generated code in a hardened container (Cloud Run Job) with gVisor, CPU/memory limits, and no outbound network.
- Keep as a configurable toggle to allow iterative rollout; current orchestrator path remains functional.
- IAM and minimal scopes enforced; artifacts are short-lived, HTTPS-signed for client access.

## Status
- Preprocessing stage: functional.
- Chat (SSE) orchestrator: functional with robust fallbacks in the worker to ensure valid result payloads.
- See `PROGRESS.md` for the latest changes and operational notes.

## Known limitations
- If the LLM returns an empty `metrics` object intentionally, we currently persist it as `{}`; the worker only auto-fills metrics when the field is missing or wrong type. We may change this behavior to always include basic row/column counts.
- Charts are optional; if not requested/returned, `chart_data.json` may be minimal or empty.
</file>

<file path="backend/functions/orchestrator/main.py">
import json
import os
import time
import uuid
import subprocess
import sys
from datetime import datetime, timezone, timedelta
from typing import Generator, Iterable

import functions_framework
from flask import Request, Response
from google.cloud import firestore
from google.cloud import storage
import pandas as pd
import base64
from concurrent.futures import ThreadPoolExecutor
import pyarrow as pa  # type: ignore
import pyarrow.parquet as pq  # type: ignore

import firebase_admin
from firebase_admin import auth as fb_auth
import google.auth
from google.auth import impersonated_credentials
import google.auth.transport.requests

import gemini_client
import sandbox_runner
from google.api_core import exceptions as gax_exceptions  # type: ignore

# Optional: read payload from GCS later if needed
# from google.cloud import storage

PROJECT_ID = os.getenv("GCP_PROJECT", os.getenv("GOOGLE_CLOUD_PROJECT", "ai-data-analyser"))
FILES_BUCKET = os.getenv("FILES_BUCKET", "ai-data-analyser-files")
PING_INTERVAL_SECONDS = int(os.getenv("SSE_PING_INTERVAL_SECONDS", "22"))
HARD_TIMEOUT_SECONDS = int(os.getenv("CHAT_HARD_TIMEOUT_SECONDS", "60"))
ORCH_IPC_MODE = os.getenv("ORCH_IPC_MODE", "base64").lower()
RUNTIME_SERVICE_ACCOUNT = os.getenv("RUNTIME_SERVICE_ACCOUNT")

# Allowed origins (comma-separated). Default supports local dev and Firebase Hosting.
ALLOWED_ORIGINS = {
    o.strip()
    for o in (os.getenv(
        "ALLOWED_ORIGINS",
        "http://localhost:5173,https://ai-data-analyser.web.app,https://ai-data-analyser.firebaseapp.com",
    ) or "").split(",")
    if o.strip()
}


def _origin_allowed(origin: str | None) -> bool:
    if not origin:
        return False
    return origin in ALLOWED_ORIGINS


# Cache for signing credentials used for V4 signing
_CACHED_SIGNING_CREDS = None
_CACHED_EXPIRES_AT = 0.0


def _impersonated_signing_credentials(sa_email: str | None):
    """Create impersonated credentials for signing using IAM Credentials API.

    Requires that the runtime service account has roles/iam.serviceAccountTokenCreator
    on the target principal (can be itself). Also requires the
    iamcredentials.googleapis.com API to be enabled.
    """
    global _CACHED_SIGNING_CREDS, _CACHED_EXPIRES_AT

    now = time.time()
    if _CACHED_SIGNING_CREDS is not None and now < _CACHED_EXPIRES_AT:
        return _CACHED_SIGNING_CREDS

    if not sa_email:
        creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
        _CACHED_SIGNING_CREDS = creds
        _CACHED_EXPIRES_AT = now + 3300  # ~55m
        return _CACHED_SIGNING_CREDS

    source_creds, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    if getattr(source_creds, "token", None) is None:
        source_creds.refresh(google.auth.transport.requests.Request())

    _CACHED_SIGNING_CREDS = impersonated_credentials.Credentials(
        source_credentials=source_creds,
        target_principal=sa_email,
        target_scopes=["https://www.googleapis.com/auth/cloud-platform"],
        lifetime=3600,
    )
    _CACHED_EXPIRES_AT = now + 3300
    return _CACHED_SIGNING_CREDS


def _sign_gs_uri(gs_uri: str, minutes: int = 15) -> str:
    """Return a signed HTTPS URL for the given gs:// URI. If not gs://, return as-is."""
    if not gs_uri or not gs_uri.startswith("gs://"):
        return gs_uri
    no_scheme = gs_uri[5:]
    parts = no_scheme.split("/", 1)
    if len(parts) != 2:
        return gs_uri
    bucket_name, blob_path = parts[0], parts[1]
    storage_client = storage.Client(project=PROJECT_ID)
    blob = storage_client.bucket(bucket_name).blob(blob_path)
    signing_creds = _impersonated_signing_credentials(RUNTIME_SERVICE_ACCOUNT)
    return blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=minutes),
        method="GET",
        credentials=signing_creds,
    )


def _sign_uris(uris: dict | None) -> dict:
    d = uris or {}
    return {k: _sign_gs_uri(v) for k, v in d.items()}


# Initialize Firebase Admin SDK
try:
    firebase_admin.get_app()
except ValueError:
    firebase_admin.initialize_app()


def _sse_format(obj: dict) -> str:
    return f"data: {json.dumps(obj, ensure_ascii=False)}\n\n"


def _heartbeat() -> str:
    return _sse_format({"type": "ping", "ts": datetime.now(timezone.utc).isoformat()})


def _events(session_id: str, dataset_id: str, uid: str, question: str) -> Iterable[str]:
    # received
    yield _sse_format({"type": "received", "data": {"sessionId": session_id, "datasetId": dataset_id}})

    # validating (placeholder)
    yield _sse_format({"type": "validating"})

    # Prepare GCS paths
    dataset_prefix = f"users/{uid}/sessions/{session_id}/datasets/{dataset_id}"
    parquet_gcs = f"{dataset_prefix}/cleaned/cleaned.parquet"
    payload_gcs = f"{dataset_prefix}/metadata/payload.json"
    message_id = str(uuid.uuid4())

    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(FILES_BUCKET)

    # Attempt to fetch payload first (for sample/schema and potential metadata-only fast path)
    payload_obj: dict = {}
    try:
        payload_blob = bucket.blob(payload_gcs)
        if payload_blob.exists(storage_client):
            payload_text = payload_blob.download_as_text()
            payload_obj = json.loads(payload_text)
    except Exception:
        payload_obj = {}

    # Load schema/sample from payload.json if present (preferred)
    schema_snippet = ""
    sample_rows: list[dict] = []
    ctx: dict = {"limits": {"sampleRowsForDisplay": 50, "maxCharts": 0}}
    try:
        if isinstance(payload_obj, dict):
            # Our payload emits: columns (dict), dataset meta, and sample_rows (list)
            cols = payload_obj.get("columns")
            header_info = payload_obj.get("header_info") or {}
            hints = payload_obj.get("analysis_hints") or {}
            dataset_summary = payload_obj.get("dataset_summary") or ""

            # Build compact context lines if v2 present
            context_lines: list[str] = []
            if header_info or hints or dataset_summary:
                hrow = header_info.get("header_row_index")
                conf = header_info.get("confidence")
                method = header_info.get("method")
                final_headers = header_info.get("final_headers") or []
                hdr_preview = final_headers[:4] if isinstance(final_headers, list) else []
                first_col_type = hints.get("first_column_type")
                likely_pivoted = hints.get("likely_pivoted")
                context_lines.append("Dataset context:")
                parts = []
                if hrow is not None and conf is not None:
                    parts.append(f"- Detected header row: {hrow} (confidence {float(conf):.2f}{', ' + method if method else ''})")
                if hdr_preview:
                    parts.append(f"- Headers: {hdr_preview}")
                if first_col_type is not None or likely_pivoted is not None:
                    parts.append(f"- Likely structure: first column = {first_col_type}, likely_pivoted={bool(likely_pivoted)}")
                if dataset_summary:
                    parts.append(f"- Summary: {dataset_summary}")
                context_lines.extend(parts)

            # Columns preview (limit 3–4 entries for readability)
            cols_preview_json = ""
            if isinstance(cols, dict):
                # take first 4 items
                items = list(cols.items())[:4]
                cols_preview_json = json.dumps({k: v for k, v in items})
            elif isinstance(cols, list):
                cols_preview_json = json.dumps(cols[:4])

            # Assemble schema_snippet
            if context_lines or cols_preview_json:
                schema_snippet = ("\n".join(context_lines) + ("\n" if context_lines else "") + (cols_preview_json or ""))[:1000]

            sr = payload_obj.get("sample_rows")
            if isinstance(sr, list):
                sample_rows = sr[:10]

            # Build ctx from payload
            dataset_meta = payload_obj.get("dataset") or {}
            final_headers = header_info.get("final_headers") or []
            dtypes = dataset_meta.get("dtypes") or {}
            ctx["dataset"] = {
                "rows": int(dataset_meta.get("rows") or 0),
                "columns": int(dataset_meta.get("columns") or 0),
                "column_names": final_headers if isinstance(final_headers, list) and final_headers else list(dtypes.keys()),
                "dtypes": dtypes if isinstance(dtypes, dict) else {},
            }
            # Map numeric/temporal indices to names if possible
            def _map_indices(indices: list[int]) -> list[str]:
                try:
                    if isinstance(indices, list) and final_headers:
                        return [str(final_headers[i]) for i in indices if 0 <= i < len(final_headers)]
                except Exception:
                    return []
                return []
            numeric_names = _map_indices(hints.get("numeric_columns") or [])
            temporal_names = _map_indices(hints.get("temporal_columns") or [])
            ctx["hints"] = {"numeric": numeric_names, "temporal": temporal_names}
    except Exception:
        sample_rows = []

    # Metadata-only fast path: answer simple schema/rows questions without loading parquet
    def _is_metadata_only(q: str) -> bool:
        ql = (q or "").lower()
        tokens = [
            "column", "columns", "schema", "datatype", "data type", "types",
            "row count", "rows", "how many rows", "number of rows", "num rows",
        ]
        return any(t in ql for t in tokens)

    if _is_metadata_only(question) and payload_obj:
        # Build lightweight artifacts from payload alone
        dataset_meta = (payload_obj.get("dataset") or {}) if isinstance(payload_obj, dict) else {}
        total_rows = int(dataset_meta.get("rows") or 0)
        total_cols = int(dataset_meta.get("columns") or 0)
        table = sample_rows or []
        metrics = {"rows": total_rows, "columns": total_cols}
        # Do not render a default chart in metadata-only answers
        chart_data = {}

        # Persist and done (no parquet or worker)
        yield _sse_format({"type": "persisting"})
        results_prefix = f"users/{uid}/sessions/{session_id}/results/{message_id}"
        uris_gs = {
            "table": f"gs://{FILES_BUCKET}/{results_prefix}/table.json",
            "metrics": f"gs://{FILES_BUCKET}/{results_prefix}/metrics.json",
            "chartData": f"gs://{FILES_BUCKET}/{results_prefix}/chart_data.json",
            "summary": f"gs://{FILES_BUCKET}/{results_prefix}/summary.json",
        }
        try:
            with ThreadPoolExecutor(max_workers=4) as ex:
                futs = []
                futs.append(ex.submit(bucket.blob(f"{results_prefix}/table.json").upload_from_string,
                                     json.dumps(table, ensure_ascii=False), content_type="application/json"))
                futs.append(ex.submit(bucket.blob(f"{results_prefix}/metrics.json").upload_from_string,
                                     json.dumps(metrics, ensure_ascii=False), content_type="application/json"))
                futs.append(ex.submit(bucket.blob(f"{results_prefix}/chart_data.json").upload_from_string,
                                     json.dumps(chart_data, ensure_ascii=False), content_type="application/json"))
                futs.append(ex.submit(bucket.blob(f"{results_prefix}/summary.json").upload_from_string,
                                     json.dumps({"summary": "Dataset metadata provided."}, ensure_ascii=False), content_type="application/json"))
                for f in futs:
                    f.result()
        except Exception as e:  # noqa: BLE001
            yield _sse_format({"type": "error", "data": {"code": "PERSIST_FAILED", "message": str(e)[:400]}})
            return

        # Sign artifact URIs for browser access
        uris = _sign_uris(uris_gs)

        fs = firestore.Client(project=PROJECT_ID)
        fs.collection("users").document(uid).collection("sessions").document(session_id).collection("messages").document(message_id).set({
            "role": "assistant",
            "question": question,
            "content": "Dataset metadata provided.",
            "createdAt": datetime.now(timezone.utc),
            "status": "done",
            "uris": uris,
        })

        yield _sse_format({
            "type": "done",
            "data": {
                "messageId": message_id,
                "chartData": chart_data,
                "tableSample": table[: min(len(table), 50)],
                "uris": uris,
                "urisGs": uris_gs,
                "summary": "Dataset metadata provided.",
            },
        })
        return

    # generating_code via Gemini (fused pre-run: code + summary)
    yield _sse_format({"type": "generating_code"})
    try:
        code, pre_summary = gemini_client.generate_code_and_summary(question, schema_snippet, sample_rows)
    except Exception as e:  # noqa: BLE001
        yield _sse_format({"type": "error", "data": {"code": "CODEGEN_FAILED", "message": str(e)[:400]}})
        return

    # Validate code (AST)
    ok, errs = sandbox_runner.validate_code(code)
    if not ok:
        yield _sse_format({"type": "error", "data": {"code": "CODE_VALIDATION_FAILED", "message": "; ".join(errs)[:400]}})
        return

    # Prepare data transfer for child process
    parquet_b64: str | None = None
    arrow_ipc_b64: str | None = None
    parquet_path: str | None = None
    # Download parquet as needed for analysis
    try:
        blob = bucket.blob(parquet_gcs)
        if not blob.exists(storage_client):
            yield _sse_format({"type": "error", "data": {"code": "MISSING_PARQUET", "message": parquet_gcs}})
            return
        if ORCH_IPC_MODE == "base64":
            parquet_bytes = blob.download_as_bytes()
            parquet_b64 = base64.b64encode(parquet_bytes).decode("ascii")
        elif ORCH_IPC_MODE == "arrow":
            # Download parquet bytes → read as Arrow Table → serialize IPC stream
            parquet_bytes = blob.download_as_bytes()
            table_arrow = pq.read_table(pa.BufferReader(parquet_bytes))
            sink = pa.BufferOutputStream()
            with pa.ipc.new_stream(sink, table_arrow.schema) as writer:
                writer.write_table(table_arrow)
            arrow_ipc_b64 = base64.b64encode(sink.getvalue().to_pybytes()).decode("ascii")
        else:
            parquet_path = f"/tmp/{dataset_id}_cleaned.parquet"
            blob.download_to_filename(parquet_path)
    except Exception as e:  # noqa: BLE001
        yield _sse_format({"type": "error", "data": {"code": "DOWNLOAD_FAILED", "message": str(e)[:300]}})
        return

    # Execute in child process
    yield _sse_format({"type": "running_fast"})
    worker_path = os.path.join(os.path.dirname(__file__), "worker.py")
    try:
        # Attach question into ctx for worker-side heuristics
        try:
            ctx["question"] = question
        except Exception:
            pass

        proc = subprocess.run(
            [os.environ.get("PYTHON_EXECUTABLE", sys.executable), worker_path],
            input=json.dumps({
                "code": code,
                "parquet_b64": parquet_b64,
                "arrow_ipc_b64": arrow_ipc_b64,
                "parquet_path": parquet_path,
                "ctx": ctx,
            }).encode("utf-8"),
            capture_output=True,
            timeout=HARD_TIMEOUT_SECONDS,
        )
    except subprocess.TimeoutExpired:
        yield _sse_format({"type": "error", "data": {"code": "TIMEOUT_HARD", "message": "Operation exceeded 60s"}})
        return

    if proc.returncode != 0:
        err = proc.stderr.decode("utf-8", errors="ignore")[:500]
        yield _sse_format({"type": "error", "data": {"code": "EXEC_FAILED", "message": err}})
        return

    try:
        result = json.loads(proc.stdout.decode("utf-8"))
    except json.JSONDecodeError as e:  # type: ignore[attr-defined]
        yield _sse_format({"type": "error", "data": {"code": "BAD_RESULT_JSON", "message": str(e)[:300]}})
        return
    except Exception as e:  # noqa: BLE001
        yield _sse_format({"type": "error", "data": {"code": "BAD_RESULT", "message": str(e)[:300]}})
        return

    table = result.get("table") or []
    metrics = result.get("metrics") or {}
    chart_data = result.get("chartData") or {}
    # Final validation/coercion before persist
    if not isinstance(table, list):
        table = []
    if not isinstance(metrics, dict):
        metrics = {}
    if not isinstance(chart_data, dict):
        # Do not synthesize a default chart; only show charts when explicitly requested
        chart_data = {}
    table_sample = table[: min(len(table), 50)]

    # summarizing (generate summary from actual results)
    yield _sse_format({"type": "summarizing"})
    try:
        summary = gemini_client.generate_summary(question, table_sample, metrics, code=code)
    except Exception as e:
        print(f"WARNING: Summary generation failed: {e}", file=sys.stderr)
        # Fallback to the pre-run summary if post-run generation fails, else neutral message
        summary = pre_summary or "No textual summary available. See the table for details."

    # persist
    yield _sse_format({"type": "persisting"})
    results_prefix = f"users/{uid}/sessions/{session_id}/results/{message_id}"
    uris_gs = {
        "table": f"gs://{FILES_BUCKET}/{results_prefix}/table.json",
        "metrics": f"gs://{FILES_BUCKET}/{results_prefix}/metrics.json",
        "chartData": f"gs://{FILES_BUCKET}/{results_prefix}/chart_data.json",
        "summary": f"gs://{FILES_BUCKET}/{results_prefix}/summary.json",
    }
    try:
        with ThreadPoolExecutor(max_workers=4) as ex:
            futs = []
            futs.append(ex.submit(bucket.blob(f"{results_prefix}/table.json").upload_from_string,
                                 json.dumps(table, ensure_ascii=False), content_type="application/json"))
            futs.append(ex.submit(bucket.blob(f"{results_prefix}/metrics.json").upload_from_string,
                                 json.dumps(metrics, ensure_ascii=False), content_type="application/json"))
            futs.append(ex.submit(bucket.blob(f"{results_prefix}/chart_data.json").upload_from_string,
                                 json.dumps(chart_data, ensure_ascii=False), content_type="application/json"))
            futs.append(ex.submit(bucket.blob(f"{results_prefix}/summary.json").upload_from_string,
                                 json.dumps({"summary": summary}, ensure_ascii=False), content_type="application/json"))
            for f in futs:
                f.result()
    except gax_exceptions.GoogleAPICallError as e:  # type: ignore[attr-defined]
        yield _sse_format({"type": "error", "data": {"code": "PERSIST_API_ERROR", "message": str(e)[:400]}})
        return
    except Exception as e:  # noqa: BLE001
        yield _sse_format({"type": "error", "data": {"code": "PERSIST_FAILED", "message": str(e)[:400]}})
        return

    # Firestore message doc
    try:
        fs = firestore.Client(project=PROJECT_ID)
        doc_path = f"users/{uid}/sessions/{session_id}/messages/{message_id}"
        print(f"DEBUG: Preparing to write to Firestore path: {doc_path}")

        # Check for empty or None values in the path
        if not all([uid, session_id, message_id]):
            print(f"CRITICAL ERROR: One or more IDs are empty. UID='{uid}', SessionID='{session_id}', MessageID='{message_id}'. Aborting Firestore write.", file=sys.stderr)
            yield _sse_format({"type": "error", "data": {"code": "FIRESTORE_PATH_INVALID", "message": "UID, session, or message ID was empty."}})
            return

        msg_data = {
            "uid": uid,
            "sessionId": session_id,
            "datasetId": dataset_id,
            "question": question,
            "summary": summary,
            "tableSample": table_sample,
            "chartData": chart_data,
            "uris": uris_gs,  # Storing the gs:// URIs in Firestore
            "createdAt": firestore.SERVER_TIMESTAMP,
            "role": "assistant",
            "status": "done",
        }
        
        print(f"DEBUG: Data for Firestore: {json.dumps(msg_data, indent=2, default=str)}")

        msg_ref = fs.document(doc_path)
        msg_ref.set(msg_data)
        
        print("DEBUG: Firestore write successful.")

    except Exception as e:
        print(f"CRITICAL ERROR during Firestore write: {e}", file=sys.stderr)
        yield _sse_format({"type": "error", "data": {"code": "FIRESTORE_WRITE_FAILED", "message": str(e)}})
        return # Stop execution if Firestore fails
    
    # Sign artifact URIs for browser access (general path)
    print("DEBUG: Signing URIs for SSE response...")
    uris = _sign_uris(uris_gs)

    # done
    yield _sse_format({
        "type": "done",
        "data": {
            "messageId": message_id,
            "chartData": chart_data,
            "tableSample": table_sample,
            "uris": uris,
            "urisGs": uris_gs,
            "summary": summary,
        },
    })
    # Close stream after final event (Option B)
    return


@functions_framework.http
def chat(request: Request) -> Response:
    origin = request.headers.get("Origin") or ""
    if request.method == "OPTIONS":
        if not _origin_allowed(origin):
            return ("Origin not allowed", 403, {"Content-Type": "text/plain"})
        headers = {
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            # Include lowercase variants to match Access-Control-Request-Headers from browsers
            "Access-Control-Allow-Headers": "Content-Type, content-type, Authorization, authorization, X-Session-Id, x-session-id",
            "Access-Control-Max-Age": "3600",
            "Cache-Control": "no-store",
        }
        return ("", 204, headers)

    try:
        if not _origin_allowed(origin):
            return (json.dumps({"error": "origin not allowed"}), 403, {"Content-Type": "application/json"})

        # Verify Firebase ID token
        auth_header = request.headers.get("Authorization", "")
        token = auth_header.split(" ", 1)[1] if auth_header.lower().startswith("bearer ") else None
        if not token:
            return (json.dumps({"error": "missing Authorization Bearer token"}), 401, {"Content-Type": "application/json"})
        try:
            decoded = fb_auth.verify_id_token(token)
            uid = decoded.get("uid")
        except Exception as e:  # noqa: BLE001
            return (json.dumps({"error": "invalid token", "detail": str(e)[:200]}), 401, {"Content-Type": "application/json"})

        payload = request.get_json(silent=True) or {}
        session_id = payload.get("sessionId") or request.headers.get("X-Session-Id")
        dataset_id = payload.get("datasetId")
        question = payload.get("question") or ""
        if not all([session_id, dataset_id, uid]):
            return (json.dumps({"error": "missing sessionId or datasetId"}), 400, {"Content-Type": "application/json"})

        headers = {
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": origin,
        }
        return Response(_events(session_id, dataset_id, uid, question), headers=headers, status=200)
    except Exception as e:  # noqa: BLE001
        return (json.dumps({"error": "internal error", "detail": str(e)[:500]}), 500, {"Content-Type": "application/json"})
</file>

<file path="PROGRESS.md">
# AI Data Analyst â€“ Progress Report

Date: 2025-10-03 01:47 (+02:00)

## Current Status
- **Preprocessing is fully functional**. Upload via signed URL triggers preprocessing; artifacts are generated and Firestore status advances to `ready`.
- **Frontend integrated (Auth + Upload + Chat SSE)**. Firebase Anonymous Auth, upload via signed URL, and chat SSE orchestrator are working end-to-end. Artifacts (`table.json`, `metrics.json`, `chart_data.json`, `summary.json`) are produced and signed for browser access.
- **CORS aligned for dev**. Dev runs on `http://localhost:5173`; functions allow Origin and lowercase headers in preflight. Frontend passes `sessionId` as a query param to avoid custom header preflight.
- **End-to-end smoke tests pass** using `test.ps1`.

## Deployed Components
- **Cloud Run service: `preprocess-svc`**
  - Framework: `FastAPI` in `backend/run-preprocess/main.py`.
  - Endpoints:
    - `GET /healthz` â€“ lightweight health endpoint (service is private; unauthenticated calls may 403/404).
    - `POST /eventarc` â€“ Eventarc target; parses CloudEvents in multiple delivery shapes (structured, binary, Pub/Sub push, and GCS notification compatibility).
  - Responsibilities:
    - Download raw file from GCS under `users/{uid}/sessions/{sid}/datasets/{datasetId}/raw/input.{csv|xlsx}`.
    - Run pipeline `backend/run-preprocess/pipeline_adapter.py` to clean and profile data.
    - Write artifacts:
      - `cleaned/cleaned.parquet`
      - `metadata/payload.json`
      - `reports/cleaning_report.json`
    - Update Firestore dataset document with `status: ready`, rows, columns, and artifact URIs.
  - Observability: integrates Google Cloud Logging (`cloud-logging: configured`).

- **Cloud Functions (Gen2): `sign-upload-url`**
  - Path: `backend/functions/sign_upload_url/main.py`.
  - Functionality: issues a V4 signed URL for direct browser PUT upload.
  - Security: uses IAM-based signing via impersonated credentials (no private key).
  - Behavior: creates initial Firestore dataset doc with `status: awaiting_upload` and `ttlAt`.

- **Cloud Functions (Gen2): `chat` (orchestrator, SSE)**
  - Path: `backend/functions/orchestrator/` (deployed and available; orchestration logic outside the scope of this stage).

- **Eventarc Trigger: `preprocess-trigger`**
  - Filters: `type=google.cloud.storage.object.v1.finalized`, `bucket=ai-data-analyser-files`.
  - Destination: Cloud Run `preprocess-svc` path `/eventarc` (region `europe-west4`).
  - Transport: Pub/Sub (managed subscription/topic).

- **Google Cloud Storage (GCS)**
  - Bucket: `ai-data-analyser-files`.
  - Structure per dataset:
    - `raw/input.csv` (or `.xlsx`)
    - `cleaned/cleaned.parquet`
    - `metadata/payload.json`
    - `reports/cleaning_report.json`

- **Firestore (Native mode)**
  - Collection path: `users/{uid}/sessions/{sid}/datasets/{datasetId}`.
  - Fields (subset): `status`, `rawUri`, `cleanedUri`, `payloadUri`, `reportUri`, `rows`, `columns`, `updatedAt`, `ttlAt`.
  - TTL Policy: Enabled on collection group `datasets` for field `ttlAt` (state: ACTIVE).

## Architecture & Design
- **Design principles**
  - Keep compute stateless and ephemeral; persist state/artifacts in Firestore + GCS.
  - Use direct-to-GCS uploads via signed URLs; minimize function runtime and egress.
  - Eliminate private keys; prefer IAM-based signing and workload identity.
  - Event-driven preprocessing via Eventarc to decouple upload from processing.

- **High-level flow**
```mermaid
flowchart TD
  A[Client] -->|GET sign-upload-url| F[Cloud Function: sign-upload-url]
  F -->|V4 signed URL + datasetId| A
  A -->|PUT file via Signed URL| B[GCS: ai-data-analyser-files]
  B -->|Object Finalized| E[Eventarc]
  E -->|HTTP CloudEvent| R[Cloud Run: preprocess-svc /eventarc]
  R -->|Clean + Profile| GCSArtifacts[(Artifacts in GCS)]
  R -->|Update| FS[(Firestore datasets)]
  GCSArtifacts -.->|cleaned.parquet| B
  GCSArtifacts -.->|payload.json, report.json| B
```

- **Security & IAM**
  - Runtime Service Account: `${PROJECT_NUMBER}-compute@developer.gserviceaccount.com`.
  - Roles:
    - `roles/datastore.user` for Firestore access.
    - Bucket-scoped `roles/storage.objectAdmin` on `ai-data-analyser-files`.
    - `roles/eventarc.eventReceiver` for Eventarc delivery.
    - `roles/iam.serviceAccountTokenCreator` (self) for IAM-based signing.
    - GCS service account granted `roles/pubsub.publisher` for CloudEvents â†’ Pub/Sub.
  - Cloud Run service is private; HTTP access requires identity.

  - Script: `backend/deploy.ps1` handles:
    - Enabling APIs.
    - Deploying Cloud Run `preprocess-svc` with buildpacks (Python 3.12).
    - Setting env vars: `FILES_BUCKET`, `GCP_PROJECT`, `TTL_DAYS`.
    - Creating/Updating Eventarc trigger.
  - Deploying Cloud Functions `sign-upload-url` and `chat`.
  - Printing service URLs and running a smoke test.

## Verification
- **Smoke test**: `test.ps1`
  - Health probe (best-effort; service is private so 404/403 is expected).
  - Requests signed URL, uploads sample CSV, waits 30s, lists artifacts, and prints Firestore `status`.

- **Logs**
  - Cloud Run: use `gcloud logging read 'resource.type="cloud_run_revision" AND resource.labels.service_name="preprocess-svc"' --limit=100 --freshness=1h`.
  - Indicators of successful processing: `preprocess_complete` log entry and 204 response to `/eventarc` after processing.

## Operational Notes
- **Known non-blocker**: `/healthz` unauthenticated requests return 404/403 because the service is private. This does not affect Eventarc-triggered processing.
- **Resource tuning** (optional): set Cloud Run `--concurrency=1` and increase `--memory` (e.g., 1Gi) if needed for heavy files.
- **Idempotency** (optional): skip reprocessing if `cleaned/` already exists.
- **Bucket lifecycle** (optional): add object TTL for `users/` prefix to match Firestore TTL.

## Recent Changes (Changelog)
- **2025-10-06 (Footer meta formatting + dynamic table width)**
  - Frontend
    - ChatMessage (frontend/src/components/ChatMessage.tsx): Footer chip now shows "N rows X M columns"; table wrapper uses w-fit so narrow tables avoid horizontal scroll.
    - TableRenderer (frontend/src/components/renderers/TableRenderer.tsx): Container switched to inline-block with max-w-full and overflow-auto; table uses min-w-max so content defines width up to chat container, then horizontal scrollbar appears only when needed.
    - App (frontend/src/App.tsx): Stable upload message id stored to update the same message meta when rows/columns arrive via Firestore.
  - Timestamp
    - 2025-10-06 16:05 (+02:00)
- **2025-10-06 (Message footer metadata)**
  - Frontend
    - ChatMessage (frontend/src/components/ChatMessage.tsx): Added footer metadata chips next to the timestamp for text messages (file name, file size, rows  columns when available).
    - App (frontend/src/App.tsx): Upload message now sets meta with file name/size; when preprocessing finishes, the same message is updated in-place with rows  columns via Firestore subscription.
  - Timestamp
    - 2025-10-06 15:15 (+02:00)
- **2025-10-06 (UI polish follow-up)**
  - Frontend
    - ChatMessage (frontend/src/components/ChatMessage.tsx): Wrap table messages in an overflow container to keep within chat width and provide internal horizontal/vertical scrollbars.
    - TableRenderer (frontend/src/components/renderers/TableRenderer.tsx): Use a scrollable container with min-w-max table to avoid spillover while allowing horizontal scroll; keeps sticky header and sorting.
  - Notes
    - Planning UI for dataset info presentation (file name/size and rowscolumns) as a metadata row under the message bubble or a subtle banner; pending selection before implementation.
  - Timestamp
    - 2025-10-06 15:06 (+02:00)
- **2025-10-06 (UI/UX tweaks)**
  - Frontend
    - Chat UI (frontend/src/App.tsx, frontend/src/components/ChatMessage.tsx): Removed separate thinking bubble; Cancel button now appears inside the assistant status message. Message timestamps now include seconds (hh:mm:ss).
    - TableRenderer (frontend/src/components/renderers/TableRenderer.tsx): Added subtle row dividers and hover highlight; implemented client-side sorting per column with icons.
    - Upload notice (frontend/src/App.tsx): After upload, assistant message shows file name and size; when preprocessing completes, a follow-up message announces rows  columns via a Firestore snapshot listener.
  - Backend
    - Summary length (backend/functions/orchestrator/gemini_client.py): Increased max_output_tokens for summary generation from 256 to 1024 to reduce truncation.
  - Timestamp
    - 2025-10-06 14:19 (+02:00)
- **2025-10-06**
  - Backend
    - Orchestrator worker (ackend/functions/orchestrator/worker.py): Removed default chart generation. chartData is returned only when the user's question explicitly asks for a chart (tokens: chart/plot/graph/visualization). If not requested, any user-code chart is coerced to {}. Result sanitation retained.
    - Orchestrator main (ackend/functions/orchestrator/main.py): Metadata-only path now emits no default chart (chart_data = {}). When result chartData is invalid, it is set to {} (no synthesized chart). Summary fallback on exception changed to a neutral, data-focused sentence.
    - Gemini client (ackend/functions/orchestrator/gemini_client.py): When LLM returns empty text, synthesize a concise, data-driven fallback using metrics (rows/columns) and a preview of column names from 	able_head.
  - Frontend
    - App (rontend/src/App.tsx): Append a chart bubble only when chartData has real content (labels > 0 and at least one numeric point in any series).
    - TableRenderer (rontend/src/components/renderers/TableRenderer.tsx): Added vertical and horizontal scroll with max-h-[420px], sticky header, and cell overflow ellipsis while keeping whitespace-nowrap.
  - Deferred
    - Daily usage limit wiring deferred until Firebase Auth integration; no limit code changes in this iteration.
  - Timestamp
    - 2025-10-06 12:58 (+02:00)
- **2025-10-05**
  - Preprocess (v2 metadata, simple & pragmatic)
    - Implemented simplified header detection with 4 signals and year-like tie-breaker; normalized confidence 0..1. Lookahead via `PREPROCESS_HEADER_LOOKAHEAD`.
    - Added manual header-row override file `metadata/preprocess_overrides.json` (per dataset) and plumbed through service; optional env `PREPROCESS_HEADER_ROW_OVERRIDE` supported internally.
    - Added v2 additive fields in `payload.json`: `schema_version: "2.0"`, `header_info` (minimal), `analysis_hints` (consolidated hints), and `dataset_summary`.
    - Kept v1 fields intact (`dataset`, `columns`, `sample_rows`, `cleaning_report`, `mode`, `version`).
  - Orchestrator
    - Prepends a compact "Dataset context" (header row + confidence, headers preview, likely structure, summary) to the schema snippet for Gemini prompts; caps schema preview to 3â€“4 columns.
  - Observability
    - Emits `header_detection` structured log with `header_row_index`, `confidence`, `method`, `is_transposed`, `lookahead`, and `low_confidence` flag to support a log-based KPI.
  - Local dev compatibility (Python 3.13)
    - Updated `backend/run-preprocess/requirements.txt` with environment markers to allow newer wheels on Python 3.13 while preserving deployment pins for 3.12.
    - Hardened Polars numeric parsing: remove parentheses via `str.replace_all`, then cast to `Float64`; fixed percent/K/M/B handling.
    - Header detection now fills NAs pre-cast and ignores empty tokens; requires at least 2 non-empty cells to avoid selecting title-only rows.
  - Tests & fixtures
    - Added fixtures under `backend/run-preprocess/tests/fixtures/`: `balance_sheet.csv`, `multi_row_header.csv`, `title_rows.csv`.
    - Added unit tests: `test_header_detection_simple.py` (positive/negative cases).
    - Added integration test: `test_integration_polars_balance_sheet.py` (CSV via Polars end-to-end).
    - Current result: 6 tests total, all passing locally on Python 3.13 (1 benign datetime parsing warning).
  - Docs
    - Added `backend/docs/payload_v1_vs_v2.md` (concise JSON examples) and annotated `backend/docs/payload_schema.json` with a note that v2 fields are additive and not validated by the v1 schema.

- **2025-10-03**
  - Frontend
    - Integrated AuthContext with Firebase Anonymous Auth; `.env.example` added; dev server port set to `5173`.
    - File upload wired to `sign-upload-url`; removed `X-Session-Id` header in favor of `sessionId` query param.
    - Chat SSE integrated; added robust spinner stop in `App.tsx` `finally` after stream ends; fixed file input reset in `ChatInput.tsx`.
  - Backend
    - CORS preflight updated to include lowercase `authorization`, `content-type`, `x-session-id` in both functions.
    - Orchestrator worker now coerces non-dict returns (DataFrame/list) and fills missing keys to avoid `run() must return a dict` surfacing to users.
  - Deploy
    - `deploy-analysis.ps1` now uses `--env-vars-file` with YAML files (`env.sign-upload-url.yaml`, `env.chat.yaml`) to reliably pass `ALLOWED_ORIGINS` on Windows PowerShell.
    - Printed function URLs unchanged; redeploy confirmed.

- **2025-10-03 (later)**
  - Frontend (Phase 1)
    - Implemented assistant placeholder message pushed immediately upon send with `kind: "status"` and live status updates mapped from SSE events (`validating`, `generating_code`, `running_fast`, `summarizing`, `persisting`).
    - Upgraded message model to a discriminated union (`text|status|error|table|chart`).
    - Added `TableRenderer.tsx` and `ChartRenderer.tsx` (Recharts) and updated `ChatMessage.tsx` to dispatch per kind.
    - On `done`, convert placeholder to summary text and append separate table and chart bubbles when present.
  - Backend (Phase 1)
    - Fixed artifact URL signing in `functions/orchestrator/main.py` general path: now signs `uris_gs` â†’ `uris` before Firestore write and SSE `done`.
  - Hosting/Deploy (Phase 1)
    - Added Firebase Hosting rewrites for `/api/sign-upload-url` and `/api/chat` to Functions Gen2 in `europe-west4`.
    - Parameterized `--allow-unauthenticated` in `backend/deploy-analysis.ps1` via `ALLOW_UNAUTHENTICATED` env var (default off) for production auth at the edge via Hosting.

- **2025-10-03 (Phase 2)**
  - Frontend
    - Added Cancel button wired to `AbortController`; converts assistant placeholder to `Cancelled.` and resets typing.
    - Fixed JSX mismatches in `App.tsx` introduced during edits.

- **2025-10-03 (Phase 3)**
  - Backend
    - Refined exception handling in `functions/orchestrator/main.py`: specific `json.JSONDecodeError` and `GoogleAPICallError` handling with clearer error codes.
    - Added final result validation/coercion before persistence (ensure `summary` non-empty fallback; shape guards for `table`, `metrics`, `chartData`).
  - Ops
    - Added GCS lifecycle rule: delete objects under `users/` prefix after 1 day via `deploy-preprocess.ps1`.

- **2025-10-03 (Phase 4)**
  - Frontend
    - Implemented Firestore chat persistence helpers in `frontend/src/services/firestore.ts` (ensure session, update dataset, save user messages, load last ~5 sessions with messages).
    - Integrated persistence into `App.tsx` (create session on new chat/ensure, update datasetId after upload, save user messages, hydrate recent sessions on auth ready).
    - Added `frontend/.env.example`; development now recommends `.env.development`. Production builds default to `/api/*` endpoints via Hosting rewrites when env vars are unset.
  - Tests
    - Enhanced `backend/test.ps1`: SSE smoke test now checks that `done` contains HTTPS signed URLs (`uris.*`) and attempts to fetch them.
- **2025-10-01**
  - Backend Performance â€“ Step 1 implemented.
    - Orchestrator (`backend/functions/orchestrator/main.py`)
      - Switched to base64 IPC: downloads `cleaned.parquet` as bytes and passes to worker over stdin JSON (`parquet_b64`). Fallback to file path via `ORCH_IPC_MODE=filepath`.
      - Uses `payload.json` keys (`sample_rows`, `columns`) for LLM context; avoids parquet `head()` when payload exists.
      - Added metadata-only fast path: schema/columns/row-count questions are answered from payload without loading parquet.
      - Parallelized GCS result uploads (table/metrics/chart_data/summary) with `ThreadPoolExecutor`.
      - Fused Gemini call: `generate_code_and_summary()` returns code + short summary in one request.
    - Worker (`backend/functions/orchestrator/worker.py`)
      - Accepts `parquet_b64` (base64) and decodes into an in-memory DataFrame; retains `parquet_path` fallback.
    - Gemini client (`backend/functions/orchestrator/gemini_client.py`)
      - Added `generate_code_and_summary()` with structured parsing; gated by `GEMINI_FUSED` env (default on).
    - Preprocess service (`backend/run-preprocess/main.py`)
      - Full in-memory I/O: `download_as_bytes()` in, `upload_from_file(BytesIO)` out; no `/tmp` in the happy path.
      - Parallelized uploads of cleaned parquet, `payload.json`, and `cleaning_report.json`.
    - Pipeline adapter (`backend/run-preprocess/pipeline_adapter.py`)
      - Added `process_bytes_to_artifacts(data, kind, ...)` to mirror file-based API for in-memory processing.
    - Deployment scripts
      - `backend/deploy-preprocess.ps1`: set `--cpu=2 --memory=2Gi --concurrency=10` and `PREPROCESS_ENGINE=polars` (engine switch to be implemented in Step 2).
      - `backend/deploy-analysis.ps1`: set `ORCH_IPC_MODE=base64` and `GEMINI_FUSED=1` for `chat` function.
    - Flags and rollback
      - `ORCH_IPC_MODE=base64|filepath`, `GEMINI_FUSED=1|0`, `PREPROCESS_ENGINE=polars|pandas` (engine wiring in Step 2).
    - Observability
      - Parallelized upload steps; will add stage timing benchmarks in a follow-up.

- **2025-10-01**
  - Backend Performance â€“ Step 2 implemented.
    - Preprocess engine defaulted to Polars for CSV via `run-preprocess/pipeline_adapter_polars.py`; Excel remains via pandas.
    - Extracted `process_df_to_artifacts()` in `run-preprocess/pipeline_adapter.py` to centralize cleaning/payload logic (shared by both adapters).
    - Dependencies: added `polars[xlsx]==1.7.1`, aligned `numpy==2.0.2` in `run-preprocess/requirements.txt`.
    - Deployed new `preprocess-svc` revision with `--cpu=2 --memory=2Gi --concurrency=10` and `PREPROCESS_ENGINE=polars`.
    - Next: run `backend/test.ps1` to smoke test end-to-end (upload â†’ Eventarc â†’ artifacts + Firestore).

- **2025-10-01**
  - Polars adapter bugfix and redeploy.
    - Replaced Polars `.str.strip()` with `.str.strip_chars()` in `run-preprocess/pipeline_adapter_polars.py` (`_drop_fully_blank_rows_pl`, `_numeric_expr_for`, repeat-header filters) to match deployed Polars API.
    - Redeployed `preprocess-svc`; Eventarc successfully processed uploaded CSV; artifacts written: `cleaned/cleaned.parquet`, `metadata/payload.json`, `reports/cleaning_report.json`.
    - `backend/test.ps1` passed end-to-end; prior `MISSING_PARQUET` error resolved.
    - Runtime confirmed at Python 3.12 via `run-preprocess/runtime.txt`.

- **2025-09-29**
  - **Frontend Redesign**: Complete UI overhaul with ChatGPT-style interface.
  - Created 4 new components: `NewSidebar.tsx` (collapsible with icon/expanded states), `FloatingChatInput.tsx` (overlay at bottom), `NewChatArea.tsx` (infinite scroll), `FloatingControls.tsx` (minimal top bar).
  - Removed visible header/footer for immersive experience; floating controls with backdrop blur.
    - Sidebar displays last 5 chat sessions, user profile with avatar, and daily usage indicator (100 requests/day limit).
    - Chat input: auto-resizing textarea, file attachment preview, keyboard shortcuts (Enter to send, Shift+Enter for new line).
    - Fully integrated with existing `ChatContext` and `AuthContext`; uses real SSE streaming from `services/api.ts`.
    - Firebase config template created at `frontend/src/lib/firebase.ts` with placeholders for future migration.
    - Updated `main.tsx` to use new `AppFinal.tsx` as entry point.
- **2025-09-28**
  - Milestone 2: Implemented LLM-driven analysis with Gemini 2.5 Flash.
    - Orchestrator (`backend/functions/orchestrator/`) now downloads `cleaned.parquet`, generates Python via LLM, validates with AST (allowlist: pandas, numpy, math, json), and executes in a sandboxed subprocess with a 60s hard timeout.
    - Persists results to GCS: `table.json`, `metrics.json`, `chart_data.json`, `summary.json`; writes Firestore `messages/{messageId}` doc.
    - SSE event flow: `received â†’ validating â†’ generating_code â†’ running_fast â†’ summarizing â†’ persisting â†’ done` (+ `ping`).
    - CORS defaults to `http://localhost:3000`.
  - Docs updated: `backend/docs/api.md` now documents SSE contract and `chartData` schema. `README.md` includes chat usage and env var notes.
  - `backend/test.ps1` extended with a best-effort SSE smoke test.
  - SSE behavior change: the orchestrator now closes the SSE stream immediately after the `done` event (removed keep-alive pings). This prevents client-side curl timeouts in smoke tests.
  - Deployment split: created `backend/deploy-preprocess.ps1` (preprocess Cloud Run + Eventarc) and `backend/deploy-analysis.ps1` (Functions: `sign-upload-url`, `chat`). Prefer running these separately; avoid re-deploying preprocess unless needed.
- **2025-09-27**
  - Standardized on `backend/deploy.ps1` as the only deployment method.
  - Removed `backend/cloudbuild.yaml` and all documentation references to Cloud Build.
  - Updated `README.md` and `backend/run-preprocess/README.md` accordingly.
- **2025-09-26 (later)**
  - Repository restructure: moved backend components under `backend/`.
  - Updated `backend/deploy.ps1` and `backend/test.ps1` to use script-relative paths.
  - Added unified CI/CD at `backend/cloudbuild.yaml` (deprecated on 2025-09-27).
- **2025-09-26**
  - Deployed `preprocess-svc` rev `preprocess-svc-00005-w5c` via `deploy.ps1`.
  - Verified end-to-end: artifacts generated and Firestore updated to `ready`.
- **2025-09-25**
  - `sign-upload-url`: switched to V4 signed URLs using impersonated credentials; removed private key reliance.
  - `run-preprocess/main.py`: implemented robust CloudEvent parsing for `/eventarc`; added `/healthz`.
  - `test.ps1`: increased wait to 30s; added Firestore status fetch.
  - Firestore TTL policy enabled for `datasets` on `ttlAt` (ACTIVE).

## Next Steps
- **Optional**: authenticate health probe in scripts with identity tokens if you want a green check.
- **Optional**: apply Cloud Run resource tuning if processing larger files.
- **Upcoming**: integrate chat/orchestrator stage to consume `payload.json` and cleaned data.

---

This file is the living progress record for this project. Update it with each change to deployments, architecture, or operational practice.
</file>

<file path="frontend/cloudbuild.yaml">
# cloudbuild.yaml — Secure & working version (Node + Secret Manager + Firebase deploy)

steps:
  # Step 1: Install dependencies
  - name: 'node:20'
    entrypoint: 'npm'
    args: ['ci']
    dir: 'frontend'

  # Step 2: Load secrets, build, and deploy
  - name: 'node:20' # Node image includes npm
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "🔐 Loading secrets from Secret Manager..."
        export VITE_CHAT_URL=$(gcloud secrets versions access latest --secret=VITE_CHAT_URL)
        export VITE_SIGN_URL=$(gcloud secrets versions access latest --secret=VITE_SIGN_URL)
        export VITE_FIREBASE_API_KEY=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_API_KEY)
        export VITE_FIREBASE_AUTH_DOMAIN=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_AUTH_DOMAIN)
        export VITE_FIREBASE_PROJECT_ID=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_PROJECT_ID)
        export VITE_FIREBASE_STORAGE_BUCKET=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_STORAGE_BUCKET)
        export VITE_FIREBASE_MESSAGING_SENDER_ID=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_MESSAGING_SENDER_ID)
        export VITE_FIREBASE_APP_ID=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_APP_ID)
        export VITE_FIREBASE_MEASUREMENT_ID=$(gcloud secrets versions access latest --secret=VITE_FIREBASE_MEASUREMENT_ID)

        echo "✅ Secrets loaded successfully"
        cd frontend

        echo "🏗️ Building production app..."
        npm run build

        echo "🚀 Deploying to Firebase Hosting..."
        npm install -g firebase-tools
        firebase experiments:enable webframeworks
        firebase deploy --only hosting --project ai-data-analyser --non-interactive

# Optional: reduce logs
options:
  logging: CLOUD_LOGGING_ONLY
</file>

<file path="frontend/src/components/ChatMessage.tsx">
import React from "react";
import { cn } from "./ui/utils";
import { Bot } from "lucide-react";
import { TableRenderer } from "./renderers/TableRenderer";
import { ChartRenderer } from "./renderers/ChartRenderer";
import { Button } from "./ui/button";

export type Message =
  | {
      id: string;
      role: "user" | "assistant";
      timestamp: Date;
      kind: "text";
      content: string;
      meta?: {
        fileName?: string;
        fileSize?: string; // formatted, e.g., "1.2 MB"
        rows?: number;
        columns?: number;
      };
    }
  | {
      id: string;
      role: "user" | "assistant";
      timestamp: Date;
      kind: "status";
      content: string;
    }
  | {
      id: string;
      role: "assistant";
      timestamp: Date;
      kind: "error";
      content: string;
    }
  | {
      id: string;
      role: "assistant";
      timestamp: Date;
      kind: "table";
      rows: any[];
    }
  | {
      id: string;
      role: "assistant";
      timestamp: Date;
      kind: "chart";
      chartData: {
        kind: string;
        labels: string[];
        series: { label: string; data: number[] }[];
      };
    };

interface ChatMessageProps {
  message: Message;
  userName: string;
  showCancel?: boolean;
  onCancel?: () => void;
}

export const ChatMessage: React.FC<ChatMessageProps> = ({ message, userName, showCancel, onCancel }) => {
  const isUser = message.role === "user";
  const timeStr = React.useMemo(() => {
    const d = message.timestamp instanceof Date ? message.timestamp : new Date(message.timestamp as any);
    try {
      return d.toLocaleTimeString(undefined, { hour: "2-digit", minute: "2-digit", second: "2-digit", hour12: false });
    } catch {
      return "";
    }
  }, [message.timestamp]);

  return (
    <div className="w-full py-8 px-4">
      <div className={cn("max-w-3xl mx-auto flex gap-6")}> 
        {/* Avatar */}
        <div className="flex-shrink-0">
          {isUser ? (
            <div className="w-8 h-8 rounded-full bg-primary text-primary-foreground flex items-center justify-center">
              {userName.charAt(0).toUpperCase()}
            </div>
          ) : (
            <div className="w-8 h-8 rounded-full bg-primary text-primary-foreground flex items-center justify-center">
              <Bot className="h-5 w-5" />
            </div>
          )}
        </div>

        {/* Message Content */}
        <div className="flex-1 space-y-2 pt-1">
          {message.kind === "text" && (
            <div className="whitespace-pre-wrap break-words">{message.content}</div>
          )}
          {message.kind === "status" && (
            <div className="whitespace-pre-wrap break-words text-muted-foreground italic">
              {message.content}
              {showCancel && (
                <div className="mt-3">
                  <Button variant="outline" size="sm" onClick={onCancel}>
                    Cancel
                  </Button>
                </div>
              )}
            </div>
          )}
          {message.kind === "error" && (
            <div className="whitespace-pre-wrap break-words border border-red-300 bg-red-50 text-red-800 rounded-xl p-4">
              {message.content}
            </div>
          )}
          {message.kind === "table" && (
            <div className="w-fit max-w-full overflow-auto">
              <TableRenderer rows={message.rows} />
            </div>
          )}
          {message.kind === "chart" && <ChartRenderer chartData={message.chartData} />}
          <div className="text-xs text-muted-foreground flex flex-wrap items-center gap-2">
            <span>{timeStr}</span>
            {message.kind === "text" && message.meta?.fileName && (
              <span className="inline-flex items-center rounded-full border px-2 py-0.5">
                {message.meta.fileName}
              </span>
            )}
            {message.kind === "text" && message.meta?.fileSize && (
              <span className="inline-flex items-center rounded-full border px-2 py-0.5">
                {message.meta.fileSize}
              </span>
            )}
            {message.kind === "text" && message.meta?.rows !== undefined && message.meta?.columns !== undefined && (
              <span className="inline-flex items-center rounded-full border px-2 py-0.5">
                {Number(message.meta.rows).toLocaleString()} rows x {Number(message.meta.columns).toLocaleString()} columns
              </span>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};
</file>

<file path="frontend/src/App.tsx">
import React, { useState, useRef, useEffect } from "react";
import { ChatSidebar } from "./components/ChatSidebar";
import { ChatMessage, type Message } from "./components/ChatMessage";
import { ChatInput } from "./components/ChatInput";
import { ChatHeader } from "./components/ChatHeader";
import { ScrollArea } from "./components/ui/scroll-area";
import { Button } from "./components/ui/button";
import { useAuth } from "./context/AuthContext";
import { ensureSession, updateSessionDataset, saveUserMessage, getRecentSessionsWithMessages, subscribeDatasetMeta } from "./services/firestore";
import { getSignedUploadUrl, putToSignedUrl, streamChat, type ChatEvent } from "./services/api";

interface Conversation {
  id: string;
  title: string;
  timestamp: Date;
  messages: Message[];
  datasetId?: string;
}

export default function App() {
  const { idToken, loading, user } = useAuth();
  const [sidebarOpen, setSidebarOpen] = useState(false);
  const [dailyUsed, setDailyUsed] = useState(3);
  const dailyLimit = 50;
  const [conversations, setConversations] = useState<Conversation[]>([]);
  const [activeConversationId, setActiveConversationId] = useState<string | null>(null);
  const [isTyping, setIsTyping] = useState(false);
  const [uploading, setUploading] = useState(false);
  const abortRef = useRef<AbortController | null>(null);
  const scrollRef = useRef<HTMLDivElement>(null);
  const bottomRef = useRef<HTMLDivElement>(null);
  const prevConvIdRef = useRef<string | null>(null);
  const placeholderIdRef = useRef<string | null>(null);
  const didInitRef = useRef<boolean>(false);
  const datasetMetaSubsRef = useRef<Record<string, () => void>>({});
  const uploadMsgIdByConvRef = useRef<Record<string, string | null>>({});

  const formatBytes = (bytes: number): string => {
    if (!Number.isFinite(bytes) || bytes <= 0) return "0 B";
    const k = 1024;
    const sizes = ["B", "KB", "MB", "GB", "TB"];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    const val = bytes / Math.pow(k, i);
    return `${val.toFixed(val >= 100 ? 0 : val >= 10 ? 1 : 2)} ${sizes[i]}`;
  };

  const activeConversation = conversations.find((c) => c.id === activeConversationId);

  // Auto-scroll to bottom when new messages or typing indicator changes
  useEffect(() => {
    const behavior: ScrollBehavior =
      prevConvIdRef.current && prevConvIdRef.current === activeConversationId
        ? 'smooth'
        : 'auto';

    // Defer to next frame so DOM has updated
    const id = requestAnimationFrame(() => {
      bottomRef.current?.scrollIntoView({ behavior, block: 'end' });
    });
    prevConvIdRef.current = activeConversationId;
    return () => cancelAnimationFrame(id);
  }, [activeConversationId, activeConversation?.messages.length, isTyping]);

  const handleNewChat = () => {
    const newConversation: Conversation = {
      id: Date.now().toString(),
      title: "New conversation",
      timestamp: new Date(),
      messages: [],
    };
    setConversations([newConversation, ...conversations]);
    setActiveConversationId(newConversation.id);
    // Persist session shell
    if (user?.uid) {
      ensureSession(user.uid, newConversation.id, newConversation.title).catch(() => {});
    }
  };

  // Start with a new chat on first mount if nothing is loaded
  useEffect(() => {
    if (didInitRef.current) return;
    didInitRef.current = true;
    if (conversations.length === 0) {
      // Defer to next tick to avoid strict-mode double invokes causing two creations
      setTimeout(() => {
        if (conversations.length === 0) handleNewChat();
      }, 0);
    }
  }, []);

  const handleSelectConversation = (id: string) => {
    setActiveConversationId(id);
  };

  const handleDeleteConversation = (id: string) => {
    // Cleanup any dataset meta listener for this conversation
    try {
      datasetMetaSubsRef.current[id]?.();
      delete datasetMetaSubsRef.current[id];
    } catch {}
    setConversations((prev) => {
      const next = prev.filter((c) => c.id !== id);
      setActiveConversationId((current) => {
        if (current === id) {
          return next.length ? next[0].id : null;
        }
        return current;
      });
      return next;
    });
  };

  // Cancel current analysis run (abort SSE) and mark placeholder as cancelled
  const handleCancel = () => {
    try {
      abortRef.current?.abort();
    } catch {
      // ignore
    }
    const pid = placeholderIdRef.current;
    const cid = activeConversationId;
    if (pid && cid) {
      setConversations((prev) =>
        prev.map((c) => {
          if (c.id !== cid) return c;
          const idx = c.messages.findIndex((m) => m.id === pid);
          if (idx === -1) return c;
          const nextMsgs = c.messages.slice();
          const msg = nextMsgs[idx] as Message;
          if (msg.role === "assistant") {
            nextMsgs[idx] = { ...(msg as any), kind: "status", content: "Cancelled." } as Message;
          }
          return { ...c, messages: nextMsgs };
        })
      );
    }
    setIsTyping(false);
    abortRef.current = null;
    placeholderIdRef.current = null;
  };

  // For prod (Hosting), default to relative /api routes via rewrites
  const SIGN_URL = ((import.meta as any).env?.VITE_SIGN_URL as string | undefined) || "/api/sign-upload-url";
  const CHAT_URL = ((import.meta as any).env?.VITE_CHAT_URL as string | undefined) || "/api/chat";

  // Load last ~5 sessions on auth ready
  useEffect(() => {
    (async () => {
      if (!loading && user?.uid) {
        try {
          const sessions = await getRecentSessionsWithMessages(user.uid, 5);
          if (sessions.length > 0) {
            setConversations((prev) => {
              if (!prev || prev.length === 0) return sessions as any;
              const prevIds = new Set(prev.map((c) => c.id));
              const newOnes = sessions.filter((s: any) => !prevIds.has(s.id)) as any;
              return [...prev, ...newOnes];
            });
            setActiveConversationId((prev) => prev ?? sessions[0].id);
          } else {
            // keep defaults / empty
          }
        } catch (_) {
          // ignore load errors in UI
        }
      }
    })();
  }, [loading, user?.uid]);

  const ensureConversation = (): string => {
    if (!activeConversationId) {
      const newId = Date.now().toString();
      const newConversation: Conversation = {
        id: newId,
        title: "New conversation",
        timestamp: new Date(),
        messages: [],
      };
      setConversations((prev) => [newConversation, ...prev]);
      setActiveConversationId(newId);
      if (user?.uid) {
        ensureSession(user.uid, newId, newConversation.title).catch(() => {});
      }
      return newId;
    }
    return activeConversationId;
  };

  const handleUploadFile = async (file: File) => {
    if (!SIGN_URL) {
      alert("Missing VITE_SIGN_URL env");
      return;
    }
    if (loading || !idToken) {
      alert("Authenticating... please retry");
      return;
    }
    setUploading(true);
    try {
      const convId = ensureConversation();
      if (!convId) return;
      const resp = await getSignedUploadUrl({
        signUrl: SIGN_URL,
        idToken,
        sessionId: convId,
        filename: file.name,
        size: file.size,
        type: file.type || "application/octet-stream",
      });
      await putToSignedUrl(resp.url, file);

      // Prepare a stable id for the upload system message so we can update it later with rows×columns
      const uploadMsgId = `${convId}-${Date.now()}-sys`;
      uploadMsgIdByConvRef.current[convId] = uploadMsgId;

      // Update conversation with datasetId and add system message (meta shows file name & size)
      setConversations((prev) =>
        prev.map((c) =>
          c.id === convId
            ? {
                ...c,
                datasetId: resp.datasetId,
                messages: [
                  ...c.messages,
                  {
                    id: uploadMsgId,
                    role: "assistant",
                    kind: "text",
                    content: "File uploaded and queued for preprocessing. You can now ask a question about your data.",
                    meta: { fileName: file.name, fileSize: formatBytes(file.size) },
                    timestamp: new Date(),
                  } as Message,
                ],
              }
            : c
        )
      );
      if (user?.uid) {
        updateSessionDataset(user.uid, convId, resp.datasetId).catch(() => {});
        // Listen for dataset metadata (rows, columns) and announce when ready
        try {
          // Clean up any prior sub for this conversation
          datasetMetaSubsRef.current[convId]?.();
        } catch {}
        const unsub = subscribeDatasetMeta(user.uid, convId, resp.datasetId, (meta) => {
          
          const r = typeof meta?.rows === "number" ? meta.rows : undefined;
          const c = typeof meta?.columns === "number" ? meta.columns : undefined;
          if (r && c) {
            setConversations((prev) =>
              prev.map((sess) => {
                if (sess.id !== convId) return sess;
                const msgs = sess.messages.slice();
                // Prefer updating the upload system message meta; fallback: update the most recent assistant text message
                const targetId = uploadMsgIdByConvRef.current[convId] || null;
                let idx = targetId ? msgs.findIndex((m) => m.id === targetId) : -1;
                if (idx === -1) {
                  for (let i = msgs.length - 1; i >= 0; i--) {
                    const m = msgs[i];
                    if (m.role === "assistant" && m.kind === "text") { idx = i; break; }
                  }
                }
                if (idx >= 0) {
                  const m = msgs[idx] as Message;
                  msgs[idx] = {
                    ...(m as any),
                    meta: { ...(m as any).meta, rows: r, columns: c },
                  } as Message;
                }
                return { ...sess, messages: msgs };
              })
            );
            try { unsub(); } catch {}
            delete datasetMetaSubsRef.current[convId];
            uploadMsgIdByConvRef.current[convId] = null;
          }
        });
        datasetMetaSubsRef.current[convId] = unsub;
      }
    } catch (e: any) {
      alert(e?.message || String(e));
    } finally {
      setUploading(false);
    }
  };

  const handleSendMessage = async (content: string) => {
    const convId = ensureConversation();
    if (!convId) return;
    const conv = conversations.find((c) => c.id === convId);
    if (!conv?.datasetId) {
      alert("Please upload a dataset first using the paperclip.");
      return;
    }
    if (loading || !idToken) {
      alert("Authenticating... please retry");
      return;
    }
    if (!CHAT_URL) {
      alert("Missing VITE_CHAT_URL env");
      return;
    }

    // Push user message
    const userMessage: Message = {
      id: `${convId}-${Date.now()}`,
      role: "user",
      kind: "text",
      content,
      timestamp: new Date(),
    };
    setConversations((prev) =>
      prev.map((c) =>
        c.id === convId
          ? {
              ...c,
              messages: [...c.messages, userMessage],
              title: c.messages.length === 0 ? (content.length > 50 ? content.slice(0, 50) + "..." : content) : c.title,
            }
          : c
      )
    );
    if (user?.uid) {
      saveUserMessage(user.uid, convId, userMessage.id, content).catch(() => {});
    }

    // Start SSE stream
    setIsTyping(true);
    const ac = new AbortController();
    abortRef.current = ac;
    // Create and push assistant placeholder immediately
    const placeholderId = `${convId}-${Date.now()}-ph`;
    placeholderIdRef.current = placeholderId;
    const placeholder: Message = {
      id: placeholderId,
      role: "assistant",
      kind: "status",
      content: "Analyzing...",
      timestamp: new Date(),
    };
    setConversations((prev) => prev.map((c) => (c.id === convId ? { ...c, messages: [...c.messages, placeholder] } : c)));

    const updatePlaceholder = (updater: (m: Extract<Message, { role: "assistant" }>) => Message) => {
      setConversations((prev) =>
        prev.map((c) => {
          if (c.id !== convId) return c;
          const idx = c.messages.findIndex((m) => m.id === placeholderId);
          if (idx === -1) return c; // fallback if not found
          const nextMsgs = c.messages.slice();
          nextMsgs[idx] = updater(nextMsgs[idx] as Extract<Message, { role: "assistant" }>);
          return { ...c, messages: nextMsgs };
        })
      );
    };

    try {
      await streamChat({
        chatUrl: CHAT_URL,
        idToken,
        sessionId: convId,
        datasetId: conv.datasetId!,
        question: content,
        signal: ac.signal,
        onEvent: (ev: ChatEvent) => {
          if (ev.type === "validating") updatePlaceholder((m) => ({ ...m, kind: "status", content: "Validating input..." }));
          else if (ev.type === "generating_code") updatePlaceholder((m) => ({ ...m, kind: "status", content: "Generating analysis code..." }));
          else if (ev.type === "running_fast") updatePlaceholder((m) => ({ ...m, kind: "status", content: "Running analysis..." }));
          else if (ev.type === "summarizing") updatePlaceholder((m) => ({ ...m, kind: "status", content: "Summarizing results..." }));
          else if (ev.type === "persisting") updatePlaceholder((m) => ({ ...m, kind: "status", content: "Saving results..." }));
          else if (ev.type === "error") {
            updatePlaceholder((m) => ({ ...m, kind: "error", content: `Error: ${ev.data.message}` }));
            setIsTyping(false);
          } else if (ev.type === "done") {
            const text = ev.data.summary || "Analysis complete.";
            // 1) Turn placeholder into final summary text
            updatePlaceholder((m) => ({ ...m, kind: "text", content: text }));
            // 2) Append table and chart bubbles if present
            const rows = Array.isArray(ev.data.tableSample) ? ev.data.tableSample : [];
            const chartData = ev.data.chartData || null;
            if (rows && rows.length > 0) {
              setConversations((prev) =>
                prev.map((c) =>
                  c.id === convId
                    ? {
                        ...c,
                        messages: [
                          ...c.messages,
                          { id: `${convId}-${Date.now()}-table`, role: "assistant", timestamp: new Date(), kind: "table", rows },
                        ],
                      }
                    : c
                )
              );
            }
            // Only append chart when it has actual data
            const hasChartData = (cd: any): boolean => {
              try {
                const labels = cd?.labels;
                const series = cd?.series;
                if (!Array.isArray(labels) || labels.length === 0) return false;
                if (!Array.isArray(series) || series.length === 0) return false;
                return series.some((s: any) => Array.isArray(s?.data) && s.data.some((x: any) => typeof x === "number"));
              } catch {
                return false;
              }
            };
            if (chartData && hasChartData(chartData)) {
              setConversations((prev) =>
                prev.map((c) =>
                  c.id === convId
                    ? {
                        ...c,
                        messages: [
                          ...c.messages,
                          { id: `${convId}-${Date.now()}-chart`, role: "assistant", timestamp: new Date(), kind: "chart", chartData },
                        ],
                      }
                    : c
                )
              );
            }
            setIsTyping(false);
            setDailyUsed((prev) => prev + 1);
          }
        },
      });
    } catch (e) {
      // Network error already surfaced in onEvent or thrown; ensure state cleanup
      setIsTyping(false);
    } finally {
      abortRef.current = null;
      // Fallback: if stream ended without a 'done' event, stop typing indicator
      setIsTyping(false);
    }
  };

  return (
    <div className="size-full flex bg-background">
      {/* Header */}
      <ChatHeader sidebarOpen={sidebarOpen} />

      {/* Sidebar */}
      <ChatSidebar
        isOpen={sidebarOpen}
        onToggle={() => setSidebarOpen(!sidebarOpen)}
        conversations={conversations}
        activeConversationId={activeConversationId}
        onSelectConversation={handleSelectConversation}
        onNewChat={handleNewChat}
        onDeleteConversation={handleDeleteConversation}
        userName="Fatih Demirciler"
        userPlan="Free"
        dailyLimit={dailyLimit}
        dailyUsed={dailyUsed}
      />

      {/* Main Chat Area */}
      <main
        className="flex-1 flex flex-col h-full transition-all duration-300 pt-14"
        style={{
          marginLeft: sidebarOpen ? "256px" : "64px",
        }}
      >
        {/* Messages */}
        <ScrollArea ref={scrollRef} className="flex-1">
          {activeConversation && activeConversation.messages.length > 0 ? (
            <div className="pb-32">
              {activeConversation.messages.map((message) => (
                <React.Fragment key={message.id}>
                  <ChatMessage
                    message={message}
                    userName="Fatih Demirciler"
                    showCancel={
                      isTyping &&
                      message.role === "assistant" &&
                      message.kind === "status" &&
                      message.id === placeholderIdRef.current
                    }
                    onCancel={handleCancel}
                  />
                </React.Fragment>
              ))}
              {/* Bottom sentinel for smooth scrolling */}
              <div ref={bottomRef} />
            </div>
          ) : (
            <div className="h-full flex items-center justify-center p-8 pb-32">
              <div className="text-center max-w-md">
                <h2 className="mb-4">Start a new conversation</h2>
                <p className="text-muted-foreground">
                  Ask me anything! I'm here to help answer your questions and have a conversation.
                </p>
              </div>
            </div>
          )}
        </ScrollArea>
      </main>

      {/* Fixed Input at Bottom */}
      <div
        className="transition-all duration-300"
        style={{
          marginLeft: sidebarOpen ? "256px" : "64px",
        }}
      >
        <ChatInput onSendMessage={handleSendMessage} onUploadFile={handleUploadFile} disabled={isTyping || uploading} />
      </div>
    </div>
  );
}
</file>

</files>
