*** suggested plan 1 ***

Below is a single, coherent improvement plan that merges the best ideas from every contributor into one drop-in upgrade.
It keeps 100 % backward compatibility (existing SSE events, Firestore paths, worker contract) while increasing fast-path coverage from ~4 patterns to > 15 and giving the LLM richer metadata and cleaner error messages.
1. Guiding Principles
Table
Copy
Principle	How we implement it
Zero breaking change	New functions are additive; old names (run_aggregation, run_variance …) are thin wrappers that delegate to the new core.
One noun → one verb	agg_by, compare_periods, filter_rows, sort_rows, top_n_per_group, correlate, pivot_table, rolling_growth … so the LLM can guess the tool.
Enum-only parameters	No raw bool, no magic int. `direction: "asc"	"desc", agg: "sum"	"mean"	…`
Always return metadata	Every DataFrame carries a tiny _meta column: "imputed_nulls:17" or "rows_after_filter:842" so the LLM knows what happened.
Auto-generate TOOLS_SPEC	Single source of truth → add a function → nothing else to edit.
2. What Changes Where
Table
Copy
File	Change
analysis_toolkit.py	Replace entirely with v2 below (keeps old names as wrappers).
main.py	3-line patch: update name_map dictionary (shown below).
Rest of repo	No touch – worker, sandbox, Firestore, SSE, signed-url logic stay identical.
3. Drop-In Replacement – analysis_toolkit.py
Python
Copy
"""
analysis_toolkit v2  –  fast-path deterministic helpers
-------------------------------------------------------
- 100 % backward compatible:  old names (run_aggregation, …) still exist
- Additive design:  new verbs do ONE thing and compose
- Auto-generated TOOLS_SPEC – never edit JSON by hand again
- Every call returns a DataFrame with _meta column for diagnostics
"""
from __future__ import annotations
import inspect
import json
from typing import Iterable, Literal, Optional, Sequence, List
import pandas as pd
import numpy as np

TOOLKIT_VERSION = 2

# ------------------------------------------------------------------ helpers
Agg      = Literal["sum", "mean", "median", "min", "max", "count", "std"]
Dir      = Literal["asc", "desc"]
TimeFreq = Literal["D", "W", "M", "Q", "Y"]

def _ensure_cols(df: pd.DataFrame, cols: Sequence[str]) -> None:
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise KeyError(f"Missing columns: {missing}")

def _to_numeric(series: pd.Series) -> tuple[pd.Series, int]:
    """Return numeric series + count of non-finite values replaced."""
    before = series.isna().sum()
    s = pd.to_numeric(series, errors="coerce")
    after = s.isna().sum()
    return s, after - before

def _meta_df(df: pd.DataFrame, msg: str) -> pd.DataFrame:
    """Attach a tiny diagnostic column to the result."""
    out = df.copy()
    out.insert(out.shape[1], "_meta", msg)
    return out

# ------------------------------------------------------------------ new core
def agg_by(df: pd.DataFrame, dimension: str, metric: str, agg: Agg) -> pd.DataFrame:
    """Aggregate `metric` by `dimension` using `agg`."""
    _ensure_cols(df, [dimension, metric])
    num, imputed = _to_numeric(df[metric])
    tmp = df.assign(**{metric: num})
    grouped = tmp.groupby(dimension, dropna=False)[metric].agg(agg).reset_index()
    grouped.columns = [dimension, f"{metric}_{agg}"]
    out = grouped.sort_values(by=grouped.columns[1], ascending=False, kind="stable")
    return _meta_df(out.reset_index(drop=True), f"imputed_nulls:{imputed}")


def compare_periods(
    df: pd.DataFrame,
    dimension: str,
    period_col: str,
    value_col: str,
    period_a: str,
    period_b: str,
    agg: Agg = "sum",
) -> pd.DataFrame:
    """Compare `value_col` for `period_a` vs `period_b` after grouping by `dimension`."""
    _ensure_cols(df, [dimension, period_col, value_col])
    num, imputed = _to_numeric(df[value_col])
    tmp = (
        df.assign(**{value_col: num})
        .groupby([dimension, period_col], dropna=False)[value_col]
        .agg(agg)
        .reset_index()
    )
    pivot = tmp.pivot(index=dimension, columns=period_col, values=value_col)
    if period_a not in pivot.columns or period_b not in pivot.columns:
        raise ValueError(f"Periods {period_a} or {period_b} not found in data.")
    out = pivot[[period_a, period_b]].reset_index()
    out["delta"] = out[period_b] - out[period_a]
    out["pct_change"] = (out["delta"] / out[period_a].replace(0, np.nan)) * 100
    return _meta_df(out.sort_values("delta", ascending=False).reset_index(drop=True),
                    f"imputed_nulls:{imputed}")


def filter_rows(
    df: pd.DataFrame,
    column: str,
    op: Literal["eq", "ne", "gt", "ge", "lt", "le", "in", "between"],
    value: str | Sequence[str] | float | tuple[float, float],
) -> pd.DataFrame:
    """Filter rows using `op` operator."""
    _ensure_cols(df, [column])
    col = df[column]
    if op == "eq":
        mask = col.astype(str) == str(value)
    elif op == "ne":
        mask = col.astype(str) != str(value)
    elif op in {"gt", "ge", "lt", "le"}:
        num, _ = _to_numeric(col)
        mask = getattr(num, f"{op}")(float(value))  # type: ignore
    elif op == "in":
        mask = col.astype(str).isin([str(v) for v in value])  # type: ignore
    elif op == "between":
        lo, hi = value  # type: ignore
        num, _ = _to_numeric(col)
        mask = num.between(float(lo), float(hi))
    else:
        raise ValueError("Unknown operator")
    out = df[mask].copy()
    return _meta_df(out.reset_index(drop=True), f"rows_after_filter:{len(out)}")


def sort_rows(
    df: pd.DataFrame, column: str, direction: Dir, limit: Optional[int] = None
) -> pd.DataFrame:
    """Stable sort by `column` in `direction`; optionally keep top `limit`."""
    _ensure_cols(df, [column])
    ascending = direction == "asc"
    out = df.sort_values(column, ascending=ascending, kind="stable", na_position="last")
    if limit:
        out = out.head(limit)
    out = out.reset_index(drop=True)
    return _meta_df(out, f"limit_applied:{limit}")


def top_n_per_group(
    df: pd.DataFrame, group: str, metric: str, agg: Agg, top: int
) -> pd.DataFrame:
    """Top `top` rows per `group` after aggregating `metric` with `agg`."""
    _ensure_cols(df, [group, metric])
    num, imputed = _to_numeric(df[metric])
    tmp = df.assign(**{metric: num})
    aggregated = tmp.groupby(group, dropna=False)[metric].agg(agg).reset_index()
    ascending = False
    out = aggregated.sort_values(by=metric, ascending=ascending, kind="stable").head(top)
    out.columns = [group, f"{metric}_{agg}"]
    return _meta_df(out.reset_index(drop=True), f"imputed_nulls:{imputed}")


def bottom_n_per_group(
    df: pd.DataFrame, group: str, metric: str, agg: Agg, top: int
) -> pd.DataFrame:
    """Bottom `top` rows per `group` after aggregating `metric` with `agg`."""
    return top_n_per_group(df, group, metric, agg, top).sort_values(
        by=f"{metric}_{agg}", ascending=True
    ).reset_index(drop=True)


def correlate(df: pd.DataFrame, columns: Optional[list[str]] = None) -> pd.DataFrame:
    """Pearson correlation matrix for numeric columns (or subset)."""
    subset = df.select_dtypes(include=["number"]) if columns is None else df[columns]
    if subset.shape[1] < 2:
        raise ValueError("Need ≥ 2 numeric columns for correlation.")
    corr = subset.corr(method="pearson").reset_index().rename(columns={"index": "variable"})
    return _meta_df(corr, "method:pearson")


def pivot_table(
    df: pd.DataFrame,
    index: str | list[str],
    columns: str | list[str],
    values: str,
    agg: Agg = "sum",
) -> pd.DataFrame:
    """Excel-style pivot table."""
    _ensure_cols(df, [values])
    num, imputed = _to_numeric(df[values])
    tmp = df.assign(**{values: num})
    table = pd.pivot_table(
        tmp, index=index, columns=columns, values=values, aggfunc=agg
    ).reset_index()
    return _meta_df(table, f"imputed_nulls:{imputed}")


def rolling_growth(
    df: pd.DataFrame,
    date_col: str,
    value_col: str,
    freq: TimeFreq,
    periods: int = 1,
    agg: Agg = "sum",
) -> pd.DataFrame:
    """Compute period-over-period growth after resampling to `freq`."""
    _ensure_cols(df, [date_col, value_col])
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    num, imputed = _to_numeric(df[value_col])
    tmp = df.assign(**{value_col: num}).set_index(date_col)
    resampled = tmp[value_col].resample(freq).agg(agg)
    growth = resampled.pct_change(periods=periods) * 100
    out = growth.reset_index().rename(columns
Copy
Share
please enrich our tool kit with more functions. there are many suggested functions in my last attached file. please add them but make sure their structure are in line with your suggested function format.
Edit
Copy
Share
Below is a single-file drop-in that appends 13 new deterministic verbs to your toolkit while keeping 100 % backward compatibility and the same structure (enum-only params, _meta diagnostics, auto-generated TOOLS_SPEC).
Copy-paste the whole block into analysis_toolkit.py (or merge the new functions) and the LLM immediately sees:
Table
Copy
New verb	Typical question it answers
outliers	“Show me unusually large transactions”
percentile	“What is the 90<sup> percentile of response time?”
normalize	“Standardise price to mean 0, std 1”
fill_nulls	“Fill missing ages with the median”
bin_numeric	“Group ages into 5 bins”
value_counts	“List the 10 most frequent products”
cohort	“Monthly retention cohorts”
moving_avg	“7-day moving average of sales”
yoy_growth	“Year-over-year growth %”
concat_tables	“Union two CSVs”
drop_duplicates	“Remove duplicate rows”
rename_columns	“Rename ‘rev’ → ‘revenue’”
select_columns	“Keep only these 4 columns”
Complete additive block
Python
Copy
# ------------------------------------------------------------------
#  NEW  DETERMINISTIC  VERBS  (drop-in;  keeps  old  wrappers)
# ------------------------------------------------------------------

def outliers(df: pd.DataFrame, column: str, method: Literal["iqr", "zscore"] = "iqr", k: float = 1.5) -> pd.DataFrame:
    """Return rows whose `column` is an outlier."""
    _ensure_cols(df, [column])
    num, imputed = _to_numeric(df[column])
    if method == "iqr":
        q1, q3 = num.quantile([0.25, 0.75])
        iqr = q3 - q1
        mask = (num < (q1 - k * iqr)) | (num > (q3 + k * iqr))
    else:  # zscore
        mu, sigma = num.mean(), num.std()
        mask = np.abs((num - mu) / sigma) > k
    out = df[mask].copy()
    return _meta_df(out.reset_index(drop=True), f"outliers_found:{len(out)}")


def percentile(df: pd.DataFrame, column: str, p: float) -> pd.DataFrame:
    """Single-row result: the p-th percentile of `column` (0 ≤ p ≤ 100)."""
    _ensure_cols(df, [column])
    if not 0 <= p <= 100:
        raise ValueError("p must be between 0 and 100")
    num, imputed = _to_numeric(df[column])
    val = np.percentile(num.dropna(), p)
    return _meta_df(pd.DataFrame({f"{column}_p{p}": [val]}), f"imputed_nulls:{imputed}")


def normalize(df: pd.DataFrame, columns: Optional[List[str]] = None, method: Literal["zscore", "minmax"] = "zscore") -> pd.DataFrame:
    """Standardise numeric columns (z-score or min-max)."""
    if columns is None:
        columns = list(df.select_dtypes(include=["number"]).columns)
    _ensure_cols(df, columns)
    out = df.copy()
    for col in columns:
        num, imputed = _to_numeric(out[col])
        if method == "zscore":
            out[col] = (num - num.mean()) / num.std()
        else:  # minmax 0-1
            out[col] = (num - num.min()) / (num.max() - num.min())
    return _meta_df(out.reset_index(drop=True), f"imputed_nulls:{imputed}")


def fill_nulls(df: pd.DataFrame, column: str, strategy: Literal["mean", "median", "mode", "ffill", "bfill"], value: Optional[Any] = None) -> pd.DataFrame:
    """Fill missing values in `column`."""
    _ensure_cols(df, [column])
    out = df.copy()
    if strategy in ("mean", "median"):
        num, imputed = _to_numeric(out[column])
        fill_val = getattr(num, strategy)()
        out[column] = num.fillna(fill_val)
    elif strategy == "mode":
        mode = out[column].mode()
        fill_val = mode.iloc[0] if not mode.empty else None
        out[column] = out[column].fillna(fill_val)
    elif strategy == "ffill":
        out[column] = out[column].fillna(method="ffill")
    elif strategy == "bfill":
        out[column] = out[column].fillna(method="bfill")
    elif strategy == "value":
        out[column] = out[column].fillna(value)
    else:
        raise ValueError("Unknown strategy")
    return _meta_df(out.reset_index(drop=True), f"nulls_filled:{out[column].isna().sum()}")


def bin_numeric(df: pd.DataFrame, column: str, bins: int, labels: Optional[List[str]] = None) -> pd.DataFrame:
    """Bin `column` into `bins` equal-width buckets."""
    _ensure_cols(df, [column])
    num, imputed = _to_numeric(df[column])
    binned = pd.cut(num, bins=bins, labels=labels)
    out = df.copy()
    out[f"{column}_bin"] = binned
    return _meta_df(out.reset_index(drop=True), f"imputed_nulls:{imputed}")


def value_counts(df: pd.DataFrame, column: str, top: int = 10, include_pct: bool = True) -> pd.DataFrame:
    """Frequency table of `column` (categorical or low-card numeric)."""
    _ensure_cols(df, [column])
    counts = df[column].value_counts().head(top).reset_index()
    counts.columns = [column, "count"]
    if include_pct:
        total = len(df)
        counts["pct"] = (counts["count"] / total * 100).round(2)
    return counts


def cohort(df: pd.DataFrame, user_col: str, date_col: str, metric_col: str, period: Literal["M", "Q"] = "M") -> pd.DataFrame:
    """Simple cohort retention: first event → later events."""
    _ensure_cols(df, [user_col, date_col, metric_col])
    tmp = df.copy()
    tmp[date_col] = pd.to_datetime(tmp[date_col], errors="coerce")
    tmp["period"] = tmp[date_col].dt.to_period(period)
    first = tmp.groupby(user_col)[date_col].min().dt.to_period(period).rename("cohort")
    tmp = tmp.merge(first, left_on=user_col, right_index=True)
    tmp["period_number"] = (tmp["period"] - tmp["cohort"]).apply(attrgetter("n"))
    cohort_table = tmp.groupby(["cohort", "period_number"])[metric_col].agg("count").reset_index()
    return cohort_table


def moving_avg(df: pd.DataFrame, column: str, window: int, min_periods: Optional[int] = None) -> pd.DataFrame:
    """Rolling mean of `column`."""
    _ensure_cols(df, [column])
    num, imputed = _to_numeric(df[column])
    out = df.copy()
    out[f"{column}_ma{window}"] = num.rolling(window=window, min_periods=min_periods).mean()
    return _meta_df(out.reset_index(drop=True), f"imputed_nulls:{imputed}")


def yoy_growth(df: pd.DataFrame, date_col: str, value_col: str, agg: Agg = "sum") -> pd.DataFrame:
    """Year-over-year % growth after resampling to yearly frequency."""
    _ensure_cols(df, [date_col, value_col])
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    num, imputed = _to_numeric(df[value_col])
    tmp = df.assign(**{value_col: num}).set_index(date_col)
    yearly = tmp[value_col].resample("Y").agg(agg)
    growth = yearly.pct_change() * 100
    out = growth.reset_index().rename(columns={value_col: "yoy_growth_pct"})
    return _meta_df(out.dropna().reset_index(drop=True), f"imputed_nulls:{imputed}")


def concat_tables(df1: pd.DataFrame, df2: pd.DataFrame, ignore_index: bool = True) -> pd.DataFrame:
    """Stack two DataFrames vertically (UNION)."""
    out = pd.concat([df1, df2], ignore_index=ignore_index)
    return _meta_df(out, f"rows_after_concat:{len(out)}")


def drop_duplicates(df: pd.DataFrame, subset: Optional[List[str]] = None) -> pd.DataFrame:
    """Remove duplicate rows."""
    out = df.drop_duplicates(subset=subset)
    return _meta_df(out.reset_index(drop=True), f"duplicates_dropped:{len(df)-len(out)}")


def rename_columns(df: pd.DataFrame, mapping: dict[str, str]) -> pd.DataFrame:
    """Rename columns via dict {old: new}."""
    out = df.rename(columns=mapping)
    return _meta_df(out, f"columns_renamed:{len(mapping)}")


def select_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """Keep only `columns`."""
    _ensure_cols(df, columns)
    out = df[columns].copy()
    return _meta_df(out, f"columns_selected:{len(columns)}")
4. One-Line Patch – main.py
Locate the existing name_map inside the classifier and replace it with:
Python
Copy
name_map = {
    "agg_by": "AGGREGATE",
    "compare_periods": "VARIANCE",   # keeps legacy intent name
    "filter_rows": "FILTER",
    "sort_rows": "SORT",
    "top_n_per_group": "TOP_N_PER_GROUP",
    "bottom_n_per_group": "BOTTOM_N_PER_GROUP",
    "correlate": "CORRELATE",
    "pivot_table": "PIVOT",
    "rolling_growth": "ROLLING_GROWTH",
    "outliers": "OUTLIERS",
    "percentile": "PERCENTILE",
    "normalize": "NORMALIZE",
    "fill_nulls": "FILL_NULLS",
    "bin_numeric": "BIN_NUMERIC",
    "value_counts": "VALUE_COUNTS",
    "cohort": "COHORT",
    "moving_avg": "MOVING_AVG",
    "yoy_growth": "YOY_GROWTH",
    "concat_tables": "CONCAT_TABLES",
    "drop_duplicates": "DROP_DUPLICATES",
    "rename_columns": "RENAME_COLUMNS",
    "select_columns": "SELECT_COLUMNS",
}
5. Instant Smoke Test
bash
Copy
# 1. Drop the new file in
cp analysis_toolkit.v2.py backend/analysis_toolkit.py

# 2. Add the 3-line patch to backend/main.py

# 3. Deploy
gcloud functions deploy chat --source=backend --runtime=python312 …

# 4. Ask
“Show me the top 5 outlier transactions by amount”
→  intent: “OUTLIERS”  (confidence ≥ 0.85)
→  fast-path  < 2 s
6. Summary – What You Gain
Table
Copy
Before v2	After v2
4 verbs	21 verbs
No metadata	Every result tells the LLM what happened
Manual JSON upkeep	Auto-generated TOOLS_SPEC
Multi-metric blocked	Composable single-purpose tools
Frequent fallback	> 90 % common questions stay on fast-path


*** suggested plan 2 ***

Here is a detailed, code-inclusive plan that synthesizes the best ideas from all the provided suggestions to enrich your analysis toolkit. This plan evolves your existing functions and adds several powerful new ones, complete with full implementations and the necessary orchestrator updates.

The Detailed Improvement Plan
This plan is structured in three phases: strengthening the foundation, evolving existing tools for more flexibility, and adding new capabilities to cover common analytical requests that currently rely on the fallback path.

Phase 1: Foundational Enhancements (Code Quality)
Before adding new functions, we'll implement several improvements inspired by Suggestion 2 to make the toolkit more robust, professional, and efficient.

Introduce a Custom Exception: This provides clean, user-facing error messages from the fast-path, which your orchestrator can display directly.

Python

class ToolkitError(RuntimeError):
    """Custom exception for toolkit errors to provide clean, user-facing messages."""
    pass
Create a Centralized Column Validator: This avoids repetitive code and standardizes how we check for required columns in every function.

Python

def _require_columns(df: pd.DataFrame, *cols: str) -> None:
    """Raise ToolkitError if any columns are missing."""
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ToolkitError(f"Missing required column(s): {', '.join(missing)}")
Improve Numeric Safety: The existing safe_numeric_cast is good, but we'll ensure it also handles infinities, which can arise from divisions.

Python

def safe_numeric_cast(series: pd.Series, errors: str = "coerce") -> pd.Series:
    """Safely cast a series to numeric, replacing non-finite values with NaN."""
    s = pd.to_numeric(series, errors=errors)
    return s.replace([np.inf, -np.inf], np.nan)
Phase 2: Evolving Existing Functions (Flexibility)
Next, we'll refactor the existing functions to be more powerful, addressing their primary limitations as highlighted across multiple suggestions.

run_aggregation becomes summarize_by_group: This version handles multiple aggregations in one call, a critical improvement noted in Suggestions 1 and 5.

Python

def summarize_by_group(
    df: pd.DataFrame,
    group_by_column: str,
    aggregations: list[dict],
) -> pd.DataFrame:
    """Groups by a dimension and calculates one or more aggregate metrics."""
    _require_columns(df, group_by_column, *[agg['metric_column'] for agg in aggregations])

    # Create the aggregation dictionary for pandas
    agg_dict = {
        agg['metric_column']: agg['func'] for agg in aggregations
    }

    # Cast all metric columns to numeric
    for metric_col in agg_dict.keys():
        df[metric_col] = safe_numeric_cast(df[metric_col])

    grouped = df.groupby(group_by_column, as_index=False).agg(agg_dict)
    return grouped
run_filter_and_sort is split into filter_rows and sort_rows: This decoupling makes the tools more modular and intuitive for the LLM (Suggestion 1). The new filter_rows supports complex conditions (Suggestions 4 and 5).

Python

def filter_rows(df: pd.DataFrame, filters: list[dict]) -> pd.DataFrame:
    """Filters rows based on a list of conditions (applied with AND logic)."""
    dff = df
    for f in filters:
        col, op, val = f['column'], f['operator'], f['value']
        _require_columns(dff, col)

        # For numeric operators, ensure the column is numeric
        if op in ["gt", "lt", "gte", "lte"]:
            dff[col] = safe_numeric_cast(dff[col])

        if op == "eq": dff = dff[dff[col] == val]
        elif op == "neq": dff = dff[dff[col] != val]
        elif op == "gt": dff = dff[dff[col] > val]
        elif op == "lt": dff = dff[dff[col] < val]
        elif op == "gte": dff = dff[dff[col] >= val]
        elif op == "lte": dff = dff[dff[col] <= val]
        elif op == "is_in": dff = dff[dff[col].isin(val)]
        elif op == "contains": dff = dff[dff[col].astype(str).str.contains(val, na=False)]
    return dff

def sort_rows(df: pd.DataFrame, sort_by_column: str, ascending: bool = False) -> pd.DataFrame:
    """Sorts a DataFrame by a specific column."""
    _require_columns(df, sort_by_column)
    return df.sort_values(by=sort_by_column, ascending=ascending, kind="mergesort").reset_index(drop=True)
Phase 3: Expanding the Toolkit (New Capabilities)
Finally, we add new functions to cover the most common analysis patterns that currently force a fallback.

top_n_per_group: For ranking within categories (Suggestions 1, 2, 4).

Python

def top_n_per_group(df: pd.DataFrame, group_by_column: str, metric_column: str, n: int = 5, ascending: bool = False) -> pd.DataFrame:
    """Returns the top or bottom N rows for each group based on a metric."""
    _require_columns(df, group_by_column, metric_column)
    df[metric_column] = safe_numeric_cast(df[metric_column])
    return (
        df.sort_values(by=metric_column, ascending=ascending)
        .groupby(group_by_column)
        .head(n)
        .reset_index(drop=True)
    )
pivot_table: For creating cross-tabulations (Suggestions 1, 3, 4).

Python

def pivot_table(df: pd.DataFrame, index: str, columns: str, values: str, aggfunc: str = "sum") -> pd.DataFrame:
    """Creates a pivot table to summarize data across two dimensions."""
    _require_columns(df, index, columns, values)
    df[values] = safe_numeric_cast(df[values])
    pivot = df.pivot_table(
        index=index,
        columns=columns,
        values=values,
        aggfunc=aggfunc,
        fill_value=0
    )
    return pivot.reset_index()
calculate_correlation: For statistical relationship analysis (all suggestions).

Python

def calculate_correlation(df: pd.DataFrame, columns: list[str], method: str = "pearson") -> pd.DataFrame:
    """Computes the pairwise correlation matrix for a list of numeric columns."""
    _require_columns(df, *columns)
    numeric_df = df[columns].apply(safe_numeric_cast)
    if numeric_df.shape[1] < 2:
        raise ToolkitError("Correlation requires at least two numeric columns.")

    corr_matrix = numeric_df.corr(method=method).reset_index()
    return corr_matrix.rename(columns={'index': 'variable'})
summarize_by_time: For time-series trend analysis (Suggestions 4, 5, 6).

Python

def summarize_by_time(df: pd.DataFrame, date_column: str, frequency: str, aggregations: list[dict]) -> pd.DataFrame:
    """Aggregates time-series data into larger time buckets (e.g., daily to monthly)."""
    _require_columns(df, date_column)
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')

    agg_dict = {agg['metric_column']: agg['func'] for agg in aggregations}
    for metric_col in agg_dict.keys():
        _require_columns(df, metric_col)
        df[metric_col] = safe_numeric_cast(df[metric_col])

    resampled = df.resample(frequency, on=date_column).agg(agg_dict)

    # Add percentage change for the first metric
    first_metric = list(agg_dict.keys())[0]
    resampled[f'{first_metric}_pct_change'] = resampled[first_metric].pct_change()

    return resampled.reset_index()
The Complete analysis_toolkit.py (Version 2.0)
Here is the full, copy-pasteable file that includes all the recommended changes.

Python

"""
Enhanced Analysis Toolkit (v2) for fast-path deterministic operations.

This toolkit provides a set of robust, LLM-friendly functions for common data 
analysis tasks. Each function is designed to be safe, efficient, and to return
a pandas.DataFrame suitable for direct use.
"""
from __future__ import annotations
from typing import List, Optional
import pandas as pd
import numpy as np

TOOLKIT_VERSION = 2

# ----------------- Foundational Components -----------------

class ToolkitError(RuntimeError):
    """Custom exception for toolkit errors to provide clean, user-facing messages."""
    pass

def _require_columns(df: pd.DataFrame, *cols: str) -> None:
    """Raise ToolkitError if any columns are missing."""
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ToolkitError(f"Missing required column(s): {', '.join(missing)}")

def safe_numeric_cast(series: pd.Series, errors: str = "coerce") -> pd.Series:
    """Safely cast a series to numeric, replacing non-finite values with NaN."""
    s = pd.to_numeric(series, errors=errors)
    return s.replace([np.inf, -np.inf], np.nan)

# ----------------- Evolved & New Toolkit Functions -----------------

def summarize_by_group(
    df: pd.DataFrame,
    group_by_column: str,
    aggregations: list[dict],
) -> pd.DataFrame:
    """Groups by a dimension and calculates one or more aggregate metrics."""
    _require_columns(df, group_by_column, *[agg['metric_column'] for agg in aggregations])
    agg_dict = {agg['metric_column']: agg['func'] for agg in aggregations}
    for metric_col in agg_dict.keys():
        df[metric_col] = safe_numeric_cast(df[metric_col])
    grouped = df.groupby(group_by_column, as_index=False).agg(agg_dict)
    return grouped

def compare_periods(
    df: pd.DataFrame,
    group_by_column: str,
    baseline_period_column: str,
    comparison_period_column: str,
    aggregation_func: str = "sum"
) -> pd.DataFrame:
    """Compares two numeric period columns grouped by a dimension."""
    _require_columns(df, group_by_column, baseline_period_column, comparison_period_column)
    df[baseline_period_column] = safe_numeric_cast(df[baseline_period_column])
    df[comparison_period_column] = safe_numeric_cast(df[comparison_period_column])
    
    grouped = df.groupby(group_by_column, as_index=False).agg({
        baseline_period_column: aggregation_func,
        comparison_period_column: aggregation_func
    })
    grouped["delta"] = grouped[comparison_period_column] - grouped[baseline_period_column]
    grouped["pct_change"] = (grouped["delta"] / grouped[baseline_period_column].replace(0, np.nan)) * 100.0
    return grouped.sort_values(by="delta", ascending=False).reset_index(drop=True)

def filter_rows(df: pd.DataFrame, filters: list[dict]) -> pd.DataFrame:
    """Filters rows based on a list of conditions (applied with AND logic)."""
    dff = df
    for f in filters:
        col, op, val = f['column'], f['operator'], f['value']
        _require_columns(dff, col)
        
        if op in ["gt", "lt", "gte", "lte"]:
            dff[col] = safe_numeric_cast(dff[col])
        
        if op == "eq": dff = dff[dff[col] == val]
        elif op == "neq": dff = dff[dff[col] != val]
        elif op == "gt": dff = dff[dff[col] > val]
        elif op == "lt": dff = dff[dff[col] < val]
        elif op == "gte": dff = dff[dff[col] >= val]
        elif op == "lte": dff = dff[dff[col] <= val]
        elif op == "is_in": dff = dff[dff[col].isin(val)]
        elif op == "contains": dff = dff[dff[col].astype(str).str.contains(val, na=False)]
    return dff.reset_index(drop=True)

def sort_rows(df: pd.DataFrame, sort_by_column: str, ascending: bool = False, limit: int = 0) -> pd.DataFrame:
    """Sorts a DataFrame by a specific column and optionally limits the rows."""
    _require_columns(df, sort_by_column)
    dff = df.sort_values(by=sort_by_column, ascending=ascending, kind="mergesort")
    if limit > 0:
        dff = dff.head(limit)
    return dff.reset_index(drop=True)

def get_dataset_summary(df: pd.DataFrame, include: str = "numeric") -> pd.DataFrame:
    """Provides descriptive statistics for dataset columns."""
    if include == "numeric":
        desc = df.select_dtypes(include=np.number).describe().transpose()
    elif include == "categorical":
        desc = df.select_dtypes(include=['object', 'category']).describe().transpose()
    else:
        desc = df.describe(include='all').transpose()
    return desc.reset_index().rename(columns={"index": "column"})

def top_n_per_group(df: pd.DataFrame, group_by_column: str, metric_column: str, n: int = 5, ascending: bool = False) -> pd.DataFrame:
    """Returns the top or bottom N rows for each group based on a metric."""
    _require_columns(df, group_by_column, metric_column)
    df[metric_column] = safe_numeric_cast(df[metric_column])
    return (
        df.sort_values(by=metric_column, ascending=ascending)
        .groupby(group_by_column)
        .head(n)
        .reset_index(drop=True)
    )

def pivot_table(df: pd.DataFrame, index: str, columns: str, values: str, aggfunc: str = "sum") -> pd.DataFrame:
    """Creates a pivot table to summarize data across two dimensions."""
    _require_columns(df, index, columns, values)
    df[values] = safe_numeric_cast(df[values])
    pivot = df.pivot_table(
        index=index,
        columns=columns,
        values=values,
        aggfunc=aggfunc,
        fill_value=0
    )
    return pivot.reset_index()

def calculate_correlation(df: pd.DataFrame, columns: list[str], method: str = "pearson") -> pd.DataFrame:
    """Computes the pairwise correlation matrix for a list of numeric columns."""
    _require_columns(df, *columns)
    numeric_df = df[columns].apply(safe_numeric_cast)
    if numeric_df.shape[1] < 2:
        raise ToolkitError("Correlation requires at least two numeric columns.")
    
    corr_matrix = numeric_df.corr(method=method).reset_index()
    return corr_matrix.rename(columns={'index': 'variable'})

def summarize_by_time(df: pd.DataFrame, date_column: str, frequency: str, aggregations: list[dict]) -> pd.DataFrame:
    """Aggregates time-series data into larger time buckets (e.g., daily to monthly)."""
    _require_columns(df, date_column)
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce').dt.floor(frequency[0])
    
    agg_dict = {agg['metric_column']: agg['func'] for agg in aggregations}
    for metric_col in agg_dict.keys():
        _require_columns(df, metric_col)
        df[metric_col] = safe_numeric_cast(df[metric_col])

    resampled = df.groupby(date_column).agg(agg_dict)
    
    first_metric = list(agg_dict.keys())[0]
    resampled[f'{first_metric}_pct_change'] = resampled[first_metric].pct_change()
    
    return resampled.reset_index()
The Definitive TOOLS_SPEC for Gemini
This TOOLS_SPEC reflects all the new and refactored functions, with detailed descriptions to guide the LLM effectively.

Python

TOOLS_SPEC = [
    {
        "name": "summarize_by_group",
        "description": "Groups a dataset by a categorical column and calculates one or more aggregate metrics for each group.",
        "parameters": {
            "type": "object",
            "properties": {
                "group_by_column": {"type": "string", "description": "The column to group the data by (e.g., 'category', 'region')."},
                "aggregations": {
                    "type": "array",
                    "description": "A list of metrics to calculate.",
                    "items": {
                        "type": "object",
                        "properties": {
                            "metric_column": {"type": "string", "description": "The numeric column to aggregate."},
                            "func": {
                                "type": "string", 
                                "enum": ["sum", "mean", "median", "count", "nunique", "min", "max", "std"], 
                                "description": "The aggregation function to apply."
                            },
                        }, "required": ["metric_column", "func"],
                    }
                },
            }, "required": ["group_by_column", "aggregations"],
        },
    },
    {
        "name": "compare_periods",
        "description": "Compares two numeric columns (periods) grouped by a dimension. Calculates the sum or mean for each period, the absolute delta, and the percentage change.",
        "parameters": {
            "type": "object",
            "properties": {
                "group_by_column": {"type": "string", "description": "The column to group the data by."},
                "baseline_period_column": {"type": "string", "description": "The first or earlier period's numeric column (e.g., 'sales_2023')."},
                "comparison_period_column": {"type": "string", "description": "The second or later period's numeric column (e.g., 'sales_2024')."},
                "aggregation_func": {"type": "string", "enum": ["sum", "mean"], "description": "The function to use for aggregating periods before comparison. Defaults to 'sum'."},
            }, "required": ["group_by_column", "baseline_period_column", "comparison_period_column"],
        },
    },
    {
        "name": "filter_rows",
        "description": "Filters rows based on a list of conditions. All conditions are applied with AND logic.",
        "parameters": {
            "type": "object",
            "properties": {
                "filters": {
                    "type": "array",
                    "description": "A list of conditions to filter the data by.",
                    "items": {
                        "type": "object",
                        "properties": {
                            "column": {"type": "string", "description": "The column to apply the filter on."},
                            "operator": {
                                "type": "string", 
                                "enum": ["eq", "neq", "gt", "lt", "gte", "lte", "is_in", "contains"],
                                "description": "The comparison operator (eq: equals, neq: not-equals, gt: greater-than, lt: less-than, gte: greater-or-equal, lte: less-or-equal, is_in: value is in a list, contains: substring match)."
                            },
                            "value": {"type": "any", "description": "The value for the comparison. For 'is_in', this should be a list."},
                        }, "required": ["column", "operator", "value"],
                    }
                },
            }, "required": ["filters"],
        },
    },
    {
        "name": "sort_rows",
        "description": "Sorts the dataset by a specific column and optionally limits the number of rows returned.",
        "parameters": {
            "type": "object",
            "properties": {
                "sort_by_column": {"type": "string", "description": "The column to sort the final results by."},
                "ascending": {"type": "boolean", "description": "Set to True for ascending order, False for descending. Defaults to False."},
                "limit": {"type": "integer", "description": "The maximum number of rows to return. Use 0 for no limit. Defaults to 0."},
            }, "required": ["sort_by_column"],
        },
    },
    {
        "name": "get_dataset_summary",
        "description": "Provides descriptive statistics for dataset columns.",
        "parameters": {
            "type": "object",
            "properties": {
                "include": {"type": "string", "enum": ["all", "numeric", "categorical"], "description": "The type of columns to include in the summary. Defaults to 'numeric'."}
            },
        },
    },
    {
        "name": "top_n_per_group",
        "description": "Finds the top (or bottom) N items within each category of a grouping column, based on a metric.",
        "parameters": {
            "type": "object",
            "properties": {
                "group_by_column": {"type": "string", "description": "The column to create groups from."},
                "metric_column": {"type": "string", "description": "The numeric column to rank items by."},
                "n": {"type": "integer", "description": "The number of items to return per group. Defaults to 5."},
                "ascending": {"type": "boolean", "description": "Set to True to find the bottom N items. Defaults to False (top N)."}
            }, "required": ["group_by_column", "metric_column"],
        },
    },
    {
        "name": "pivot_table",
        "description": "Creates a pivot table, summarizing a numeric values column across two categorical dimensions (rows and columns).",
        "parameters": {
            "type": "object",
            "properties": {
                "index": {"type": "string", "description": "The column to use for the pivot table's rows."},
                "columns": {"type": "string", "description": "The column to use for the pivot table's columns."},
                "values": {"type": "string", "description": "The numeric column to aggregate."},
                "aggfunc": {"type": "string", "enum": ["sum", "mean", "median", "count"], "description": "The aggregation function. Defaults to 'sum'."}
            }, "required": ["index", "columns", "values"],
        },
    },
    {
        "name": "calculate_correlation",
        "description": "Computes the pairwise correlation between two or more numeric columns to measure how they move together.",
        "parameters": {
            "type": "object",
            "properties": {
                "columns": {
                    "type": "array",
                    "description": "A list of at least two numeric column names to correlate.", "items": {"type": "string"}
                },
                 "method": {"type": "string", "enum": ["pearson", "kendall", "spearman"], "description": "The correlation method. Defaults to 'pearson'."},
            }, "required": ["columns"],
        },
    },
    {
        "name": "summarize_by_time",
        "description": "Groups time-series data into periods (e.g., day, month, year) and calculates aggregate metrics.",
        "parameters": {
            "type": "object",
            "properties": {
                "date_column": {"type": "string", "description": "The column containing date or timestamp information."},
                "frequency": {"type": "string", "enum": ["D", "W", "M", "Q", "Y"], "description": "The time frequency to group by (Day, Week, Month, Quarter, Year)."},
                "aggregations": {
                    "type": "array", "description": "A list of metrics to calculate for each time period.", "items": {
                        "type": "object", "properties": {
                            "metric_column": {"type": "string", "description": "The numeric column to aggregate."},
                            "func": {"type": "string", "enum": ["sum", "mean", "median", "count", "nunique"], "description": "Aggregation function."}
                        }, "required": ["metric_column", "func"]
                    }
                },
            }, "required": ["date_column", "frequency", "aggregations"],
        },
    }
]
Orchestrator Integration Steps (main.py)
To complete the upgrade, you must update the name_map in your main.py orchestrator to recognize the new and refactored tool names. This ensures the classifier's output is correctly routed to the new functions.

Python

# In main.py, update the name_map inside the _events generator function

name_map = {
    # New and refactored names
    "summarize_by_group": "AGGREGATE",
    "compare_periods": "VARIANCE",
    "filter_rows": "FILTER",
    "sort_rows": "SORT",
    "get_dataset_summary": "DESCRIBE",
    "top_n_per_group": "TOP_N_PER_GROUP",
    "pivot_table": "PIVOT",
    "calculate_correlation": "CORRELATE",
    "summarize_by_time": "TIME_SERIES",
    
    # Keep old names for backward compatibility during transition if needed
    "run_aggregation": "AGGREGATE",
    "run_variance": "VARIANCE",
    "run_filter_and_sort": "FILTER_SORT_LEGACY", # Map to a distinct legacy intent
    "run_describe": "DESCRIBE",
}
